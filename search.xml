<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【JVM】生命周期</title>
      <link href="2021/07/15/jvm-sheng-ming-zhou-qi/"/>
      <url>2021/07/15/jvm-sheng-ming-zhou-qi/</url>
      
        <content type="html"><![CDATA[<h2 id="JVM的生命周期"><a href="#JVM的生命周期" class="headerlink" title="JVM的生命周期"></a>JVM的生命周期</h2><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/image-20210524221836756.png" alt=""></p><h3 id="实例1"><a href="#实例1" class="headerlink" title="实例1"></a>实例1</h3><pre class=" language-lang-java"><code class="language-lang-java">public class StackStruTest {    public static void main(String[] args) {        int i = 2;        int j = 3;        int k = i + j;        try {            Thread.sleep(6000);        } catch (InterruptedException e) {            e.printStackTrace();        }        System.out.println("Hello!!!");    }}//jps用于打印程序中运行的进程//以下为cmd命令窗口的输入输出记录//D:\Code\JAVA\MyJVMLesson>jps   //2932 Jps//11916//2572 StackStruTest  //睡眠中,进程还存活////D:\Code\JAVA\MyJVMLesson>jps//28544 Jps//11916</code></pre><h3 id="虚拟机退出情况"><a href="#虚拟机退出情况" class="headerlink" title="虚拟机退出情况"></a>虚拟机退出情况</h3><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/image-20210524222258967.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
            <tag> java虚拟机 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【JVM】类加载子系统</title>
      <link href="2021/07/15/jvm-lei-jia-zai-zi-xi-tong/"/>
      <url>2021/07/15/jvm-lei-jia-zai-zi-xi-tong/</url>
      
        <content type="html"><![CDATA[<h1 id="类加载子系统"><a href="#类加载子系统" class="headerlink" title="类加载子系统"></a>类加载子系统</h1><h2 id="一、内存结构概述"><a href="#一、内存结构概述" class="headerlink" title="一、内存结构概述"></a>一、内存结构概述</h2><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/image-20210525204437812.png" alt=""></p><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/image-20210525204554174.png" alt=""></p><h2 id="二、类加载器和类加载的过程"><a href="#二、类加载器和类加载的过程" class="headerlink" title="二、类加载器和类加载的过程"></a>二、类加载器和类加载的过程</h2><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/image-20210526130706669.png" alt=""></p><h3 id="2-1类的加载过程一：Loading"><a href="#2-1类的加载过程一：Loading" class="headerlink" title="2.1类的加载过程一：Loading"></a>2.1类的加载过程一：Loading</h3><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/image-20210526130743536.png" alt=""><br><em>全限定类名：就是类名全称，带包路径的用点隔开，例如: java.lang.String。</em></p><ul><li>加载的.class文件可以来源于如下：</li></ul><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/image-20210526130950633.png" alt=""></p><h3 id="2-2类的加载过程二：Linking"><a href="#2-2类的加载过程二：Linking" class="headerlink" title="2.2类的加载过程二：Linking"></a>2.2类的加载过程二：Linking</h3><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/image-20210526095624707.png" alt=""></p><p>注意一下准备阶段的两点，变量初始化为0，常量在编译的时候就已经初始化</p><h3 id="2-3类的加载过程三：Initialization"><a href="#2-3类的加载过程三：Initialization" class="headerlink" title="2.3类的加载过程三：Initialization"></a>2.3类的加载过程三：Initialization</h3><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/image-20210526103040562.png" alt=""></p><ul><li>构造器方法\<clinit\>()是用来对静态代码块初始化的，当没有静态变量，静态代码块时，将没有\<clinit\>()类构造器方法</clinit\></clinit\></li></ul><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/image-20210526103946909.png" alt=""></p><ul><li>\<init>是构造器函数</init></li></ul><p>实例代码文件：D:\Code\JAVA\MyJVMLesson\src\com\chapter02\ClinitTest1.java</p><p><a href="https://blog.csdn.net/lpy943739901/article/details/107875872">IDEA jclasslib 解决无法查看内部类字节码问题_lpy943739901的博客-CSDN博客</a></p><ul><li>\<clinit\>()先执行父类的构造器，执行完毕后才开始执行子类的构造器，如下图所示，先加载Father里面的A，再加载Son里面的B。</clinit\></li></ul><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/image-20210526125009392.png" alt=""></p><ul><li><p>一个类中\<clinit\>()方法只会被加载一次，通过加锁的方式实现，如下图所示。</clinit\></p><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/image-20210526130532116.png" alt="image-20210526130532116"></p><p>输出如下所示：</p><pre class=" language-lang-java"><code class="language-lang-java">//输出结果//线程2开始//线程1开始//线程2初始化当前类</code></pre></li></ul><h3 id="2-3类的加载器"><a href="#2-3类的加载器" class="headerlink" title="2.3类的加载器"></a>2.3类的加载器</h3><ul><li>概述</li></ul><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/image-20210527132608812.png" alt=""></p><p>引导类又称为启动类加载器。</p><ul><li>类加载器的分类</li></ul><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/image-20210527132822285.png" alt=""></p><p>启动类加载器 —-&gt; 主要加载的是JVM自身需要的类</p><p>拓展类加载器 —-&gt; 加载<code>&lt;JAVA_HOME&gt;/lib/ext</code>目录下或者由系统变量-Djava.ext.dir指定位路径中的类库，开发者可以直接使用标准扩展类加载器</p><p>系统类加载器 —-&gt;  负责加载系统类路径<code>java -classpath</code>或<code>-D java.class.path</code> 指定路径下的类库</p><ul><li>自定义类加载器实现</li></ul><p><img src="/images/%E3%80%90JVM%E3%80%91%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/image-20210527150752565.png" alt=""></p><h2 id="三、双亲委派机制"><a href="#三、双亲委派机制" class="headerlink" title="三、双亲委派机制"></a>三、双亲委派机制</h2>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java虚拟机 </tag>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【JVM】运行时数据区</title>
      <link href="2021/07/15/jvm-yun-xing-shi-shu-ju-qu/"/>
      <url>2021/07/15/jvm-yun-xing-shi-shu-ju-qu/</url>
      
        <content type="html"><![CDATA[<h2 id="运行时数据区"><a href="#运行时数据区" class="headerlink" title="运行时数据区"></a>运行时数据区</h2><h3 id="1-整体概述"><a href="#1-整体概述" class="headerlink" title="1.整体概述"></a>1.整体概述</h3><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img.png" alt=""><br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_1.png" alt="img_1.png"></p><h3 id="2-程序计数器（PC寄存器）"><a href="#2-程序计数器（PC寄存器）" class="headerlink" title="2.程序计数器（PC寄存器）"></a>2.程序计数器（PC寄存器）</h3><h4 id="2-1介绍"><a href="#2-1介绍" class="headerlink" title="2.1介绍"></a>2.1介绍</h4><p>JVM中的PC寄存器是对物理PC寄存器的一种抽象模拟<br>CPU只有将数据装载到寄存器才能够运行</p><p><strong>作用</strong>：存储下一条指令的地址</p><p>操作流程：<br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_2.png" alt=""></p><h4 id="2-2两个常见的问题"><a href="#2-2两个常见的问题" class="headerlink" title="2.2两个常见的问题"></a>2.2两个常见的问题</h4><p>(1)使用PC寄存器存储字节码指令地址有什么用？</p><p>答：CPU不停的切换各个线程，需要通过JVM字节码解释器改变PC寄存器的值来明确下一条应该执行哪条命令。</p><p>(2)PC寄存器为什么会被设定为线程私有？</p><p>答：为了能够准确地记录各个线程正在执行的当前字节码指令地址，以便各个线程之间进行独立计算，互不干扰</p><h3 id="3-虚拟机栈"><a href="#3-虚拟机栈" class="headerlink" title="3.虚拟机栈"></a>3.虚拟机栈</h3><p>虚拟机架构分为栈和寄存器两种方式。<br>由于跨平台的设计，Java虚拟机指令是根据栈来设计的。</p><h4 id="3-1内存中的栈和堆"><a href="#3-1内存中的栈和堆" class="headerlink" title="3.1内存中的栈和堆"></a>3.1内存中的栈和堆</h4><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_5.png" alt=""></p><h4 id="3-2栈中可能遇到的异常"><a href="#3-2栈中可能遇到的异常" class="headerlink" title="3.2栈中可能遇到的异常"></a>3.2栈中可能遇到的异常</h4><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_3.png" alt=""></p><h4 id="3-3设置栈空间大小"><a href="#3-3设置栈空间大小" class="headerlink" title="3.3设置栈空间大小"></a>3.3设置栈空间大小</h4><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_4.png" alt=""></p><h4 id="3-4栈的存储单位——栈帧"><a href="#3-4栈的存储单位——栈帧" class="headerlink" title="3.4栈的存储单位——栈帧"></a>3.4栈的存储单位——栈帧</h4><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_6.png" alt=""></p><h4 id="3-5栈帧的内部结构"><a href="#3-5栈帧的内部结构" class="headerlink" title="3.5栈帧的内部结构"></a>3.5栈帧的内部结构</h4><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_7.png" alt=""></p><h5 id="3-5-1-栈帧结构之一局部变量表-重要！！！！"><a href="#3-5-1-栈帧结构之一局部变量表-重要！！！！" class="headerlink" title="3.5.1 栈帧结构之一局部变量表(重要！！！！)"></a>3.5.1 栈帧结构之一局部变量表(重要！！！！)</h5><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_8.png" alt=""><br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_10.png" alt="img_10.png"></p><ul><li><p>局部变量表中最基本的存储单元——Slot(变量槽)<br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_11.png" alt=""><br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_12.png" alt="img_12.png"></p><ul><li><em>如上右图，64位的类型占两格(slot)，索引时按照其实位置索引，例：long m的起始索引为1。</em></li><li><em>方法参数和局部变量，按照申明顺序复制到局部变量表中</em></li><li><em>对于非静态方法，栈中需要多存储一个对象引用存放在index为0的slot处</em></li><li><em>因此，在静态方法中，不能使用this这个关键字</em><br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_13.png" alt=""></li></ul></li><li><p>Slot的重复利用<br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_14.png" alt=""><br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_15.png" alt="img_15.png"></p></li></ul><p>前面章节提到过，在类体中，根据变量定义的位置不同，以及定义的方式不同，类属性又可细分为以下 3 种类型：</p><ul><li>类体中、所有函数之外：此范围定义的变量，称为类属性或类变量；</li><li>类体中，所有函数内部：以“self.变量名”的方式定义的变量，称为实例属性或实例变量；</li><li>类体中，所有函数内部：以“变量名=变量值”的方式定义的变量，称为局部变量。<br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_17.png" alt=""><br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_18.png" alt="img_18.png"><br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_19.png" alt="img_19.png"></li></ul><h5 id="3-5-2栈帧结构之一操作数栈"><a href="#3-5-2栈帧结构之一操作数栈" class="headerlink" title="3.5.2栈帧结构之一操作数栈"></a>3.5.2栈帧结构之一操作数栈</h5><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_20.png" alt=""><br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_21.png" alt="img_21.png"><br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_22.png" alt="img_22.png"></p><h5 id="3-5-3栈帧结构之一动态链接"><a href="#3-5-3栈帧结构之一动态链接" class="headerlink" title="3.5.3栈帧结构之一动态链接"></a>3.5.3栈帧结构之一动态链接</h5><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_23.png" alt=""><br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_24.png" alt="img_24.png"></p><h5 id="3-5-4栈帧结构之一方法返回地址"><a href="#3-5-4栈帧结构之一方法返回地址" class="headerlink" title="3.5.4栈帧结构之一方法返回地址"></a>3.5.4栈帧结构之一方法返回地址</h5><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_28.png" alt=""><br>存放的是<strong>调用该方法</strong>的pc寄存器的值</p><h5 id="3-5-5栈帧结构之一附加信息"><a href="#3-5-5栈帧结构之一附加信息" class="headerlink" title="3.5.5栈帧结构之一附加信息"></a>3.5.5栈帧结构之一附加信息</h5><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_29.png" alt=""></p><h4 id="3-6虚方法与非虚方法"><a href="#3-6虚方法与非虚方法" class="headerlink" title="3.6虚方法与非虚方法"></a>3.6虚方法与非虚方法</h4><p>不  可变即非虚方法<br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_25.png" alt=""><br><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_26.png" alt="img_26.png"></p><h4 id="3-7动态类型语言和静态类型语言"><a href="#3-7动态类型语言和静态类型语言" class="headerlink" title="3.7动态类型语言和静态类型语言"></a>3.7动态类型语言和静态类型语言</h4><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_27.png" alt=""><br>Java是静态类型语言，Python是动态类型语言。</p><p>虚拟机中invokedynamic指令一定程度上实现了<strong>动态类型语言</strong>的特性</p><h4 id="3-6栈的相关问题"><a href="#3-6栈的相关问题" class="headerlink" title="3.6栈的相关问题"></a>3.6栈的相关问题</h4><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_30.png" alt=""><br>最后一个问题：如果局部变量在内部消亡就线程安全，不消亡就线程不安全。</p><h3 id="4-本地方法栈"><a href="#4-本地方法栈" class="headerlink" title="4 本地方法栈"></a>4 本地方法栈</h3><h4 id="4-1本地方法"><a href="#4-1本地方法" class="headerlink" title="4.1本地方法"></a>4.1本地方法</h4><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_31.png" alt=""><br>例如：Object类中的getclass()方法，Thread类中的start0()方法</p><h4 id="4-2本地方法栈"><a href="#4-2本地方法栈" class="headerlink" title="4.2本地方法栈"></a>4.2本地方法栈</h4><p><img src="/images/%E3%80%90JVM%E3%80%91%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA/img_32.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java虚拟机，学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【NoSQL】入门笔记</title>
      <link href="2021/07/15/nosql-ru-men-bi-ji/"/>
      <url>2021/07/15/nosql-ru-men-bi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="1-NoSQL简史"><a href="#1-NoSQL简史" class="headerlink" title="1.NoSQL简史"></a>1.NoSQL简史</h2><p><img src="/images/%E3%80%90NoSQL%E3%80%91%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0.assets/clip_image001.png" alt=""></p><h2 id="2-什么是NoSQL？"><a href="#2-什么是NoSQL？" class="headerlink" title="2.什么是NoSQL？"></a>2.什么是NoSQL？</h2><p><img src="/images/%E3%80%90NoSQL%E3%80%91%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0.assets/clip_image002.png" alt=""></p><p>非关系型数据库统称为NoSQL</p><h2 id="3-为什么使用NoSQL？"><a href="#3-为什么使用NoSQL？" class="headerlink" title="3.为什么使用NoSQL？"></a>3.为什么使用NoSQL？</h2><p>一般来说，公司都是关系型数据库和非关系型数据库搭配使用。关系型数据库用于最终的数据存储，持久化；而非关系型数据库用于解决快速查询插入数据这一问题，抛弃了数据库的事务处理，从而降低了数据的安全性。</p><h2 id="4-关系型数据库VS非关系型数据库对比"><a href="#4-关系型数据库VS非关系型数据库对比" class="headerlink" title="4.关系型数据库VS非关系型数据库对比"></a>4.关系型数据库VS非关系型数据库对比</h2><p><img src="/images/%E3%80%90NoSQL%E3%80%91%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0.assets/clip_image003.png" alt=""></p><h2 id="5-常见的数据库排行"><a href="#5-常见的数据库排行" class="headerlink" title="5.常见的数据库排行"></a>5.常见的数据库排行</h2><p>官方网站：<a href="https://db-engines.com/en/ranking">https://db-engines.com/en/ranking</a></p><p><img src="/images/%E3%80%90NoSQL%E3%80%91%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0.assets/image-20210316091604144.png" alt=""></p><h2 id="6-NoSQL四大家族"><a href="#6-NoSQL四大家族" class="headerlink" title="6.NoSQL四大家族"></a>6.NoSQL四大家族</h2><p><img src="/images/%E3%80%90NoSQL%E3%80%91%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0.assets/image-20210316091657756.png" alt=""></p><p><img src="/images/%E3%80%90NoSQL%E3%80%91%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0.assets/image-20210316091829314.png" alt=""></p><p>上面提到的类JSON格式的数据是BSON，二进制JSON格式数据</p><p>一般，非关系型数据库被用于构建项目初期使用，方面随着需求的改变随时改变数据库结构，而不是前期将数据库表结构写死，导致功能很难拓展</p><p><img src="/images/%E3%80%90NoSQL%E3%80%91%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0.assets/image-20210316092924989.png" alt=""></p><p>列族存储一般是用于大数据存储，应对分布式数据存储</p><p><img src="/images/%E3%80%90NoSQL%E3%80%91%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0.assets/image-20210316093137012.png" alt=""></p><h2 id="7-NoSQL优缺点"><a href="#7-NoSQL优缺点" class="headerlink" title="7.NoSQL优缺点"></a>7.NoSQL优缺点</h2><p><img src="/images/%E3%80%90NoSQL%E3%80%91%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0.assets/image-20210316093305521.png" alt=""></p><p><img src="/images/%E3%80%90NoSQL%E3%80%91%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0.assets/image-20210316093345871.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NoSQL </tag>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【机器学习】AdaBoost模型</title>
      <link href="2020/12/13/ji-qi-xue-xi-adaboost-mo-xing/"/>
      <url>2020/12/13/ji-qi-xue-xi-adaboost-mo-xing/</url>
      
        <content type="html"><![CDATA[<p>AdaBoost是典型的Boosting算法，属于Boosting家族的一员。</p><p>在说AdaBoost之前，先说说Boosting提升算法。</p><h2 id="1-Boosting算法"><a href="#1-Boosting算法" class="headerlink" title="1.Boosting算法"></a>1.Boosting算法</h2><p>其核心思想是针对同一个训练集训练不同的<strong>分类器(弱分类器)</strong>，然后把这些弱分类器集合起来，构成一个更强的<strong>最终分类器（强分类器）</strong>。</p><p>通俗来讲，可以类比为团队思想，对某一件事情，每一个做决策的人都是知识面较<strong>弱小的个体</strong>（相较于团体），而若干个人通过“投票”做出决策，则好比一个<strong>强大的团体</strong>。</p><p>主要思想是“三个臭皮匠顶个诸葛亮”。</p><p>Boosting算法要涉及到两个部分，加法模型和前向分步算法。</p><p>加法模型就是说强分类器由一系列弱分类器线性相加而成。一般组合形式如下：</p><p><img src="/images/机器学习/image-20201213200542242.png" alt=""></p><p>前向分步就是说在训练过程中，下一轮迭代产生的分类器是在上一轮的基础上训练得来的。也就是可以写成这样的形式：</p><p><img src="/images/机器学习/Boosting.png" alt=""></p><p>由于采用的损失函数不同，Boosting算法也因此有了不同的类型，AdaBoost就是损失函数为指数损失的Boosting算法。</p><h2 id="2-AdaBoost"><a href="#2-AdaBoost" class="headerlink" title="2.AdaBoost"></a>2.AdaBoost</h2><h3 id="2-1两个重要思路"><a href="#2-1两个重要思路" class="headerlink" title="2.1两个重要思路"></a>2.1两个重要思路</h3><blockquote><p>将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值，提高那些被错误分类的样本权值。</p><p>加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。</p></blockquote><p>参考资料：</p><p><a href="https://www.cnblogs.com/ScorpioLu/p/8295990.html">AdaBoost算法详解</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> AdaBoost </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Linux】常用命令</title>
      <link href="2020/12/13/linux-chang-yong-ming-ling/"/>
      <url>2020/12/13/linux-chang-yong-ming-ling/</url>
      
        <content type="html"><![CDATA[<p>一、文件复制命令cp</p><pre><code>命令格式：cp [-adfilprsu] 源文件(source) 目标文件(destination)cp [option] source1 source2 source3 ... directory</code></pre><p>参数说明：<br>-a:是指archive的意思，也说是指复制所有的目录<br>-d:若源文件为连接文件(link file)，则复制连接文件属性而非文件本身<br>-f:强制(force)，若有重复或其它疑问时，不会询问用户，而强制复制<br>-i:若目标文件(destination)已存在，在覆盖时会先询问是否真的操作<br>-l:建立硬连接(hard link)的连接文件，而非复制文件本身<br>-p:与文件的属性一起复制，而非使用默认属性<br>-r:递归复制，用于目录的复制操作<br>-s:复制成符号连接文件(symbolic link)，即“快捷方式”文件<br>-u:若目标文件比源文件旧，更新目标文件 </p><p>如将/test1目录下的file1复制到/test3目录，并将文件名改为file2,可输入以下命令：<br>cp /test1/file1 /test3/file2</p><p>二、文件移动命令mv</p><pre><code>命令格式：mv [-fiv] source destination</code></pre><p>参数说明：<br>-f:force，强制直接移动而不询问<br>-i:若目标文件(destination)已经存在，就会询问是否覆盖<br>-u:若目标文件已经存在，且源文件比较新，才会更新</p><p>如将/test1目录下的file1复制到/test3 目录，并将文件名改为file2,可输入以下命令：<br>mv /test1/file1 /test3/file2</p><p>三、文件删除命令rm</p><pre><code>命令格式：rm [fir] 文件或目录</code></pre><p>参数说明：<br>-f:强制删除<br>-i:交互模式，在删除前询问用户是否操作<br>-r:递归删除，常用在目录的删除</p><p>如删除/test目录下的file1文件，可以输入以下命令：<br>rm -i /test/file1</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 常用命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【深度学习】AlexNet</title>
      <link href="2020/12/01/shen-du-xue-xi-alexnet/"/>
      <url>2020/12/01/shen-du-xue-xi-alexnet/</url>
      
        <content type="html"><![CDATA[<h2 id="一、论文总览"><a href="#一、论文总览" class="headerlink" title="一、论文总览"></a>一、论文总览</h2><p><img src="/images/AlexNet/image-20201201224135135.png" alt="论文总览"></p><p><img src="/images/AlexNet/image-20201201225850820.png" alt="研究成果"></p><p>Top1-error  VS  Top5-error：</p><p>Top-1：神经网络返回的最大概率值代表的内容是正确的。</p><p>Top-5：神经网络返回的前5个最大概率值代表的内容中有一个是正确的。</p><h2 id="二、摘要"><a href="#二、摘要" class="headerlink" title="二、摘要"></a>二、摘要</h2><p><img src="/images/AlexNet/image-20201202092304790.png" alt="摘要总结"></p><h2 id="三、AlexNet网络结构"><a href="#三、AlexNet网络结构" class="headerlink" title="三、AlexNet网络结构"></a>三、AlexNet网络结构</h2><ol><li><p>网络连接方式</p><p><img src="/images/AlexNet/image-20201202100140973.png" alt="网络结构"></p></li><li><p>连接参数数据流（维度）计算</p><p><img src="/images/AlexNet/image-20201202102608909.png" alt="特征图参数变化"></p></li><li><p>连接参数量计算</p><p><img src="/images/AlexNet/image-20201202151340508.png" alt="连接参数量Weights"></p><p>上面红线和蓝线，分布标记处理两步卷积参数计算示例</p><p>值得注意的是，AlexNet模型中，第一个全连接层的参数量达到了总数的一半还要多，以至于后续模型构建中在该层进行了不少改进或者丢弃。</p></li></ol><h2 id="四、结构特点"><a href="#四、结构特点" class="headerlink" title="四、结构特点"></a>四、结构特点</h2><p>1.激活函数</p><p><img src="/images/AlexNet/image-20201202191433128.png" alt="激活函数"></p><p>2.Local Response Normalization</p><p><img src="/images/AlexNet/image-20201202191518810.png" alt="侧抑制"></p><p>3.Overlapping Pooling(重叠池化)</p><p>常见池化 VS 重叠池化：</p><p><img src="/images/AlexNet/image-20201202202031693.png" alt=""></p><h2 id="五、训练技巧-减轻过拟合"><a href="#五、训练技巧-减轻过拟合" class="headerlink" title="五、训练技巧(减轻过拟合)"></a>五、训练技巧(减轻过拟合)</h2><p>1.Data Augmentation（数据增强/数据扩充）</p><p>​    法一：通过平移剪裁和水平反射来扩充数据集</p><p><img src="/images/AlexNet/image-20201202205316414.png" alt=""></p><p>​        训练时，从256*256的原始图片中随机裁剪224*224的图片区域，得到照片1；然后，再水平翻转得到图片2；则一张图片可扩充至:</p><script type="math/tex; mode=display">(256-224)^2*2 = 32^2*2=1024*2 = 2048 张</script><p>​        测试时，从原始图片中的四角和中心就，剪裁出5张224*224的图片区域，再翻转得到10张测试区域。</p><p>​    法二：更改训练图像中RGB通道的强度</p><p><img src="/images/AlexNet/image-20201202205445003.png" alt=""></p><p>​        具体而言，我们在整个ImageNet训练集的图像的RGB像素值上使用PCA。对于每个训练图像，我们添加多个通过PCA找到的主成分，大小与相应的特征值成比例，乘以一个随机值，该随机值属于均值为0、标准差为0.1的高斯分布。（从百分之六十多才增加了一个百分点，效果不太明显，后续已不使用）</p><p>2.Dropout(随机失活)</p><p><img src="/images/AlexNet/image-20201202211141396.png" alt=""></p><p>AlexNet的前两个完全连接的层中，我们使用了dropout。如果没有dropout，我们的网络将出现大量的过度拟合。 dropout大约会使收敛所需的迭代次数加倍。</p><h2 id="六、实验结果与分析"><a href="#六、实验结果与分析" class="headerlink" title="六、实验结果与分析"></a>六、实验结果与分析</h2><ol><li><p>ILSVERC-2012比赛</p><p><img src="/images/AlexNet/image-20201202211626687.png" alt="训练模型改进步骤"></p></li><li><p>卷积核可视化</p><p><img src="/images/AlexNet/image-20201202211722723.png" alt="卷积可视化"></p><p> GPU 1上的内核在很大程度上与颜色无关（频率和方向），而GPU 2上的内核在很大程度上是颜色特定的。 这种特殊化发生在每次运行期间，并且与任何特定的随机权重初始化（对GPU进行重新编号的模数）无关。</p><p>问什么论文只对训练好的第一个卷积层的卷积核进行可视化呢？</p><p>原因有二：其一，第一层的卷积核规格为11*11，可视化的结果更为直观，后面的小规格卷积核无法看出效果；其二，第一层提取的是较为底层的特征（颜色，纹理，方向等），越往后续卷积运算，卷积核提取的是更为底层的抽象的高级特征，人类一般直接感知。（这也证实了AlexNet网络随着深度的加深，提取处理越高级抽象的特征）</p></li><li><p>特征相似性</p><p><img src="/images/AlexNet/image-20201202213136133.png" alt="特征的相似性"></p></li></ol><h2 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h2><p><img src="/images/AlexNet/image-20201202215051107.png" alt=""></p><p>启发点可以作用于后续论文写作的参考思想（即引入参考文献点）</p><p><img src="/images/AlexNet/image-20201202215527103.png" alt=""></p><p><img src="/images/AlexNet/image-20201202215817108.png" alt=""></p><p><img src="/images/AlexNet/image-20201202220029435.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 卷积神经网络 </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【目标检测】NMS</title>
      <link href="2020/11/26/mu-biao-jian-ce-nms/"/>
      <url>2020/11/26/mu-biao-jian-ce-nms/</url>
      
        <content type="html"><![CDATA[<h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>非极大值抑制（Non-Maximum Suppression，NMS），顾名思义就是抑制不是极大值的元素，可以理解为局部最大搜索。这个局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小。</p><p>滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高，并且抑制那些分数低的窗口。</p><p><img src="/images/606386-20170826152837558-1289161833.png" alt="人脸检测框重叠例子"></p><h2 id="二、NMS原理"><a href="#二、NMS原理" class="headerlink" title="二、NMS原理"></a>二、NMS原理</h2><p>对于Bounding Box的<strong>集合B</strong>及其对应的<strong>置信度S</strong>,采用下面的计算方式.选择具有最大score的<strong>检测框M</strong>,将其从B集合中移除并加入到最终的<strong>检测结果集合D</strong>中.通常将B中剩余检测框中与M的IoU大于阈值Nt的框从B中移除.重复这个过程,直到B为空.</p><p><strong>重叠率(重叠区域面积比例IOU)阈值</strong></p><p>常用的阈值是 <code>0.3 ~ 0.5</code>.<br>其中用到排序,可以按照右下角的坐标排序或者面积排序,也可以是通过SVM等分类器得到的得分或概率,R-CNN中就是按得分进行的排序.</p><p><img src="/images/606386-20170826153025589-977347485.png" alt="示例图"></p><p>就像上面的图片一样，定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。非极大值抑制的方法是：先假设有6个矩形框，根据分类器的类别分类概率做排序，假设从小到大属于车辆的概率 分别为A、B、C、D、E、F。</p><p>(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;</p><p>(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。</p><p>(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。</p><p>就这样一直重复，找到所有被保留下来的矩形框</p><p><strong>示例分析</strong></p><p>非极大值抑制之前：</p><p><img src="/images/70" alt="before"></p><p>非极大值抑制之后：</p><p><img src="/images/7011" alt="after"></p><hr><p>参考文献：</p><p>[1]  <a href="https://www.cnblogs.com/makefile/p/nms.html">非极大值抑制（Non-Maximum Suppression，NMS） - 康行天下 - 博客园 (cnblogs.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> R-CNN </tag>
            
            <tag> NMS </tag>
            
            <tag> Non-Maximum Suppression </tag>
            
            <tag> 非极大值抑制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【数据预处理】：图像去均值</title>
      <link href="2020/11/26/shu-ju-yu-chu-li-tu-xiang-qu-jun-zhi/"/>
      <url>2020/11/26/shu-ju-yu-chu-li-tu-xiang-qu-jun-zhi/</url>
      
        <content type="html"><![CDATA[<h2 id="1-什么是均值？"><a href="#1-什么是均值？" class="headerlink" title="1.什么是均值？"></a>1.什么是均值？</h2><p>对于每帧图像来说，均值分为两种：<strong>image mean</strong> 和 <strong>pixel mean</strong>。</p><p><strong>image mean：</strong><br>简单的说，读入<strong>一张彩色图像</strong>(N*N*3)，这时候，求出的image_mean是N*N*3个像素的均值，相当于把所有训练集在同一个空间位置（同一张图片）上的像素的对应通道求了均值，也就是caffe里生成的mean.binaryproto文件。</p><p><strong>pixel mean：</strong><br>而pixel mean的话，其实是把训练集里面<strong>所有图片</strong>的所有R通道像素，求了均值，G,B通道类似，也就是不考虑空间位置（单独考虑单个图像）了。所以求出来就是三个数值（R_mean,G_mean,B_mean），所以其实就是把image_mean再求了一次均值。</p><h2 id="2-为什么要去均值？"><a href="#2-为什么要去均值？" class="headerlink" title="2.为什么要去均值？"></a>2.<strong>为什么要去均值？</strong></h2><h3 id="2-1-从主成分分析（PCA）入手解释"><a href="#2-1-从主成分分析（PCA）入手解释" class="headerlink" title="2.1.从主成分分析（PCA）入手解释"></a>2.1.从主成分分析（PCA）入手解释</h3><p>说白了就是<strong>数据特征标准化</strong>。</p><p><strong>特征标准化</strong>指的是（独立地）使得数据的每一个维度具有<strong>零均值和单位方差</strong>。这是归一化中最常见的方法并被广泛地使用（例如，在使用支持向量机（SVM）时，特征标准化常被建议用作预处理的一部分）。在实际应用中，特征标准化的具体做法是：首先计算每一个维度上数据的均值（使用全体数据计算），之后在每一个维度上都减去该均值。下一步便是在数据的每一维度上除以该维度上数据的标准差。</p><p>对于<strong>自然图像</strong>，更多的是做<strong>图像零均值化</strong>，并不需要估计<strong>样本的方差</strong>。这是因为在自然图像上进行训练时，对每一个像素单独估计均值和方差意义不大，因为（理论上）图像任一部分的统计性质都应该和其它部分相同，图像的这种特性被称作平稳性（stationarity）。</p><p>对于图像，这种归一化可以<strong>移除图像的平均亮度值 (intensity)</strong>。很多情况下我们对图像的照度并不感兴趣，而更多地关注其内容，比如在对象识别任务中，图像的整体明亮程度并不会影响图像中存在的是什么物体。这时对每个数据点移除像素的均值是有意义的。</p><h3 id="2-2-从深度学习反向传播计算入手"><a href="#2-2-从深度学习反向传播计算入手" class="headerlink" title="2.2.从深度学习反向传播计算入手"></a>2.2.从深度学习反向传播计算入手</h3><p>了解到基本在deep learning中只要你是使用gradient descent来训练模型的话都要在数据预处理步骤进行数据归一化。当然这也是有一定原因的。</p><p>根据公式</p><p><img src="/images/0" alt=""></p><p>如果输入层 x 很大，在反向传播时候传递到输入层的梯度就会变得很大。梯度大，学习率就得非常小，否则会越过最优。在这种情况下，学习率的选择需要参考输入层数值大小，而直接将数据归一化操作，能很方便的选择学习率。而且受 x 和 w 的影响，各个梯度的数量级不相同，因此，它们需要的学习率数量级也就不相同。</p><h3 id="2-3-图像示例"><a href="#2-3-图像示例" class="headerlink" title="2.3 图像示例"></a>2.3 图像示例</h3><p>很多情况下我们对图像的照度并不感兴趣，而更多地关注其内容，比如在对象识别任务中，图像的整体明亮程度并不会影响图像中存在的是什么物体。这时对每个数据点移除像素的均值是有意义的。而另一个资料显示在每个样本上减去数据的统计平均值可以移除共同的部分，凸显个体差异。其效果如下所示：</p><p><img src="/images/1189532-20181212170428032-122756585.png" alt="图像去均值"></p><p>可以看到天空的纹理被移除了，凸显了汽车和高楼等主要特征。</p><hr><p>参考文献：</p><p>[1] <a href="https://blog.csdn.net/weixin_37251044/article/details/81157344">【数据预处理】：图像去均值：image mean 和 pixel mean_Jack_Kuo的博客-CSDN博客</a></p><p>[2] <a href="https://www.cnblogs.com/Jerry-home/p/10109460.html">深度学习中图像预处理为什么要减去均值？ - Jerry199 - 博客园 (cnblogs.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据预处理 </tag>
            
            <tag> 图像 </tag>
            
            <tag> 去均值 </tag>
            
            <tag> R-CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【目标检测】Selective Search</title>
      <link href="2020/11/26/mu-biao-jian-ce-selective-search/"/>
      <url>2020/11/26/mu-biao-jian-ce-selective-search/</url>
      
        <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>文章名：《物体识别中的选择性搜索方法》</p><p>作者： J.R.R. Uijlings University of Trento, Italy.意大利特伦托大学</p><p>发表： IJCV 2012</p><p>《Selective Search for Object Recognition》这篇论文是J.R.R. Uijlings发表在2012 IJCV上的一篇文章，主要介绍了选择性搜索（Selective Search）的方法。物体识别（Object Recognition），在图像中找到确定一个物体，并找出其为具体位置，经过长时间的发展已经有了不少成就。</p><h2 id="二、摘要"><a href="#二、摘要" class="headerlink" title="二、摘要"></a>二、摘要</h2><p>本文主要介绍物体识别中的一种选择性搜索（Selective Search）方法。</p><p>物体识别，在之前的做法主要是基于穷举搜索（Exhaustive Search）：选择一个窗口扫描整张图像（image），改变窗口的大小，继续扫描整张图像。这种做法是比较原始直观，改变窗口大小，扫描整张图像，非常耗时。若能过滤掉一些无用的box将会节省大量时间。这就是本文中Selective Search(选择性搜索)的优点。</p><p>选择性搜索（Selective Search)综合了穷举搜索（Exhausticve Search)和分割（Segmentation)的方法，意在找到一些可能的目标位置集合。作者将穷举搜索和分割结合起来，采取组合策略保证搜索的多样性，其结果达到平均最好重合率为0.879。能够大幅度降低搜索空间，提高程序效率，减小计算量。</p><h2 id="三、介绍-Introduce"><a href="#三、介绍-Introduce" class="headerlink" title="三、介绍(Introduce)"></a>三、介绍(Introduce)</h2><p>图像（Image）包含的信息非常的丰富，其中的物体（Object）有不同的形状（shape）、尺寸（scale）、颜色（color）、纹理（texture），要想从图像中识别出一个物体非常的难，还要找到物体在图像中的位置，这样就更难了。下图给出了四个例子，来说明物体识别（Object Recognition）的复杂性以及难度。（a）中的场景是一张桌子，桌子上面放了碗，瓶子，还有其他餐具等等。比如要识别“桌子”，我们可能只是指桌子本身，也可能包含其上面的其他物体。这里显示出了图像中不同物体之间是有一定的层次关系的。（b）中给出了两只猫，可以通过纹理（texture）来找到这两只猫，却又需要通过颜色（color）来区分它们。（c）中变色龙和周边颜色接近，可以通过纹理（texture）来区分。（d）中的车辆，我们很容易把车身和车轮看做一个整体，但它们两者之间在纹理（texture）和颜色（color）方面差别都非常地大。</p><p><img src="/images/selectiveSearch/image-20201126160630821.png" alt="待识别图像示例"></p><p>selective search的策略是，既然是不知道尺度是怎样的，那我们就尽可能遍历所有的尺度好了，但是不同于暴力穷举，我们可以先得到小尺度的区域，然后一次次合并得到大的尺寸就好了，这样也符合人类的视觉认知。既然特征很多，那就把我们知道的特征都用上，但是同时也要照顾下计算复杂度，不然和穷举法也没啥区别了。最后还要做的是能够对每个区域进行排序，这样你想要多少个候选我就产生多少个，不然总是产生那么多你也用不完不是吗？好了这就是整篇文章的思路了，那我们一点点去看。</p><h2 id="四、Selective-Search"><a href="#四、Selective-Search" class="headerlink" title="四、Selective Search"></a>四、Selective Search</h2><h3 id="1-Hierarchical-Grouping-Algorithm"><a href="#1-Hierarchical-Grouping-Algorithm" class="headerlink" title="1.Hierarchical Grouping Algorithm"></a>1.Hierarchical Grouping Algorithm</h3><p>图像中区域特征比像素更具代表性，作者使用<strong>Felzenszwalb and Huttenlocher</strong>的方法产生图像初始区域，使用贪心算法对区域进行迭代分组:</p><ol><li>计算所有邻近区域之间的相似性；</li><li>两个最相似的区域被组合在一起；</li><li>计算合并区域和相邻区域的相似度；</li><li>重复2、3过程，直到整个图像变为一个地区。</li></ol><p>在每次迭代中，形成更大的区域并将其添加到区域提议列表中。以自下而上的方式创建从较小的细分segments到较大细分segments的区域提案，如下图。</p><p><img src="/images/selectiveSearch/image-20201126152206641.png" alt="Hierarchical Grouping"></p><p>Hierarchical Grouping Algorithm的具体操作如下图：</p><p><img src="/images/selectiveSearch/image-20201126152357835.png" alt="算法流程"></p><p>简单的对算法的过程叙述一下：</p><p><strong>输入：</strong>图片（三通道）</p><p><strong>输出：</strong>物体位置的可能结果L</p><ol><li>使用 Felzenszwalb and Huttenlocher提出的方法得到初始<strong>分割</strong>区域R={r1,r2,…,rn}；</li><li>初始化相似度集合S=∅；</li><li>计算两两相邻区域之间的<strong>相似度</strong>，将其添加到相似度集合S中；</li><li>从集合S中找出，相似度最大的两个区域 ri 和rj，将其合并成为一个区域 rt，从集合中删去原先与ri和rj相邻区域之间计算的相似度，计算rt与其相邻区域（与ri或rj相邻的区域）的相似度，将其结果加入到相似度集合S中。同时将新区域 rt 添加到区域集合R中；</li><li>获取每个区域的Bounding Boxes L，输出物体位置的可能结果L。</li></ol><h3 id="2-Diversification-Strategies（多元化策略）"><a href="#2-Diversification-Strategies（多元化策略）" class="headerlink" title="2.Diversification Strategies（多元化策略）"></a>2.Diversification Strategies（多元化策略）</h3><p>这个部分讲述作者提到的多样性的一些策略，使得抽样多样化，主要有下面三个不同方面：</p><p>(1)利用各种不同不变性的<strong>色彩空间</strong>；</p><p>(2)采用不同的<strong>相似性度量</strong>；</p><p>(3)通过改变<strong>起始区域</strong>。此部分比较简单，不详细介绍，作者对比了一些初始化区域的方法，发现方法[1]效果最好。</p><ul><li><strong>Colour Spaces.</strong></li></ul><p>考虑到场景、光照条件的不同，作者提出使用八种不变性属性的各种颜色空间应用在Hierarchical Grouping Algorithm。如下表：</p><p><img src="/images/selectiveSearch/image-20201126154238157.png" alt="颜色空间"></p><p>+/-表示部分不变性；1/3表示三个颜色通道中有一个是不变性，以此类推。</p><ul><li><p><strong>Similarity Measures</strong>（颜色相似度衡量）</p><p>$C_i={c_i^1….c_i^n}$表示每一个区域用三通道的颜色直方图表示，每个颜色通道的25 bins的直方图，这样每个区域都可以得到一个n=75维的向量。使用L1-norm标准化后，用下式计算区域间的相似度。</p></li></ul><p><img src="/images/selectiveSearch/image-20201126154739485.png" alt=""></p><p>合并区域的颜色直方图计算如下：</p><p><img src="/images/selectiveSearch/image-20201126154808810.png" alt=""></p><p><img src="/images/selectiveSearch/image-20201126154829344.png" alt=""></p><ul><li><strong>纹理相似度衡量</strong></li></ul><p>论文采用SIFT-Like特征，具体操作：采用方差为1的高斯分布对每个颜色通道的8个不同方向做梯度统计，然后将统计结果（尺寸与区域大小一致）以bins=10计算直方图。直方图区间数为8<em>3</em>10=240。</p><p><img src="/images/selectiveSearch/image-20201126154857606.png" alt=""></p><p> $T_i={t_i^1…t_i^n}$表示每一个区域的纹理直方图，有240维。</p><ul><li><strong>尺度相似度衡量</strong></li></ul><p>为了保证区域合并操作的尺度较为均匀，使用如下公式，使用尺寸相似度，目的是尽量让小的区域先合并。</p><p><img src="/images/selectiveSearch/image-20201126155044692.png" alt=""></p><p>size(im)是指区域中的尺寸(以像素为单位)</p><ul><li><strong>形状重合度衡量</strong></li></ul><p>为了衡量两个区域是否更加重合，合并后区域的Bounding Box越小，其重合度越高，公式如下。</p><p><img src="/images/selectiveSearch/image-20201126155126101.png" alt=""></p><ul><li><strong>最终的相似度衡量</strong></li></ul><p><img src="/images/selectiveSearch/image-20201126155205039.png" alt=""></p><h2 id="五、使用选择性搜索进行物体识别"><a href="#五、使用选择性搜索进行物体识别" class="headerlink" title="五、使用选择性搜索进行物体识别"></a>五、使用选择性搜索进行物体识别</h2><p>利用Selective Search对图像进行处理后，形成了可能的目标区域集L。下一步集合Sift或者CNN或者bag-of–words等一些特征提取方法，对每一个可能的目标区域进行特征处理，形成该区域的特征向量V。然后将V送入训练好的SVM多分类器进行判别。在穷举搜索（Exhaustive Search）方法中，寻找合适的位置假设需要花费大量的时间，能选择用于物体识别的特征不能太复杂，只能使用一些耗时少的特征。由于选择搜索 （Selective Search）在得到物体的位置假设这一步效率较高，其可以采用诸如SIFT等运算量大，表示能力强的特征。</p><h3 id="1-特征生成"><a href="#1-特征生成" class="headerlink" title="1.特征生成"></a>1.特征生成</h3><p>系统在实现过程中，使用color-SIFT特征以及spatial pyramid divsion方法。在一个尺度下σ=1.2下抽样提取特征。使用SIFT、Extended OpponentSIFT、RGB-SIFT特征，在四层金字塔模型 1×1、2×2、3×3、4×4，提取特征，可以得到一个维的特征向量。</p><h3 id="2-训练过程"><a href="#2-训练过程" class="headerlink" title="2.训练过程"></a>2.训练过程</h3><p>训练方法采用SVM。首先选择包含真实结果（ground truth）的物体窗口作为正样本（positive examples），选择与正样本窗口重叠20%~50%的窗口作为负样本（negative examples）。在选择样本的过程中剔除彼此重叠70%的负样本，这样可以提供一个较好的初始化结果。在重复迭代过程中加入hard negative examples（得分很高的负样本）。其训练过程框图如下：</p><p><img src="/images/selectiveSearch/image-20201126155232931.png" alt="算法总框架"></p><p><img src="/images/selectiveSearch/image-20201126155445548.png" alt="算法流程图"></p><h2 id="六、算法评估"><a href="#六、算法评估" class="headerlink" title="六、算法评估"></a>六、算法评估</h2><p>很自然地，通过算法计算得到的包含物体的Bounding Boxes与真实情况（ground truth）的窗口重叠越多，那么算法性能就越好。这是使用的指标是平均最高重叠率ABO（Average Best Overlap）。对于每个固定的类别c，每个真实情况（ground truth）表示为 ，令计算得到的位置假设L中的每个值l，那么 ABO的公式表达为：</p><p><img src="/images/selectiveSearch/image-20201126155556177.png" alt=""></p><p>重叠率的计算方式：</p><p><img src="/images/selectiveSearch/image-20201126155618022.png" alt=""></p><p>上面结果给出的是一个类别的ABO，对于所有类别下的性能评价，很自然就是使用所有类别的ABO的平均值MABO（Mean Average Best Overlap）来评价。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> R-CNN </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 目标检测 </tag>
            
            <tag> selective search </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【机器学习】sklearn中的PCA</title>
      <link href="2020/11/19/ji-qi-xue-xi-sklearn-zhong-de-pca/"/>
      <url>2020/11/19/ji-qi-xue-xi-sklearn-zhong-de-pca/</url>
      
        <content type="html"><![CDATA[<h2 id="PCA导包-amp-实例化"><a href="#PCA导包-amp-实例化" class="headerlink" title="PCA导包&amp;实例化"></a>PCA导包&amp;实例化</h2><p>主成分分析（Principal Components Analysis），简称PCA，是一种数据降维技术，用于数据预处理。</p><p>PCA的一般步骤是：先对原始数据零均值化，然后求协方差矩阵，接着对协方差矩阵求特征向量和特征值，这些特征向量组成了新的特征空间。</p><pre class=" language-lang-python"><code class="language-lang-python">#导包语句from sklearn.decomposition import PCA#实例化sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False)</code></pre><p><strong>参数：</strong></p><p><strong>n_components:</strong>  </p><blockquote><p>意义：PCA算法中所要保留的主成分个数n，也即保留下来的特征个数n</p><p>类型：int 或者 string，缺省时默认为None，所有成分被保留。</p><p>​     赋值为int，比如n_components=1，将把原始数据降到一个维度。</p><p>​     赋值为string，比如n_components=’mle’，将自动选取特征个数n，使得满足所要求的方差百分比。</p></blockquote><p><strong>copy:</strong></p><blockquote><p>类型：bool，True或者False，缺省时默认为True。</p><p>意义：表示是否在运行算法时，将原始训练数据复制一份。若为True，则运行PCA算法后，原始训练数据的值不       会有任何改变，因为是在原始数据的副本上进行运算；若为False，则运行PCA算法后，原始训练数据的值会改，因为是在原始数据上进行降维计算。</p></blockquote><p><strong>whiten:</strong></p><blockquote><p>类型：bool，缺省时默认为False</p><p>意义：白化，使得每个特征具有相同的方差。</p></blockquote><h2 id="PCA属性"><a href="#PCA属性" class="headerlink" title="PCA属性"></a>PCA属性</h2><ul><li><strong>components_</strong> ：返回具有最大方差的成分。</li><li><strong>explained<em>variance_ratio</em></strong>：返回所保留的<strong>n<em>components</em></strong>个成分各自的方差百分比。</li><li><strong>n<em>components</em></strong>：返回所保留的成分个数n。</li><li><strong>mean_</strong>：</li><li><strong>noise<em>variance</em>：</strong></li></ul><h2 id="PCA方法"><a href="#PCA方法" class="headerlink" title="PCA方法"></a>PCA方法</h2><p><strong>1、fit(X,y=None)</strong></p><p>fit(X)，表示用数据X来<strong>训练</strong>PCA模型，仅仅是训练模型，不对数据进行降维</p><p>函数返回值：调用fit方法的对象本身。比如pca.fit(X)，表示用X对pca这个对象进行训练。</p><p><em>拓展：fit()可以说是scikit-learn中通用的方法，每个需要训练的算法都会有fit()方法，它其实就是算法中的“训练”这一步骤。因为PCA是无监督学习算法，此处y自然等于None。</em></p><p><strong>2、fit_transform(X)</strong></p><p>用X来<strong>训练</strong>PCA模型，同时返回<strong>降维</strong>后的数据。</p><p>newX=pca.fit_transform(X)，newX就是降维后的数据。</p><p><strong>3、inverse_transform()</strong></p><p>将降维后的数据转换成原始数据，X=pca.inverse_transform(newX)</p><p><strong>4、transform(X)</strong></p><p>将数据X转换成<strong>降维</strong>后的数据。当模型训练好后，对于新输入的数据，都可以用transform方法来降维。</p><p>此外，还有get_covariance()、get_precision()、get_params(deep=True)、score(X, y=None)等方法，以后用到再补充吧。</p><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><pre class=" language-lang-python"><code class="language-lang-python">import numpy as npfrom sklearn.decomposition import PCAX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])pca = PCA(n_components=2)        #降至二维数据newX = pca.fit_transform(X)      #等价于pca.fit(X) 再  pca.transform(X)invX = pca.inverse_transform(X)  #将降维后的数据转换成原始数据print(X)    [[-1 -1]     [-2 -1]     [-3 -2]     [ 1  1]     [ 2  1]     [ 3  2]]print(newX）    array([[ 1.38340578,  0.2935787 ],           [ 2.22189802, -0.25133484],           [ 3.6053038 ,  0.04224385],           [-1.38340578, -0.2935787 ],           [-2.22189802,  0.25133484],           [-3.6053038 , -0.04224385]])print(invX)    [[-1 -1]     [-2 -1]     [-3 -2]     [ 1  1]     [ 2  1]     [ 3  2]]print(pca.explained_variance_ratio_)   #返回所保留的**n_components_**个成分各自的方差百分比。    [ 0.99244289  0.00755711]</code></pre><p>我们所训练的pca对象的n_components值为2，即保留2个特征，第一个特征占所有特征的方差百分比为0.99244289，意味着几乎保留了所有的信息。即第一个特征可以99.24%表达整个数据集，因此我们可以降到1维：</p><pre class=" language-lang-python"><code class="language-lang-python">pca = PCA(n_components=1)newX = pca.fit_transform(X)print(pca.explained_variance_ratio_)[ 0.99244289]</code></pre><h2 id="PCA理论推导过程"><a href="#PCA理论推导过程" class="headerlink" title="PCA理论推导过程"></a>PCA理论推导过程</h2><p>(参考文章地址：<a href="http://blog.codinglabs.org/articles/pca-tutorial.html">http://blog.codinglabs.org/articles/pca-tutorial.html</a>)</p><p>实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维。</p><p>降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。</p><p>例如上面淘宝店铺的数据，从经验我们可以知道，<strong>“浏览量”</strong>和<strong>“访客数”</strong>往往具有较强的相关关系，而<strong>“下单数”</strong>和<strong>“成交数”</strong>也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词，可以直观理解为“当某一天这个店铺的浏览量较高（或较低）时，我们应该很大程度上认为这天的访客数也较高（或较低）”。这种情况表明，如果我们删除浏览量或访客数其中一个指标，我们应该期待并不会丢失太多信息。因此我们可以删除一个，以降低机器学习算法的复杂度。</p><p>上面给出的是降维的朴素思想描述，可以有助于直观理解降维的动机和可行性，但并不具有操作指导意义。例如，我们到底删除哪一列损失的信息才最小？亦或根本不是单纯删除几列，而是通过某些变换将原始数据变为更少的列但又使得丢失的信息最小？到底如何度量丢失信息的多少？如何根据原始数据决定具体的降维操作步骤？</p><p>要回答上面的问题，就要对降维问题进行数学化和形式化的讨论。而PCA是一种具有严格数学基础并且已被广泛采用的降维方法。下面我不会直接描述PCA，而是通过逐步分析问题，让我们一起重新“发明”一遍PCA。</p><h3 id="几个数学概念"><a href="#几个数学概念" class="headerlink" title="几个数学概念"></a>几个数学概念</h3><ol><li><p>内积，投影与基</p><p><strong>内积</strong>是一中向量运算规则，相同维度的向量被定义为：</p><p><img src="/images/DeepLearning/image-20201119175755022.png" alt=""></p><p>计算方式容易理解，但是实际几何意义却不够直观，于是，引入内积的另一种定义方式：</p><p><img src="/images/image-20201120094011541.png" alt=""></p><p>对于第二种定义，暂时还不明确其真实几何意义，先补一点数学知识。假设A和B是两个n维向量，我们知道n维向量可以等价表示为n维空间中的一条从原点发射的有向线段，为了简单起见我们假设A和B均为二维向量，则A=(x1,y1)，B=(x2,y2)。则在二维平面上A和B可以用两条发自原点的有向线段表示，见下图：</p><p><img src="/images/image-20201120094219873.png" alt="直角坐标系"></p><p>好，现在我们从A点向B所在直线引一条垂线。我们知道垂线与B的交点叫做A在B上的投影，再设A与B的夹角是a，则投影的矢量长度为|A|cos(a)，其中$|A|=\sqrt{x_1^2+y_1^2}$是向量A的模，也就是A线段的标量长度。</p><p>现在事情似乎是有点眉目了：A与B的内积等于A到B的投影长度乘以B的模。再进一步，如果我们假设B的模为1，即让|B|=1|B|=1，那么就变成了：</p><script type="math/tex; mode=display">A\cdot B = |A|cos(a)</script><p>也就是说，<strong>设向量B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度</strong>！这就是内积的一种几何解释，也是我们得到的第一个重要结论。</p><hr><p><strong>投影</strong>是一个简单的数学概念，看图说话</p><p><img src="/images/image-20201120095454939.png" alt="投影是什么？"></p><p>在代数表示方面，我们经常用线段终点的点坐标表示向量，例如上面的向量可以表示为(3,2)，这是我们再熟悉不过的向量表示。</p><p>不过我们常常忽略，<strong>只有一个(3,2)本身是不能够精确表示一个向量的</strong>。我们仔细看一下，<strong>这里的3实际表示的是向量在x轴上的投影值是3，在y轴上的投影值是2。</strong>也就是说我们其实隐式引入了一个定义：以x轴和y轴上正方向长度为1的向量为标准。那么一个向量(3,2)实际是说在x轴投影为3而y轴的投影为2。注意投影是一个矢量，所以可以为负。</p><p>所以，<strong>要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了</strong>。只不过我们经常省略第一步，而默认以(1,0)和(0,1)为基。</p><p>然后我们计算向量在不同基上的投影，我们可以通过如下公式转化计算：</p><script type="math/tex; mode=display">|A|cos(a) = A\cdot B = a_1b_1 + a_2b_2 + a_3b_3 + ... + a_nb_n</script><p>例如，(1,1)和(-1,1)也可以成为一组基。一般来说，我们希望基的模是1，因为从内积的意义可以看到，如果基的模是1，那么就可以方便的用向量点乘基而直接获得其在新基上的坐标了！实际上，对应任何一个向量我们总可以找到其同方向上模为1的向量，只要让两个分量分别除以模就好了。</p><p><img src="/images/image-20201120095917919.png" alt=""></p><p>现在，我们想获得(3,2)在新基上的坐标，即在两个方向上的投影矢量值，那么根据内积的几何意义，我们只要分别计算(3,2)和两个基的内积，不难得到新的坐标为$(\frac{5}{\sqrt{2}},-\frac{1}{\sqrt{2}})$。下图给出了新的基以及(3,2)在新基上坐标值的示意图：</p><p><img src="/images/image-20201120100159072.png" alt="坐标变化"></p></li><li><p>基变换的矩阵表示</p></li></ol><ol><li><p>协方差矩阵及优化目标</p></li><li><p>方差</p></li><li><p>协方差</p></li><li><p>协方差矩阵</p></li><li><p>协方差矩阵对角化</p></li></ol><h3 id="算法推导"><a href="#算法推导" class="headerlink" title="算法推导"></a>算法推导</h3>]]></content>
      
      
      <categories>
          
          <category> sklearn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> sklearn </tag>
            
            <tag> PCA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【深度学习】碎碎念</title>
      <link href="2020/11/16/shen-du-xue-xi-sui-sui-nian/"/>
      <url>2020/11/16/shen-du-xue-xi-sui-sui-nian/</url>
      
        <content type="html"><![CDATA[<h2 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h2><h3 id="1、简述"><a href="#1、简述" class="headerlink" title="1、简述"></a>1、简述</h3><p><strong>梯度消失</strong>主要表象为：在神经网络模型中，<strong>靠近输入层的hidden layer1的权值更新会变得很慢，导致靠近输入层的隐藏层权值几乎不变</strong>，因此，靠近输入层的隐藏层输出值非常接近初始值且不变化，使得该神经网路模型仅等价于只有后几层的隐藏层在学习。</p><p><strong>梯度爆炸</strong>主要表征为：当初始的权值过大，在进行逆向传播时，各层之间的梯度值都会较大，<strong>由于连续累乘，导致靠近输入层的隐含层，梯度值变化幅度的较大</strong>，极端情况下，权重变得非常大，以至于溢出，得到NaN值</p><p><img src="/images/20190727204639532.png" alt="神经网络实例"></p><h3 id="2、原因"><a href="#2、原因" class="headerlink" title="2、原因"></a>2、原因</h3><p><strong>为什么会出现梯度消失呢？</strong></p><p>因为通常神经网络所用的激活函数是sigmoid函数，这个函数有个特点，就是能将负无穷到正无穷的数映射到0和1之间，并且对这个函数求导的结果是$f^{‘}(x) = f(x)(1-f(x))$。因此<strong>两个0到1之间的数相乘</strong>，得到的结果就会变得很小了。神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后一层产生的偏差就因为<strong>乘了很多的小于1的数而越来越小，最终就会变为0</strong>，从而导致靠近输入层的权重没有更新，这就是<strong>梯度消失</strong>。</p><ul><li><p>深度网络角度</p><p><img src="/images/image-20201118113731407.png" alt="数学理论推导"></p><p>从上图严格的数学推导中可以看出，最终参数的变化值主要由<strong>红色三角形部分</strong>和<strong>黑色三角形部分</strong>组成，由于激活函数采用的是<strong>sigmoid函数</strong>，因此，$|f(x)|&lt;1$；分如下两种情况：</p><ol><li>$\omega$比较小时，<strong>红色三角形部分</strong>就相当于很多个小于一的数进行连乘，层次越深参数变化值越小，所谓<strong>梯度消失</strong></li><li>$\omega$比较大时，<strong>黑色三角形部分</strong>就相对于很多个非常大的数进行连乘，层次越深，连乘的$\omega$数量越多，参数变化值越大，所谓<strong>梯度爆炸</strong></li></ol><p>因此，梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足。如下图所示，对于四个隐层的网络来说，第四隐藏层比第一隐藏层的更新速度慢了两个数量级：</p><p><img src="/images/Hidden_layer" alt=""></p></li><li><p>激活函数角度</p><p>上文中提到计算权值更新信息的时候需要计算前层偏导信息，因此如果激活函数选择不合适，比如使用sigmoid，梯度消失就会很明显了，下图第一张是sigmoid的损失函数图，下图第二张是其导数的图像，如果使用sigmoid作为损失函数，其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失。</p><p><img src="/images/sigmoid" alt=""></p><p><img src="/images/image-20201202153040971.png" alt=""></p><p>可以看到，在sigmoid函数的尾端，梯度非常之小，容易造成梯度消失；而ReLU梯度则一直保持为1</p><p>以上是从数学图像上分析出的结果，接下来进行严格的数学推导：</p><p>sigmoid函数数学表达式为：</p><script type="math/tex; mode=display">\sigma(x)=\frac{1}{1+e^{-x}}</script><script type="math/tex; mode=display">\sigma^{'}(x) = \sigma(x)(1-\sigma(x))=-(\sigma(x)-\frac{1}{2})^2+\frac{1}{4}</script><p>可见sigmoid的导数的最大值为1/4,<strong>通常来说当激活函数是sigmoid时,梯度消失比梯度爆炸更容易发生。</strong></p><p>同理，tanh作为激活函数，它的导数图如下，可以看出，tanh比sigmoid要好一些，但是它的导数仍然是小于1的。tanh数学表达为：</p><p><img src="/images/23131" alt=""></p><p><img src="/images/2324435" alt=""></p></li></ul><h3 id="3、解决方案"><a href="#3、解决方案" class="headerlink" title="3、解决方案"></a>3、解决方案</h3><p><strong>如何确定是否出现梯度爆炸？</strong>如：</p><ol><li>模型无法从训练数据中获得更新（如低损失）。</li><li>模型不稳定，导致更新过程中的损失出现显著变化。</li><li>训练过程中，模型损失变成 NaN。</li></ol><p><strong>1. 重新设计网络模型</strong></p><p>​    梯度爆炸可以通过重新设计层数更少的网络来解决。使用更小的批尺寸对网络训练也有好处。另外也许是<strong>学习率过大</strong>导致的问题，减小学习率。</p><p><strong>2. 使用 ReLU 激活函数</strong></p><p>   <strong>梯度爆炸的发生可能是因为激活函数</strong>，如之前很流行的 Sigmoid 和 Tanh 函数。使用 ReLU 激活函数可以减少梯度爆炸。采用 ReLU 激活函数是最适合隐藏层的，是目前使用最多的激活函数。</p><p>​                         <img src="/images/45345" alt=""></p><p>   relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。</p><p>  relu的主要贡献在于：</p><ol><li>— 解决了梯度消失、爆炸的问题</li><li>— 计算方便，计算速度快， 加速了网络的训练</li></ol><p>同时也存在一些缺点：</p><ol><li>由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）</li><li>输出不是以0为中心的</li></ol><p><strong>leakrelu</strong>                 <img src="/images/dggfdgd" alt=""></p><p> leak relu就是为了解决relu的0区间带来的影响，而且包含了relu的所有优点，其数学表达为：</p><p>​                        <img src="/images/20190422164503917" alt=""></p><p>其中k是leak系数，一般选择0.01或者0.02，或者通过学习而来。</p><p><strong>elu</strong></p><p>elu激活函数也是为了解决relu的0区间带来的影响，其数学表达为：</p><p>​                            <img src="/images/20190727205122374" alt=""></p><p>其函数及其导数数学形式为：</p><p><img src="/images/24325547567" alt=""></p><p>但是elu相对于leakrelu来说，计算要更耗时间一些。</p><p><strong>3. 使用长短期记忆网络</strong></p><p>   在循环神经网络中，梯度爆炸的发生可能是因为某种网络的训练本身就存在不稳定性，使用长短期记忆<strong>（LSTM）</strong>单元和相关的门类型神经元结构可以减少梯度爆炸问题。采用 LSTM 单元是适合循环神经网络的序列预测的最新最好实践。</p><p><strong>4. 使用梯度截断（Gradient Clipping）</strong></p><p>​    梯度剪切这个方案主要是<strong>针对梯度爆炸</strong>提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。</p><p><strong>5. 使用权重正则化（Weight Regularization）</strong></p><p>   如果梯度爆炸仍然存在，可以尝试另一种方法，即检查网络权重的大小，并惩罚产生较大权重值的损失函数。该过程被称为权重正则化，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）。比如在tensorflow中，若搭建网络的时候已经设置了正则化参数，则调用以下代码可以直接计算出正则损失：</p><pre class=" language-lang-python"><code class="language-lang-python">regularization_loss = tf.add_n(tf.losses.get_regularization_losses(scope='my_resnet_50'))</code></pre><p>正则化是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式：</p><p>​                      <img src="/images/2019042216393113" alt=""><br>其中，α 是指正则项系数，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。</p><p>注：事实上，在深度神经网络中，往往是梯度消失出现的更多一些。</p><p><strong>6、预训练加finetunning</strong></p><p>​    其基本思想是每次训练一层隐藏层节点，将上一层隐藏层的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，这就是逐层预训练。在预训练完成后，再对整个网络进行“微调”（fine-tunning）。Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。<strong>此思想相当于是先寻找局部最优，然后整合起来寻找全局最优</strong>，此方法有一定的好处，但是目前应用的不是很多了。现在基本都是直接拿imagenet的预训练模型直接进行finetunning。</p><p><strong>7、批量归一化</strong></p><p>   Batchnorm具有加速网络收敛速度，提升训练稳定性的效果，<strong>Batchnorm本质上是解决反向传播过程中的梯度问题</strong>。batchnorm全名是batch normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化保证网络的稳定性。</p><p>from：<a href="https://blog.csdn.net/qq_30815237/article/details/88933753">Batch Normalization批量归一化</a></p><p><strong>8、残差结构</strong></p><p>   残差网络的出现导致了image net比赛的终结，自从残差提出后，几乎所有的深度网络都离不开残差的身影，相比较之前的几层，几十层的深度网络，残差可以很轻松的构建几百层，一千多层的网络而不用担心梯度消失过快的问题，<strong>原因就在于残差的捷径（shortcut）部分，残差网络通过加入 shortcut connections，变得更加容易被优化。</strong>包含一个 shortcut connection 的几层网络被称为一个残差块（residual block），如下图所示：<br>                <img src="/images/65867879" alt=""><br>   相比较于以前网络的直来直去结构，残差中有很多这样的跨层连接结构，这样的结构在反向传播中具有很大的好处，见下式：<br>            <img src="/images/20190727205433298" alt=""></p><p>   式子的第一个因子<img src="/images/20190422171703141" alt="">表示的损失函数到达 L的梯度，<strong>小括号中的1表明短路机制可以无损地传播梯度，</strong>而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。</p><h2 id="Python函数-range-和arange"><a href="#Python函数-range-和arange" class="headerlink" title="Python函数 range()和arange()"></a>Python函数 range()和arange()</h2><ul><li>range(start, end, step)，返回一个list对象，起始值为start，终止值为end，但不含终止值，步长为step。只能创建int型list。</li><li>arange(start, end, step)，与range()类似，但是返回一个array对象。需要引入import numpy as np，并且arange可以使用float型数据。</li></ul><pre class=" language-lang-python"><code class="language-lang-python">>>> import numpy as np>>> range(1,10,2)range(1, 10, 2)  #返回的是对象，需要通过list函数转化为[1,3,5,7,9]>>> np.arange(1,10,2)array([1, 3, 5, 7, 9])>>> range(1,5,0.5)Traceback (most recent call last):  File "<stdin>", line 1, in <module>TypeError: range() integer step argument expected, got float. #range中只能包含int类型>>> np.arange(1,5,0.5)array([ 1. ,  1.5,  2. ,  2.5,  3. ,  3.5,  4. ,  4.5])  #np.arange中可以包含float类型</code></pre>]]></content>
      
      
      <categories>
          
          <category> 带整理的笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
            <tag> 草稿 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【数据集】ImageNet</title>
      <link href="2020/11/16/shu-ju-ji-imagenet/"/>
      <url>2020/11/16/shu-ju-ji-imagenet/</url>
      
        <content type="html"><![CDATA[<h2 id="ImageNet介绍"><a href="#ImageNet介绍" class="headerlink" title="ImageNet介绍"></a>ImageNet介绍</h2><p>ImageNet 是一个计算机视觉系统识别项目， 是目前世界上图像识别最大的数据库。是美国斯坦福的计算机科学家，模拟人类的识别系统建立的。能够从图片识别物体。ImageNet是一个非常有前景的研究项目，未来用在机器人身上，就可以直接辨认物品和人了。</p><p><img src="/images/ImageNet.png" alt="ImageNet介绍"></p><h2 id="ImageNet数据集的意义"><a href="#ImageNet数据集的意义" class="headerlink" title="ImageNet数据集的意义"></a>ImageNet数据集的意义</h2><ul><li>ImageNet拥有用于分类、定位和检测任务评估的数据。 </li><li>与分类数据类似，定位任务有1000个类别。准确率是根据最高五项检测结果计算出来的。 </li><li>所有图像中至少有一个边框。对200个目标的检测问题有470000个图像，平均每个图像有1.1个目标。 </li></ul><h2 id="ImageNet数据集与ILSVRC竞赛的关系"><a href="#ImageNet数据集与ILSVRC竞赛的关系" class="headerlink" title="ImageNet数据集与ILSVRC竞赛的关系"></a>ImageNet数据集与ILSVRC竞赛的关系</h2><p>​    ILSVRC使用ImageNet的一个子集，总共有大约120万个训练图像，50,000个验证图像，以及150,000个测试图像；1000类别标记。<br>​    Large Scale Visual Recognition Challenge 即ILSVRC(2012~2017)比赛，是基于该数据集的1000个类别的比赛。训练集120万张图片。 从2010年起， 每年ImageNet 的项目组织都会举办一场基于ImageNet 的大规模视觉识别竞赛（ <strong><em>lmageNet Large Scale VisualRecognition Challenge</em></strong> , ILSVRC ） 。在ILSVRC 竞赛中诞生了许多成功的图像识别方法，其中很多是深度学习方法， 它们在赛后又会得到进一步发展与应用。<br>   可以说， ImageNet 数据集和ILSVRC 竞赛大大促进了计算机视觉技术， 乃至深度学习的发展， 在深度学习的浪潮中占有举足轻重的地位。</p><h2 id="与CIFAR-10数据集比较"><a href="#与CIFAR-10数据集比较" class="headerlink" title="与CIFAR-10数据集比较"></a>与CIFAR-10数据集比较</h2><p>   相比CIFAR-10 , ImageNet 数据集图片数量更多， 分辨率更高，含有的类别更多（高上干个图像类别），图片中含高更多的无关噪声和变化，因此识别难度比CIFAR-10 高得多。</p><h2 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h2><p>J. Deng, W. Dong, R. Socher, L.J. Li, Kai Li, and Li Fei-Fei （2009）: <strong><em>ImageNet: A large-scale hierarchical image database</em></strong>. In IEEE Conference on Computer Vision and Pattern Recognition, 2009. CVPR 2009. 248 – 255.</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据集介绍 </tag>
            
            <tag> 计算机视觉系统 </tag>
            
            <tag> 图像识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【深度学习】神经网络-CNN</title>
      <link href="2020/11/16/shen-du-xue-xi-shen-jing-wang-luo-cnn/"/>
      <url>2020/11/16/shen-du-xue-xi-shen-jing-wang-luo-cnn/</url>
      
        <content type="html"><![CDATA[<h1 id="学习结构概述"><a href="#学习结构概述" class="headerlink" title="学习结构概述"></a>学习结构概述</h1><p><img src="/images/image-20201116095100679.png" alt="模型学习结构图"></p><h1 id="CNN原理"><a href="#CNN原理" class="headerlink" title="CNN原理"></a>CNN原理</h1><h2 id="1-数据输入层（数据预处理）"><a href="#1-数据输入层（数据预处理）" class="headerlink" title="1.数据输入层（数据预处理）"></a>1.数据输入层（数据预处理）</h2><p>有三种常见的数据预处理方法，去均值（demean），归一化，PCA/白化，对于CNN来说，一般只会做去均值+归一化，尤其是我们输入的是图像数据，我们往往只做去均值，这是为了保持原图像数据的完整性，避免失真。</p><p><img src="/images/数据预处理" alt="3种常见的数据预处理方式"></p><p><img src="/images/1569451-20190608152240913-1523305295.png" alt="数据去均值&amp;归一化"></p><h2 id="2-卷积介绍"><a href="#2-卷积介绍" class="headerlink" title="2.卷积介绍"></a>2.卷积介绍</h2><p>深度学习上使用的<strong>卷积</strong>与数学意义上的<strong>卷积</strong>，有一点不同。数学上，为了卷积的一些运算特性，在进行卷积运算时，<strong>会先将卷积核沿$y=x$轴进行对称翻转</strong>；但是，在深度学习中，数学上的运算特性作用不大，故省去了镜像翻转的这一步，直接进行点积操作。</p><p><img src="/images/神经网络CNN/image-20201124105539221.png" alt="三维卷积"></p><h2 id="3-单层卷积网络"><a href="#3-单层卷积网络" class="headerlink" title="3.单层卷积网络"></a>3.单层卷积网络</h2><h3 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h3><p><img src="/images/神经网络CNN/image-20201124113414172.png" alt="单层卷积网络实例"></p><p>上图所示，卷积核充当了传统神经网络中权重$\omega$的作用，然后通过python的广播机制对每个卷积预算后的矩阵加上一个偏执常量。（广播机制：python会自动将单个元素与前面的矩阵进行复制扩充对齐）</p><p>当卷积核不断增加时，输出的结果数据规格也在变化(假设为：M*N*K)，其中K就代表这卷积核数，即特征数。（因此，就可以根据分类种类，选取不同的卷积核数，使得输出的结果能够最后和分类种类数对齐。）</p><p>在卷积神经网络模型中，决定模型参数量的，主要是看卷积核数，与输入的数据量大小无关。（例如：单层卷积网络模型中有10个3*3*3的卷积核，则其参数量为：(3*3*3+1)*10 = 280，其中加一是考虑到偏执常量。这就是卷积设计网络的<strong>“避免过拟合”</strong>特征。）</p><h3 id="维度-amp-标号总结"><a href="#维度-amp-标号总结" class="headerlink" title="维度&amp;标号总结"></a>维度&amp;标号总结</h3><p><img src="/images/神经网络CNN/image-20201124120705437.png" alt="标号总结"></p><p>其中，padding表示添加的外围边界宽度，filter表示卷积核，Activations表示激活函数的输入，bias表示偏执常量(为了方便计算，编程时采用四维张量)</p><h3 id="卷积网络实例1"><a href="#卷积网络实例1" class="headerlink" title="卷积网络实例1"></a>卷积网络实例1</h3><p><img src="/images/神经网络CNN/image-20201124121650285.png" alt="卷积网络实例"></p><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p><img src="/images/神经网络CNN/image-20201124121941815.png" alt="最大池化层"></p><h3 id="卷积网络实例2"><a href="#卷积网络实例2" class="headerlink" title="卷积网络实例2"></a>卷积网络实例2</h3><p><img src="/images/神经网络CNN/image-20201124122422964.png" alt="卷积网络实例"></p><p>随着网络深度的加深，数据的宽度和高度都会逐渐减小</p><p><img src="/images/神经网络CNN/image-20201124122628962.png" alt="参数维度"></p><h2 id="3-为什么要使用卷积网络（优势）"><a href="#3-为什么要使用卷积网络（优势）" class="headerlink" title="3.为什么要使用卷积网络（优势）"></a>3.为什么要使用卷积网络（优势）</h2><p>先看张图</p><p><img src="/images/神经网络CNN/image-20201124155002461.png" alt="卷积网络和传统神经网络"></p><p>从上图可以看出，传统网络中，一层含3072个神经元与一层含4704个神经元之间的参数量达到了14000；而在卷积网络中，仅需6个5*5*3维的卷积核即可，实际参数量为$(5<em>5</em>3+1)*6 = 456$ （图上计算有误）。</p><p>为什么卷积网络可以减少权重变量？主要是依靠以下两个特性</p><h3 id="权值共享"><a href="#权值共享" class="headerlink" title="权值共享"></a>权值共享</h3><p><img src="/images/神经网络CNN/image-20201124160253715.png" alt="权值共享实例"></p><p>在输入图像的不同区域使用相同的参数（卷积核），9个参数将会有16个输出</p><h3 id="稀疏连接"><a href="#稀疏连接" class="headerlink" title="稀疏连接"></a>稀疏连接</h3><p><img src="/images/神经网络CNN/image-20201124161033315.png" alt="稀疏连接"></p><p>右边的每个神经元节点仅与左边9个(部分)神经元连接</p><h1 id="CNN实例"><a href="#CNN实例" class="headerlink" title="CNN实例"></a>CNN实例</h1><h2 id="目标探测介绍"><a href="#目标探测介绍" class="headerlink" title="目标探测介绍"></a>目标探测介绍</h2><h3 id="通俗理解"><a href="#通俗理解" class="headerlink" title="通俗理解"></a>通俗理解</h3><p>模型知道某个物品在图像在哪的算法就是目标检测。</p><h3 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h3><p>根据分类对象个数的不同分为单目标和多目标任务</p><p><img src="/images/image-20201116100137605.png" alt="目标检测任务分类"></p><h3 id="直接思路"><a href="#直接思路" class="headerlink" title="直接思路"></a>直接思路</h3><p>转化为分类，回归问题</p><p>使用不同的候选区域框，取不同的区域，逐个判断是否为需要检测的目标对象</p><p><img src="/images/image-20201116100628415.png" alt="目标检测直接思路"></p><h2 id="传统方法——DPM"><a href="#传统方法——DPM" class="headerlink" title="传统方法——DPM"></a>传统方法——DPM</h2><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>提取图像特征，制作出激励模板，在原始图像滑动计算，得到激励效果图，根据激励分布确定目标位置</p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>直观简单</li><li>运算速度快</li><li>适应运动物体变形</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>与深度学习模型相比性能一般</li><li>识别对象单一</li><li>大幅度旋转无法适应，稳定性差</li></ul><h2 id="神经网络分类：R-CNN系列方法"><a href="#神经网络分类：R-CNN系列方法" class="headerlink" title="神经网络分类：R-CNN系列方法"></a>神经网络分类：R-CNN系列方法</h2><h3 id="模型思路"><a href="#模型思路" class="headerlink" title="模型思路"></a>模型思路</h3><p>对多个位置，不同尺寸，用卷积神经网络判断区域内图片是不是需要检测的物体</p><p>候选位置(proposal)提出方法：EdgeBox</p><p><img src="/images/image-20201116102406321.png" alt="神经网络分类"></p><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol><li><p>分类器的训练——直接使用ImageNet模型(有关ImageNet数据集的介绍见博客另一篇文章)</p><p><img src="/images/image-20201116102702513.png" alt="ImageNet"></p></li><li><p>Fine-tune分类模型</p><p>选择20类进行探测；对原始分类模型结构进行更改；最终确定21个目标（20类+其他）</p></li><li><p>特征提取</p><p>图片计算候选区域；候选区域切分图片大小，变成输入大小；提取相应特征；存储特征（一般容量很大）</p><p><img src="/images/image-20201116104241163.png" alt="特征提取"></p></li><li><p>单独目标探测器训练</p><p><img src="/images/image-20201117104716437.png" alt="单独目标探测器训练"></p></li><li><p>单独目标回归器训练——基于候选区域微调</p><p><img src="/images/image-20201117105334368.png" alt="候选区域微调"></p></li></ol><h3 id="测试过程"><a href="#测试过程" class="headerlink" title="测试过程"></a>测试过程</h3><p><img src="/images/image-20201117105559250.png" alt="R-CNN测试过程"></p><h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><p><img src="/images/image-20201117105824181.png" alt="IoU计算"></p><p>分子是预测区域与真实区域之间的交集面积，分母是并集面积</p><h2 id="神经网络回归：YoLo系列方法"><a href="#神经网络回归：YoLo系列方法" class="headerlink" title="神经网络回归：YoLo系列方法"></a>神经网络回归：YoLo系列方法</h2><h2 id="实例：目标探测模型训练与部署"><a href="#实例：目标探测模型训练与部署" class="headerlink" title="实例：目标探测模型训练与部署"></a>实例：目标探测模型训练与部署</h2>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> CNN </tag>
            
            <tag> 目标检测 </tag>
            
            <tag> 目标分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【深度学习】R-CNN</title>
      <link href="2020/11/15/shen-du-xue-xi-r-cnn/"/>
      <url>2020/11/15/shen-du-xue-xi-r-cnn/</url>
      
        <content type="html"><![CDATA[<h2 id="模型简述"><a href="#模型简述" class="headerlink" title="模型简述"></a>模型简述</h2><p>R-CNN概括起来就是selective search+CNN+L-SVM的检测器</p><h2 id="框架主要内容"><a href="#框架主要内容" class="headerlink" title="框架主要内容"></a>框架主要内容</h2><ul><li>Region Proposals区域选择：用selective search(选择性搜索)代替传统的滑动窗口搜索，提取出2000个候选region proposal(区域)</li><li>CNN特征提取：对于每个region，用摘掉了最后一层softmax层的AlexNet(一种很经典的网络结构)来提取特征</li><li>SVM判别类别：训练出来K个L-SVM(linear SVM)作为分类器(每个目标类一个SVM分类器，K为目标类个数)，使用AlexNet提取出来的特征作为输入，得到每个region属于某一类的得分</li><li>选定框回归：最后对每个类别用NMS(non-maximum-suppression 非最大抑制,一种选取检测框的算法)来舍弃掉一部分region，得到detection的结果(对得到的结果做针对boundingbox回归(边框回归)，用来修正预测的boundingbox的位置)</li></ul><h2 id="算法模块"><a href="#算法模块" class="headerlink" title="算法模块"></a>算法模块</h2><h4 id="1、Region-Proposals区域选择"><a href="#1、Region-Proposals区域选择" class="headerlink" title="1、Region Proposals区域选择"></a>1、Region Proposals区域选择</h4><p>在目标检测时，为了定位到目标的具体位置，通常会把图像分成许多子块，然后把子块作为输入，送到目标识别的模型中。分子块的最直接方法叫滑动窗口法（Sliding Window Approach）。滑动窗口的方法就是按照子块的大小在整幅图像上穷举所有子图像块。<br> 而论文中使用的Selective Search方法是则是基于区域推荐的，目的是为了改善传统提取特征方法中机从左到右、从上到下穷举式带来的低效。R-CNN不关心特定的区域推荐算法，选择Selective Search只是为了与需要对比的前任论文保持一致。R-CNN一张图片选出约2000个候选区域</p><h4 id="2、CNN提取特征-AlexNet网络结构"><a href="#2、CNN提取特征-AlexNet网络结构" class="headerlink" title="2、CNN提取特征(AlexNet网络结构)"></a>2、CNN提取特征(AlexNet网络结构)</h4><p><img src="/images/CNN.jpg" alt="CNN网络结构(AlexNet)"></p><p><img src="/images/dim3.png" alt="AlexNet维度变化"></p><p>R-CNN使用的CNN网络是AlexNet，该网络的具体结构见上图。AlexNet原本是为做<strong>图像分类任务</strong>，R-CNN是为做目标检测任务，故替换掉了AlexNet的最后一层的<strong>全连接层</strong>（4096*1000）。故R-CNN的结构<strong>实际是5个卷积层、2个全连接层</strong>(上图全链接8层去掉)。输入是Region Proposal计算的推荐区域的图像，由于该<strong>CNN网络输入限定</strong>为2000*227*227*3（RGB）的输入，故在R-CNN中将Region Proposal的推荐区域<strong>仿射变形</strong>到227*227的格式上，网络输出是2000*4096*1的特征向量（提取了2000个特征，每个特征由4096*1向量代表）。 </p><h4 id="3、SVM判别类别"><a href="#3、SVM判别类别" class="headerlink" title="3、SVM判别类别"></a>3、SVM判别类别</h4><p>使用训练过的对应类别的SVM给特征向量中的每个类进行打分，每个类别对应一个<strong>二分类SVM</strong>，CNN输出2000*4096，SVM输出2000*N（N是数据集中目标的类别），然后为了减少重复的bounding box(边框)，使用了<strong>非极大值抑制法</strong>：如果一个区域的得分与大于它得分的区域有很大程度的交叉（intersection-overunion（IoU））根据score进行排序，把score最大的bounding box作为选定的框，计算其余bounding box与当前最大score与box的IoU，去除IoU大于设定的阈值的bounding box。然后重复上面的过程，直至候选bounding box为空，然后再将score小于一定阈值的选定框删除得到一类的结果。</p><h4 id="4、选定框回归"><a href="#4、选定框回归" class="headerlink" title="4、选定框回归"></a>4、选定框回归</h4><p><img src="/images/Region_picture.png" alt="Region proposal"></p><p>如上图所示，通过selective search得到的region proposal可能与ground truth相差较大，尽管这个region proposal可能有很高的分类评分，但对于检测来说，它依然是不合格的。<br> 为了进一步提高定位的准确度，作者在对各个region proposal打分后，利用回归的方法重新预测了一个新的矩形框，借此来进一步修正bounding box的大小和位置。</p><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>由于在实际检测中，带标签的训练数据肯定是<strong>缺少</strong>的。作者给出<strong>pre-training</strong>和<strong>fine-tuning</strong>结合的方法：</p><p><strong>1、使用imagenet数据集预训练AlexNet，所谓pre-training（使用辅助数据集进行有监督的预训练）。</strong></p><p><strong>2、fine-tune，将模型的最后一层修改类别数。使用pascal voc 数据集fine-tune。</strong></p><p>输入为warped region proposal(扭曲区域，即进行仿射变换后的图像)，输出为21维类向量。（与pre-training网络的唯一改变是由输出1000类改为输出21类）。包括20类目标和背景。把与ground truth的IOU&gt;0.5的region proposal作为正例，其余为负例，即背景。每个batch里包含32个正例，96个负例共128个。学习率为0.001，是pre-training的1/10，为了不破坏与训练的结果。</p><p><strong>3、训练SVM</strong></p><p>训练某类别的SVM，与该类目标的groud-truth的IoU&gt;0.3的region proposal的特征向量作为正例，其余作为负例。为了减小内存使用，采用standard hard negative mining method。</p><p><strong>4、训练bounding box regression</strong></p><p>受DPM （Object detection with discriminatively trained part based models）的灵感，训练一个线性回归模型，给定pool5层的特征预测一个新的检测窗口。</p><p><strong>5、正负样本和softmax的问题：</strong></p><p>为什么最后分类的时候用SVM代替了softmax，因为作者通过实验发现还是SVM更好。那为什么不一开始就用SVM做fine-tuning呢？我认为是SVM是一个二分类器，并不适合做fine-tuning。当用softmax做fine-tuning时，如果采用和SVM一样的区分正负样本策略，则效果会差很多。作者猜测是因为这样做会引起样本数太少导致过拟合。softmax区分样本的方法更宽泛，将正样本的数量提高近30倍，这样就避免了过拟合。值得注意的是，这样做得到的结果是次优的，因为并没有用精确的定位以及更严格的负样本来fine-tuning。svm训练的时候，因为svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格。</p>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> R-CNN </tag>
            
            <tag> CNN </tag>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Python】报错笔记</title>
      <link href="2020/11/15/python-bao-cuo-bi-ji/"/>
      <url>2020/11/15/python-bao-cuo-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>记录python编程过程中遇到的一些报错，警告的处理办法</p><h1 id="fillna-函数填充报错SettingWithCopyWarning"><a href="#fillna-函数填充报错SettingWithCopyWarning" class="headerlink" title="fillna()函数填充报错SettingWithCopyWarning"></a>fillna()函数填充报错SettingWithCopyWarning</h1><pre class=" language-lang-python"><code class="language-lang-python">#编写决策树模型预测泰坦尼克号乘客生还数据时#对缺失数据的填补，采用中位数和平均值都是对模型偏差程度影响最小的策略X['age'].fillna(X['age'].mean(), inplace=True)#报错语句如下：SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrameSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy  self._update_inplace(new_data)</code></pre><h3 id="报错原因"><a href="#报错原因" class="headerlink" title="报错原因"></a>报错原因</h3><p>这是因为凡是出现链式赋值（例x=y=z=1）的情况，pandas不确定到底返回的是引用还是拷贝。所以干脆报warning,我上边那个函数就是对age列的缺失值全部赋为平均值。</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>未找到解决办法</p><h1 id="np-array-的shape-2-与-2-1-的分别是什么意思"><a href="#np-array-的shape-2-与-2-1-的分别是什么意思" class="headerlink" title="np.array 的shape (2,)与(2,1)的分别是什么意思"></a>np.array 的shape (2,)与(2,1)的分别是什么意思</h1><h3 id="1-2-的shape值-2-，意思是一维数组，数组中有2个元素。"><a href="#1-2-的shape值-2-，意思是一维数组，数组中有2个元素。" class="headerlink" title="[1,2]的shape值(2,)，意思是一维数组，数组中有2个元素。"></a>[1,2]的shape值(2,)，意思是一维数组，数组中有2个元素。</h3><pre class=" language-lang-python"><code class="language-lang-python">a = np.array([1,2,3])a.shape</code></pre><p>输出为(3,)<br><strong>它更多的是和多维做对比，表示一维数组，如果非要从行列的角度考虑。如果 W.shape = (2, 4), x.shape = (2, ) np.dot(W.T, x)：此时x是列向量, np.dot(x, W) 此时 x 是行向量，两种情况下返回的也均是一维向量，无所谓行列的概念。</strong></p><h3 id="1-2-的shape值是-2-1-，意思是一个二维数组，每行有1个元素。"><a href="#1-2-的shape值是-2-1-，意思是一个二维数组，每行有1个元素。" class="headerlink" title="[[1],[2]]的shape值是(2,1)，意思是一个二维数组，每行有1个元素。"></a>[[1],[2]]的shape值是(2,1)，意思是一个二维数组，每行有1个元素。</h3><pre class=" language-lang-python"><code class="language-lang-python">a = np.array([[1,2,3],[4,5,6]])a.shape</code></pre><p>输出为(2,3)</p><h3 id="1-2-的shape值是（1，2），意思是一个二维数组，每行有2个元素。"><a href="#1-2-的shape值是（1，2），意思是一个二维数组，每行有2个元素。" class="headerlink" title="[[1,2]]的shape值是（1，2），意思是一个二维数组，每行有2个元素。"></a>[[1,2]]的shape值是（1，2），意思是一个二维数组，每行有2个元素。</h3><pre class=" language-lang-python"><code class="language-lang-python">a = np.array([[1,2,3]])a.shape</code></pre><p>输出为(1,3)</p><h1 id="Python的reshape的用法"><a href="#Python的reshape的用法" class="headerlink" title="Python的reshape的用法"></a>Python的reshape的用法</h1><h2 id="numpy中reshape函数的三种常见相关用法"><a href="#numpy中reshape函数的三种常见相关用法" class="headerlink" title="numpy中reshape函数的三种常见相关用法"></a>numpy中reshape函数的三种常见相关用法</h2><ul><li><p>numpy.arange(n).reshape(a, b)   依次生成n个自然数，并且以a行b列的数组形式显示</p><pre class=" language-lang-python"><code class="language-lang-python">np.arange(16).reshape(2,8) #生成16个自然数，以2行8列的形式显示# Out: # array([[ 0,  1,  2,  3,  4,  5,  6,  7],#       [ 8,  9, 10, 11, 12, 13, 14, 15]])</code></pre></li><li><p>mat (or array).reshape(c, -1)   必须是<strong>矩阵格式或者数组格式</strong>，才能使用 .reshape(c, -1) 函数， 表示将此矩阵或者数组重组，以 c行d列的形式表示，<strong>-1用于标记该维度大小采用自动计算确定</strong></p><pre class=" language-lang-python"><code class="language-lang-python">arr.shape    # (a,b)arr.reshape(m,-1) #改变维度为m行、d列 （-1表示列数自动计算，d= a*b /m ）arr.reshape(-1,m) #改变维度为d行、m列 （-1表示行数自动计算，d= a*b /m ）</code></pre><p>-1的作用就在此<strong>: 自动计算d：d=数组或者矩阵里面所有的元素个数/c</strong>, d必须是整数，不然报错）</p><p>（reshape(-1, m)即列数固定，行数需要计算）</p><pre class=" language-lang-python"><code class="language-lang-python">arr=np.arange(16).reshape(2,8)arr'''out:array([[ 0,  1,  2,  3,  4,  5,  6,  7],       [ 8,  9, 10, 11, 12, 13, 14, 15]])'''arr.reshape(4,-1) #将arr变成4行的格式，列数自动计算的(c=4, d=16/4=4)'''out:array([[ 0,  1,  2,  3],       [ 4,  5,  6,  7],       [ 8,  9, 10, 11],       [12, 13, 14, 15]])''' arr.reshape(8,-1) #将arr变成8行的格式，列数自动计算的(c=8, d=16/8=2)'''out:array([[ 0,  1],       [ 2,  3],       [ 4,  5],       [ 6,  7],       [ 8,  9],       [10, 11],       [12, 13],       [14, 15]])''' arr.reshape(10,-1) #将arr变成10行的格式，列数自动计算的(c=10, d=16/10=1.6 != Int)'''out:ValueError: cannot reshape array of size 16 into shape (10,newaxis)'''</code></pre></li><li><p>numpy.arange(a,b,c).reshape(m,n) ：将array的维度变为m 行 n列。</p><p>numpy.arange(a,b,c)  从 数字a起, 步长为c, 到b结束，生成array</p><pre class=" language-lang-python"><code class="language-lang-python">np.arange(1,12,2)#间隔2生成数组，范围在1到12之间# Out: array([ 1,  3,  5,  7,  9, 11])np.arange(1,12,2).reshape(3,2)'''Out: array([[ 1,  3],       [ 5,  7],       [ 9, 11]])'''</code></pre></li></ul><h2 id="reshape-1-1-转化成1行："><a href="#reshape-1-1-转化成1行：" class="headerlink" title="reshape(1,-1)转化成1行："></a>reshape(1,-1)转化成1行：</h2><pre class=" language-lang-python"><code class="language-lang-python">import numpy as npx = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])print(x.shape)print(x.reshape(1,-1))'''Out:(4, 4)[[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]]'''</code></pre><h2 id="reshape-2-1-转换成两行："><a href="#reshape-2-1-转换成两行：" class="headerlink" title="reshape(2,-1)转换成两行："></a>reshape(2,-1)转换成两行：</h2><p><img src="/images/reshape.png" alt="reshape(2,-1)"></p><h2 id="reshape-1-1-转换成1列："><a href="#reshape-1-1-转换成1列：" class="headerlink" title="reshape(-1,1)转换成1列："></a>reshape(-1,1)转换成1列：</h2><p><img src="/images/reshape2.png" alt="reshape(-1,1)"></p><h1 id="numpy的ravel-和-flatten-函数"><a href="#numpy的ravel-和-flatten-函数" class="headerlink" title="numpy的ravel() 和 flatten()函数"></a>numpy的ravel() 和 flatten()函数</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>首先声明两者所要实现的功能是一致的（将多维数组降位一维）。这点从两个单词的意也可以看出来，ravel(散开，解开)，flatten（变平）。两者的区别在于返回拷贝（copy）还是返回视图（view），numpy.flatten()返回一份拷贝，对拷贝所做的修改不会影响（reflects）原始矩阵，而numpy.ravel()返回的是视图（view，也颇有几分C/C++引用reference的意味），会影响（reflects）原始矩阵。</p><h2 id="两者功能"><a href="#两者功能" class="headerlink" title="两者功能"></a>两者功能</h2><pre class=" language-lang-python"><code class="language-lang-python">In [14]: x=np.array([[1,2],[3,4]])# flattenh函数和ravel函数在降维时默认是行序优先In [15]: x.flatten()Out[15]: array([1, 2, 3, 4])In [17]: x.ravel()Out[17]: array([1, 2, 3, 4])# 传入'F'参数表示列序优先In [18]: x.flatten('F')Out[18]: array([1, 3, 2, 4])In [19]: x.ravel('F')Out[19]: array([1, 3, 2, 4])#reshape函数当参数只有一个-1时表示将数组降为一维In [21]: x.reshape(-1)Out[21]: array([1, 2, 3, 4])#x.T表示x的转置In [22]: x.T.reshape(-1)Out[22]: array([1, 3, 2, 4])12345678910111213141516171819202122</code></pre><h2 id="两者区别"><a href="#两者区别" class="headerlink" title="两者区别"></a>两者区别</h2><pre class=" language-lang-python"><code class="language-lang-python">>>> x = np.array([[1, 2], [3, 4]])>>> a = x.flatten()>>> a[1] = 100>>>> aarray([  1, 100,   3,   4])>>> xarray([[1, 2],       [3, 4]])            12345678</code></pre><p>通过上面的程序可以发现flatten函数返回的是拷贝,修改返回的a之后原始的x并未改变。</p><pre class=" language-lang-python"><code class="language-lang-python">>>> x = np.array([[1, 2], [3, 4]])>>> a = x.ravel()>>> aarray([1, 2, 3, 4])>>> a[1] = 100>>> aarray([  1, 100,   3,   4])>>> xarray([[  1, 100],       [  3,   4]])12345678910</code></pre><p><strong>可以看到ravel返回的是视图(参考数据库中的视图进行理解)，修改a之后x的内容并不会发生改变。不过注意a已经变为一维的了，x还是二维的，a只是将x的数据以不同的方式进行呈现</strong>。</p><h1 id="成功解决AxisError-axis-1-is-out-of-bounds-for-array-of-dimension-0"><a href="#成功解决AxisError-axis-1-is-out-of-bounds-for-array-of-dimension-0" class="headerlink" title="成功解决AxisError: axis -1 is out of bounds for array of dimension 0"></a>成功解决AxisError: axis -1 is out of bounds for array of dimension 0</h1><p>python机器学习及实践（从零开始kaggle竞赛之路）第二章的2.1.2.5集成模型程序报错：numpy.core._internal.AxisError: axis 0 is out of bounds for array of dimension 0:</p><p>1.原因是本段代码在python3以上执行时：</p><pre><code>print(np.sort(zip(etr.feature_importances_,bosten.feature_names),axis=0))</code></pre><p>zip函数返回的是迭代器结果</p><p>在python2.7下执行是通过的</p><p>2.修改代码为：</p><pre><code>print(np.sort(list(zip(etr.feature_importances_,bosten.feature_names)),axis=0))</code></pre><p>3.报错原因：主要是因为python3中将zip进行了更新，zip() 返回的是一个对象，目的其实是为了减少内存。对象是不能直接迭代的，若需要迭代则需转化为可迭代对象。</p><h1 id="python内置函数中enumerate-函数的作用"><a href="#python内置函数中enumerate-函数的作用" class="headerlink" title="python内置函数中enumerate() 函数的作用"></a>python内置函数中enumerate() 函数的作用</h1><p>1.描述</p><p>enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。</p><p>2.语法</p><pre class=" language-lang-python"><code class="language-lang-python">enumerate(sequence, [start=0])</code></pre><p>3.参数</p><ul><li>sequence — 一个序列、迭代器或其他支持迭代对象。</li><li>start — 下标起始位置。</li></ul><p>4.返回值</p><p>返回 enumerate(枚举) 对象。</p>]]></content>
      
      
      <categories>
          
          <category> 编程笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
            <tag> python </tag>
            
            <tag> 报错 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【深度学习】DeepLearning入门</title>
      <link href="2020/11/12/shen-du-xue-xi-deeplearning-ru-men/"/>
      <url>2020/11/12/shen-du-xue-xi-deeplearning-ru-men/</url>
      
        <content type="html"><![CDATA[<p> 学习神经网络，深度学习的一些笔记</p><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="1、两个问题"><a href="#1、两个问题" class="headerlink" title="1、两个问题"></a>1、两个问题</h3><h4 id="1-1-为什么需要使用激活函数"><a href="#1-1-为什么需要使用激活函数" class="headerlink" title="1.1 为什么需要使用激活函数"></a>1.1 为什么需要使用激活函数</h4><p>如果网络中的神经元没有激活函数，如下图所示，可以看出该网络是x1和x2的线性组合</p><p><img src="/images/picture1.png" alt="无激活函数网络"></p><p>因此，最终训练出来的永远都是一条直线，而不能很好的达到分类效果</p><h4 id="1-2-随机初始化"><a href="#1-2-随机初始化" class="headerlink" title="1.2 随机初始化"></a>1.2 随机初始化</h4><p><strong>将参数全部初始化为0，在神经网络中不适用</strong></p><p>尽管在逻辑回归中，可以这样操作，但是实际神经网络训练中起不到作用。</p><p><img src="/images/picture2.png" alt="无激活函数网络"></p><p>如果我们初始化所有θ全为0（全部相等），那么对于每个隐藏单元的输入基本都是一样的，所以求得的偏导也是一致的。</p><p><strong>这就意味着这个神经网络计算不出很好的函数，当我们有很多的隐藏单元时，所有的隐藏单元都在计算相同的特征，都在以相同的函数作为输入。—-高度冗余。所以无论后面有几个输出单元，最终只能得到一个特征，这种情况阻止了神经网络学习有趣的东西</strong></p><h3 id="2、激活函数"><a href="#2、激活函数" class="headerlink" title="2、激活函数"></a>2、激活函数</h3><h4 id="2-1-为什么要用激活函数"><a href="#2-1-为什么要用激活函数" class="headerlink" title="2.1 为什么要用激活函数"></a>2.1 为什么要用激活函数</h4><p>神经网络中激活函数的主要作用是提供网络的<strong>非线性建模能力</strong>，如不特别说明，激活函数一般而言是非线性函数。假设一个示例神经网络中仅包含线性卷积和全连接运算，那么该网络仅能够表达线性映射，即便增加网络的深度也依旧还是线性映射，难以有效建模实际环境中非线性分布的数据。<strong>加入（非线性）激活函数之后，深度神经网络才具备了分层的非线性映射学习能力。</strong></p><h4 id="2-2-激活函数性质"><a href="#2-2-激活函数性质" class="headerlink" title="2.2 激活函数性质"></a>2.2 激活函数性质</h4><ul><li><strong>非线性：</strong>如果不用激活函数，每一层输出都是上层输入的线性函数，<strong>无论神经网络有多少层，输出都是输入的线性组合。</strong>如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。<strong>当激活函数是非线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。</strong></li><li><strong>可微性：</strong> 当优化方法是基于梯度的时候，这个性质是必须的。</li><li><strong>单调性：</strong> 当激活函数是单调的时候，单层网络能够保证是凸函数。</li><li><strong>$f(x)\approx x$：</strong>当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。但是，如果激活函数是恒等激活函数的时候（即<em>f</em>(<em>x</em>)=<em>x</em>），如果MLP（多层感知机）使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。</li><li><strong>输出值的范围：</strong> 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况下，一般需要更小的学习率。</li></ul><h4 id="2-3-常见激活函数"><a href="#2-3-常见激活函数" class="headerlink" title="2.3 常见激活函数"></a>2.3 常见激活函数</h4><ul><li><p>逻辑函数Sigmoid</p><p>​    逻辑函数（logistic function）或逻辑曲线（logistic curve）是一种常见的S函数，它是皮埃尔·弗朗索瓦·韦吕勒在1844或1845年在研究它与人口增长的关系时命名的。</p><p>​    一个简单的Logistic函数表达式为:</p><script type="math/tex; mode=display">f(x)=\frac{1}{1+e^{-x}}</script><p><img src="/images/DeepLearning/1180694-20190819084349786-867823424.jpg" alt="标准逻辑函数的图像"></p><p>逻辑函数形如S，所以通常也叫做S形函数。</p><p>从函数图像易知f(x)的定义域为[-∞, +∞]， 值域是(0,1)</p><p>对f(x)求导数，易得</p><p><img src="/images/DeepLearning/image-20201119155801659.png" alt=""></p></li><li><p>双曲正切函数tanh</p><pre><code>双曲正切函数是双曲函数的一种。在数学中，双曲函数是一类与常见的三角函数类似的函数。双曲正切函数的定义为</code></pre><p><img src="/images/DeepLearning/image-20201119155843774.png" alt=""></p><p><img src="/images/DeepLearning/70" alt="双曲正切函数的图像"></p><p>从函数图像易知f(x)的定义域为[-∞, +∞]， 值域是(-1,1)</p><p>对f(x)求导数，易得</p><p><img src="/images/DeepLearning/image-20201119160045120.png" alt=""></p></li><li><p>线性整流函数ReLU</p><p>​    线性整流函数（Rectified Linear Unit, ReLU）,又称修正线性单元, 是一种人工神经网络中常用的激活函数，通常指代以斜坡函数及其变种为代表的非线性函数。</p><p>​    通常意义下，线性整流函数指代数学中的斜坡函数，即</p><p><img src="/images/DeepLearning/image-20201119160149198.png" alt=""></p><p><img src="/images/DeepLearning/1180694-20190819084423993-1732278271.jpg" alt="ReLU函数图像"></p><p><img src="/images/DeepLearning/image-20201119161206031.png" alt="带泄露的ReLu函数图像"></p></li></ul><h2 id="深度神经网络"><a href="#深度神经网络" class="headerlink" title="深度神经网络"></a>深度神经网络</h2><h3 id="1、什么是神经深度神经网络"><a href="#1、什么是神经深度神经网络" class="headerlink" title="1、什么是神经深度神经网络"></a>1、什么是神经深度神经网络</h3><p><img src="/images/deep_netwok.jpg" alt="What is a deep neural network?"></p><p>上图依次是：</p><p>单层神经网络(传统的机器学习模型，如：logistic回归)；</p><p>双层神经网络模型(隐含层+输出层，注意：不算输入层这一层)，三层神经网络模型；</p><p>深度神经网络模型(层次更深)；</p><h3 id="2、一些描述深度神经网络的符号"><a href="#2、一些描述深度神经网络的符号" class="headerlink" title="2、一些描述深度神经网络的符号"></a>2、一些描述深度神经网络的符号</h3><p>以下图网络结构为例：</p><p><img src="/images/network.jpg" alt="深度神经网络"></p><p>该结构有4层网络结构(layer neural network)，其中，三层是隐含层(hidden layers)</p><p><img src="/images/network2.jpg" alt="深度神经网络"></p><p>其中，需要预先了解一些参数：</p><ul><li>$L=4$表示的是网络层数，用$l$对其进行标号；</li><li>$n^{[l]}$表示第$l$层的神经元节点数；</li><li>$a^{[l]}$表示第$l$层的激活函数输出(节点上的计算值)；</li><li>$z^{[l]}$是上层计算完的结果值，即输入到该层神经元的值；</li><li>$\omega^{[l]}$表示第$l$层的权重；</li><li>输入数据$x$记为$a^{[0]}$，输出$ \hat{y}$记为$a^{[L]}$。</li><li>换算公式：</li></ul><script type="math/tex; mode=display">a^{[l]} = g^{[l]}(z^{[l]})\\z^{[l]} = \omega^{[l]} a^{[l-1]} + b^{[l]}\\</script><p>向前传播变量计算流程图：</p><p><img src="/images/model1.png" alt="向前传播变量计算流程图"></p><p>向后传播变量计算流程图：</p><p><img src="/images/after_net.jpg" alt="向后传播变量计算流程图"></p><h3 id="3、维度对齐那些事"><a href="#3、维度对齐那些事" class="headerlink" title="3、维度对齐那些事"></a>3、维度对齐那些事</h3><h4 id="3-1-输入特征为一维值时"><a href="#3-1-输入特征为一维值时" class="headerlink" title="3.1 输入特征为一维值时"></a>3.1 输入特征为一维值时</h4><p><img src="/images/dim1.jpg" alt="维度对齐过程"></p><p>先通过$z$和$x$来考虑$\omega$的维度，推理可得，$\omega^{[i]}$的维度为$(n^{[i]},n^{[i-1]})$；</p><p>再考虑偏置常量$b$的维度，主要是求和方向维度对齐，$b^{[i]}$维度为$(n^{[i]},1)$。</p><h4 id="3-2-输入特征为m维值时"><a href="#3-2-输入特征为m维值时" class="headerlink" title="3.2 输入特征为m维值时"></a>3.2 输入特征为m维值时</h4><p><img src="/images/dim2.jpg" alt="维度对齐过程"></p><p>主要是改变了偏置常量$b$的维度，$b^{[i]}$维度为$(n^{[i]},m)$。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> Cousera </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文合集】CVPR2019</title>
      <link href="2020/11/09/lun-wen-he-ji-cvpr2019/"/>
      <url>2020/11/09/lun-wen-he-ji-cvpr2019/</url>
      
        <content type="html"><![CDATA[<h3 id="检测-38"><a href="#检测-38" class="headerlink" title="检测 38"></a>检测 38</h3><p>1、Stereo R-CNN based 3D Object Detection for Autonomous Driving<br>作者：Peiliang Li, Xiaozhi Chen, Shaojie Shen<br>论文链接：<a href="https://arxiv.org/abs/1902.09738">https://arxiv.org/abs/1902.09738</a></p><p>2、Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression<br>作者：Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, Silvio Savarese<br>论文链接：<a href="https://arxiv.org/abs/1902.09630">https://arxiv.org/abs/1902.09630</a><br>论文解读：<a href="https://mp.weixin.qq.com/s/6QsyYtEVjavoLfU_lQF1pw">https://mp.weixin.qq.com/s/6QsyYtEVjavoLfU_lQF1pw</a></p><p>3、ROI-10D: Monocular Lifting of 2D Detection to 6D Pose and Metric Shape 作者：Fabian Manhardt, Wadim Kehl, Adrien Gaidon<br>论文链接：<a href="https://arxiv.org/abs/1812.02781">https://arxiv.org/abs/1812.02781</a></p><p>4、Bi-Directional Cascade Network for Perceptual Edge Detection<br>作者：Jianzhong He, Shiliang Zhang, Ming Yang, Yanhu Shan, Tiejun Huang<br>论文链接：<a href="https://arxiv.org/abs/1902.10903">https://arxiv.org/abs/1902.10903</a><br>Github源码：<a href="https://github.com/pkuCactus/BDCN">https://github.com/pkuCactus/BDCN</a></p><p>5、RepMet: Representative-based metric learning for classification and one-shot object detection<br>作者：Leonid Karlinsky, Joseph Shtok, Sivan Harary, Eli Schwartz, Amit Aides, Rogerio Feris, Raja Giryes, Alex M. Bronstein<br>论文链接：<a href="https://arxiv.org/abs/1806.04728">https://arxiv.org/abs/1806.04728</a></p><p>6、Region Proposal by Guided Anchoring<br>作者：Jiaqi Wang, Kai Chen, Shuo Yang, Chen Change Loy, Dahua Lin<br>论文链接：<a href="https://arxiv.org/abs/1901.03278">https://arxiv.org/abs/1901.03278</a><br>论文解读：<a href="https://mp.weixin.qq.com/s/Sl958JkcJjy-HW9_c-SH4g">https://mp.weixin.qq.com/s/Sl958JkcJjy-HW9_c-SH4g</a><br>Github链接：<a href="https://github.com/open-mmlab/mmdetection">https://github.com/open-mmlab/mmdetection</a></p><p>7、Less is More: Learning Highlight Detection from Video Duration<br>作者：Bo Xiong, Yannis Kalantidis, Deepti Ghadiyaram, Kristen Grauman<br>论文链接：<a href="https://arxiv.org/abs/1903.00859">https://arxiv.org/abs/1903.00859</a></p><p>8、AIRD: Adversarial Learning Framework for Image Repurposing Detection<br>作者：Ayush Jaiswal, Yue Wu, Wael AbdAlmageed, Iacopo Masi, Premkumar Natarajan<br>论文链接：<a href="https://arxiv.org/abs/1903.00788">https://arxiv.org/abs/1903.00788</a></p><p>9、Feature Selective Anchor-Free Module for Single-Shot Object Detection<br>作者：Chenchen Zhu, Yihui He, Marios Savvides<br>论文链接：<a href="https://arxiv.org/abs/1903.00621">https://arxiv.org/abs/1903.00621</a><br>论文解读：<a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid=2247487638&amp;idx=2&amp;sn=1e9f26013b3d9ab4fd4137729894606a&amp;chksm=ec1ffd6fdb687479183be59ec102f28bff4a5521903707fef744449e7630252c5298b66f339b&amp;scene=21&amp;token=236193575&amp;lang=zh_CN#wechat_redirect">CVPR2019 | FSAF：来自CMU的Single-Shot目标检测算法</a><br>一作直播：<a href="https://bbs.cvmart.net/topics/379/基于Anchor-free特征选择模块的单阶目标检测">CVPR2019 专题直播 | CMU 诸宸辰:基于 Anchor-free 特征选择模块的单阶目标检测</a></p><p>10、Learning Attraction Field Representation for Robust Line Segment Detection<br>作者：Nan Xue, Song Bai, Fudong Wang, Gui-Song Xia, Tianfu Wu, Liangpei Zhang<br>论文链接：<a href="https://arxiv.org/abs/1812.02122">https://arxiv.org/abs/1812.02122</a><br>代码链接：<a href="https://github.com/cherubicXN/afm_cvpr2019">https://github.com/cherubicXN/afm_cvpr2019</a></p><p>11、Latent Space Autoregression for Novelty Detection<br>作者：Davide Abati, Angelo Porrello, Simone Calderara, Rita Cucchiara<br>论文链接：<a href="https://arxiv.org/abs/1807.01653">https://arxiv.org/abs/1807.01653</a><br>代码链接: <a href="https://github.com/aimagelab/novelty-detection">https://github.com/aimagelab/novelty-detection</a></p><p>12、Strong-Weak Distribution Alignment for Adaptive Object Detection<br>作者：Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, Kate Saenko<br>论文链接：<a href="https://arxiv.org/abs/1812.04798">https://arxiv.org/abs/1812.04798</a></p><p>13、Few-shot Adaptive Faster R-CNN<br>作者：Tao Wang, Xiaopeng Zhang, Li Yuan, Jiashi Feng<br>论文链接：<a href="https://arxiv.org/abs/1903.09372">https://arxiv.org/abs/1903.09372</a></p><p>14、Attention Based Glaucoma Detection: A Large-scale Database and CNN Model<br>作者：Liu Li, Mai Xu, Xiaofei Wang, Lai Jiang, Hanruo Liu<br>论文链接：<a href="https://arxiv.org/abs/1903.10831">https://arxiv.org/abs/1903.10831</a></p><p>15、Bounding Box Regression with Uncertainty for Accurate Object Detection（目标检测边界框回归损失算法）<br>作者：Yihui He, Chenchen Zhu, Jianren Wang, Marios Savvides, Xiangyu Zhang<br>论文链接：<a href="https://arxiv.org/abs/1809.08545">https://arxiv.org/abs/1809.08545</a><br>代码链接：<a href="https://github.com/yihui-he/KL-Loss">https://github.com/yihui-he/KL-Loss</a></p><p>16、Precise Detection in Densely Packed Scenes<br>作者：Eran Goldman , Roei Herzig, Aviv Eisenschtat, Jacob Goldberger, Tal Hassner<br>论文链接：<a href="https://arxiv.org/abs/1904.00853">https://arxiv.org/abs/1904.00853</a></p><p>17、Activity Driven Weakly Supervised Object Detection<br>作者：Zhenheng Yang, Dhruv Mahajan, Deepti Ghadiyaram, Ram Nevatia, Vignesh Ramanathan<br>论文链接：<a href="https://arxiv.org/pdf/1904.01665.pdf">https://arxiv.org/pdf/1904.01665.pdf</a></p><p>18、Monocular 3D Object Detection Leveraging Accurate Proposals and Shape Reconstruction<br>作者：Jason Ku, Alex D. Pon, Steven L. Waslander<br>论文链接：<a href="https://arxiv.org/pdf/1904.01690.pdf">https://arxiv.org/pdf/1904.01690.pdf</a></p><p>19、Libra R-CNN: Towards Balanced Learning for Object Detection(目标检测)<br>作者：Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng, Wanli Ouyang, Dahua Lin<br>论文链接：<a href="https://arxiv.org/abs/1904.02701">https://arxiv.org/abs/1904.02701</a></p><p>20、Moving Object Detection under Discontinuous Change in Illumination Using Tensor Low-Rank and Invariant Sparse Decomposition<br>作者：Moein Shakeri, Hong Zhang<br>论文链接：<a href="https://arxiv.org/abs/1904.03175">https://arxiv.org/abs/1904.03175</a></p><p>21、Towards Universal Object Detection by Domain Attention<br>作者：Xudong Wang, Zhaowei Cai, Dashan Gao, Nuno Vasconcelos<br>论文链接：<a href="https://arxiv.org/abs/1904.04402">https://arxiv.org/abs/1904.04402</a><br>项目链接：<a href="https://www.svcl.ucsd.edu/projects/universal-detection/">https://www.svcl.ucsd.edu/projects/universal-detection/</a></p><p>22、NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection<br>作者：Golnaz Ghiasi, Tsung-Yi Lin, Ruoming Pang, Quoc V. Le<br>论文链接：<a href="https://arxiv.org/abs/1904.07392">https://arxiv.org/abs/1904.07392</a></p><p>23、Deep Anomaly Detection for Generalized Face Anti-Spoofing<br>作者：Daniel Pérez-Cabo, David Jiménez-Cabello, Artur Costa-Pazo, Roberto J. López-Sastre<br>论文链接：<a href="https://arxiv.org/abs/1904.08241">https://arxiv.org/abs/1904.08241</a></p><p>24、Cascaded Partial Decoder for Fast and Accurate Salient Object Detection<br>作者：Zhe Wu, Li Su, Qingming Huang<br>论文链接：<a href="https://arxiv.org/abs/1904.08739">https://arxiv.org/abs/1904.08739</a></p><p>25、A Simple Pooling-Based Design for Real-Time Salient Object Detection<br>作者：Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, Jianmin Jiang<br>论文链接：<a href="https://arxiv.org/abs/1904.09569">https://arxiv.org/abs/1904.09569</a><br>源码链接：<a href="https://mmcheng.net/poolnet/">https://mmcheng.net/poolnet/</a></p><p>26、CapSal: Leveraging Captioning to Boost Semantics for Salient Object Detection<br>作者：Lu Zhang; Huchuan Lu ; Zhe Lin ; Jianming Zhang; You He<br>论文链接：<a href="https://drive.google.com/open?id=1JcZMHBXEX-7AR1P010OXg_wCCC5HukeZ">https://drive.google.com/open?id=1JcZMHBXEX-7AR1P010OXg_wCCC5HukeZ</a> （需要申请）<br>源码链接：<a href="https://github.com/zhangludl/code-and-dataset-for-CapSal">https://github.com/zhangludl/code-and-dataset-for-CapSal</a></p><p>27、Deep Fitting Degree Scoring Network for Monocular 3D Object Detection<br>作者：Lijie Liu1, Jiwen Lu, Chunjing Xu, Qi Tian, Jie Zhou<br>论文链接：<a href="https://arxiv.org/pdf/1904.12681.pdf">https://arxiv.org/pdf/1904.12681.pdf</a></p><p>28、A Mutual Learning Method for Salient Object Detection with intertwined Multi-Supervision<br>作者：Runmin Wu, Mengyang Feng, Wenlong Guan, Dong Wang, Huchuan Lu, Errui Ding<br>论文链接：待定<br>源码链接：<a href="https://github.com/JosephineRabbit/MLMSNet">https://github.com/JosephineRabbit/MLMSNet</a></p><p>29、ScratchDet:Exploring to Train Single-Shot Object Detectors from Scratch(Oral)<br>作者：Rui Zhu, Shifeng Zhang, Xiaobo Wang, Longyin Wen, Hailin Shi, Liefeng Bo, Tao Mei<br>论文链接：<a href="https://arxiv.org/abs/1810.08425v3">https://arxiv.org/abs/1810.08425v3</a><br>源码链接：<a href="https://github.com/KimSoybean/ScratchDet">https://github.com/KimSoybean/ScratchDet</a><br>论文解读：<a href="https://mp.weixin.qq.com/s/TZj0QzDXE6QbCY5-pT6RNQ">CVPR 2019 Oral | 京东AI研究院提出 ScratchDet：随机初始化训练SSD目标检测器</a></p><p>30、Pyramid Feature Attention Network for Saliency detection<br>作者：Ting Zhao, Xiangqian Wu<br>论文链接：<a href="https://arxiv.org/abs/1903.00179">https://arxiv.org/abs/1903.00179</a><br>源码链接：<a href="https://github.com/CaitinZhao/cvpr2019_Pyramid-Feature-Attention-Network-for-Saliency-detection">https://github.com/CaitinZhao/cvpr2019_Pyramid-Feature-Attention-Network-for-Saliency-detection</a></p><p>31、Shifting More Attention to Video Salient Objection Detection（Oral）<br>作者：Deng-Ping Fan, Wenguan Wang, Ming-Ming Cheng, Jianbing Shen<br>论文链接：待定<br>源码链接：<a href="https://github.com/DengPingFan/DAVSOD">https://github.com/DengPingFan/DAVSOD</a></p><p>32、PPGNet: Learning Point-Pair Graph for Line Segment Detection<br>作者：Ziheng Zhang, Zhengxin Li, Ning Bi, Jia Zheng, Jinlei Wang, Kun Huang, Weixin Luo, Yanyu Xu, Shenghua Gao<br>论文链接：<a href="https://arxiv.org/abs/1905.03415">https://arxiv.org/abs/1905.03415</a><br>源码链接：<a href="https://github.com/svip-lab/PPGNet">https://github.com/svip-lab/PPGNet</a></p><p>33、Diversify and Match: A Domain Adaptive Representation Learning Paradigm for Object Detection<br>作者：Taekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon Choi, Changick Kim<br>论文链接：<a href="https://arxiv.org/abs/1905.05396">https://arxiv.org/abs/1905.05396</a></p><p>34、Direction-aware Spatial Context Features for Shadow Detection（Oral）<br>作者：Xiaowei Hu, Chi-Wing Fu, Lei Zhu, Jing Qin and Pheng-Ann Heng<br>论文链接：待更新<br>源码链接：<a href="https://github.com/xw-hu/DSC">https://github.com/xw-hu/DSC</a></p><p>35、FastDraw: Addressing the Long Tail of Lane Detection by Adapting a Sequential Prediction Network<br>作者：Jonah Philion<br>论文链接：<a href="https://arxiv.org/abs/1905.04354">https://arxiv.org/abs/1905.04354</a></p><p>36、An Iterative and Cooperative Top-down and Bottom-up Inference Network for Salient Object Detection(南开大学)<br>作者：Wenguan Wang, Jianbing Shen, Ming-Ming Cheng, Ling Shao<br>论文链接：<a href="https://mftp.mmcheng.net/Papers/19cvprIterativeSOD.pdf">https://mftp.mmcheng.net/Papers/19cvprIterativeSOD.pdf</a></p><p>37、Contrast Prior and Fluid Pyramid Integration for RGBD Salient Object Detection（南开大学）<br>作者：Jiaxing Zhao<em>, Yang Cao</em>, Deng-Ping Fan<em>, Xuan-Yi Li, Le Zhang, Ming-Ming Cheng. (</em> co-first author)<br>论文链接：<a href="https://dpfan.net/wp-content/uploads/19cvprRrbdSOD-cameraReady.pdf">https://dpfan.net/wp-content/uploads/19cvprRrbdSOD-cameraReady.pdf</a><br>项目链接：<a href="https://mmcheng.net/rgbdsalpyr/">https://mmcheng.net/rgbdsalpyr/</a></p><p>38、Bottom-up Object Detection by Grouping Extreme and Center Points<br>作者：Xingyi Zhou, Jiacheng Zhuo, Philipp Krähenbühl<br>论文链接：<a href="https://arxiv.org/abs/1901.08043">https://arxiv.org/abs/1901.08043</a><br>源码链接：<a href="https://github.com/xingyizhou/ExtremeNet">https://github.com/xingyizhou/ExtremeNet</a></p><p>39、Exploring the Bounds of the Utility of Context for Object Detection<br>探讨用于物体检测的上下文效用的界限<br>作者：Ehud Barnea, Ohad Ben-Shahar<br>论文链接：<a href="https://arxiv.org/abs/1711.05471v4">https://arxiv.org/abs/1711.05471v4</a><br>源码链接：<a href="https://github.com/EhudBarnea/ContextAnalysis">https://github.com/EhudBarnea/ContextAnalysis</a></p><h3 id="分割-50"><a href="#分割-50" class="headerlink" title="分割 50"></a>分割 50</h3><p>1、Attention-guided Unified Network for Panoptic Segmentation<br>作者：Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, Xingang Wang<br>论文链接：<a href="https://arxiv.org/abs/1812.03904">https://arxiv.org/abs/1812.03904</a><br>论文解读：<a href="https://mp.weixin.qq.com/s/1tohID6SM3weS476XU5okw">https://mp.weixin.qq.com/s/1tohID6SM3weS476XU5okw</a></p><p>2、FEELVOS: Fast End-to-End Embedding Learning for Video Object Segmentation<br>作者：Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, Liang-Chieh Chen<br>论文链接：<a href="https://arxiv.org/abs/1902.09513">https://arxiv.org/abs/1902.09513</a></p><p>3、Associatively Segmenting Instances and Semantics in Point Clouds<br>作者：Xinlong Wang, Shu Liu, Xiaoyong Shen, Chunhua Shen, Jiaya Jia<br>论文链接：<a href="https://arxiv.org/abs/1902.09852">https://arxiv.org/abs/1902.09852</a><br>代码链接：<a href="https://github.com/WXinlong/ASIS">https://github.com/WXinlong/ASIS</a></p><p>4、3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans<br>作者：Ji Hou Angela Dai Matthias Nießner<br>论文链接：<a href="https://niessnerlab.org/projects/hou20183dsis.html">https://niessnerlab.org/projects/hou20183dsis.html</a><br>YouTube视频：<a href="https://youtu.be/IH9rNLD1-JE">https://youtu.be/IH9rNLD1-JE</a></p><p>5、Data augmentation using learned transforms for one-shot medical image segmentation<br>作者：Amy Zhao, Guha Balakrishnan, Frédo Durand, John V. Guttag, Adrian V. Dalca<br>论文链接：<a href="https://arxiv.org/abs/1902.09383">https://arxiv.org/abs/1902.09383</a></p><p>6、FickleNet: Weakly and Semi-supervised Semantic Image Segmentation using Stochastic Inference<br>作者：Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, Sungroh Yoon<br>论文链接：<a href="https://arxiv.org/abs/1902.10421">https://arxiv.org/abs/1902.10421</a></p><p>7、Dual Attention Network for Scene Segmentation<br>作者：Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu<br>论文链接：<a href="https://arxiv.org/abs/1809.02983">https://arxiv.org/abs/1809.02983</a><br>Github源码：<a href="https://github.com/junfu1115/DANet">https://github.com/junfu1115/DANet</a></p><p>8、Mask Scoring R-CNN<br>作者：Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang Huang, Xinggang Wang<br>论文链接：<a href="https://arxiv.org/abs/1903.00241">https://arxiv.org/abs/1903.00241</a><br>Github链接：<a href="https://github.com/zjhuang22/maskscoring_rcnn">https://github.com/zjhuang22/maskscoring_rcnn</a><br>论文解读：<a href="https://mp.weixin.qq.com/s/aP7O7AF6WoynWK_FFHkOTw">https://mp.weixin.qq.com/s/aP7O7AF6WoynWK_FFHkOTw</a></p><p>9、Hybrid Task Cascade for Instance Segmentation（实例分割）<br>作者：Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, Chen Change Loy, Dahua Lin<br>论文链接：<a href="https://arxiv.org/abs/1901.07518">https://arxiv.org/abs/1901.07518</a><br>论文解读：<a href="https://mp.weixin.qq.com/s/xug0xKfc9RgJEUci1a_xog">https://mp.weixin.qq.com/s/xug0xKfc9RgJEUci1a_xog</a><br>Github链接：<a href="https://github.com/open-mmlab/mmdetection">https://github.com/open-mmlab/mmdetection</a></p><p>10、Object Counting and Instance Segmentation with Image-level Supervision<br>作者：Hisham Cholakkal, Guolei Sun (equal contribution), Fahad Shahbaz Khan, Ling Shao<br>论文链接：<a href="https://arxiv.org/abs/1903.02494">https://arxiv.org/abs/1903.02494</a></p><p>11、MS-TCN: Multi-Stage Temporal Convolutional Network for Action Segmentation<br>作者：Yazan Abu Farha, Juergen Gall<br>论文链接：<a href="https://arxiv.org/abs/1903.01945">https://arxiv.org/abs/1903.01945</a></p><p>12、Structured Knowledge Distillation for Semantic Segmentation(语义分割）<br>作者：Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, Jingdong Wang<br>论文链接：<a href="https://arxiv.org/abs/1903.04197">https://arxiv.org/abs/1903.04197</a></p><p>13、RVOS: End-to-End Recurrent Network for Video Object Segmentation<br>作者：Carles Ventura, Miriam Bellver, Andreu Girbau, Amaia Salvador, Ferran Marques, Xavier Giro-i-Nieto<br>论文链接：<a href="https://arxiv.org/abs/1903.05612">https://arxiv.org/abs/1903.05612</a><br>项目链接：<a href="https://imatge-upc.github.io/rvos/">https://imatge-upc.github.io/rvos/</a></p><p>14、Structured Knowledge Distillation for Semantic Segmentation（语义分割）<br>作者：Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, Jingdong Wang<br>论文链接：<a href="https://arxiv.org/abs/1903.04197">https://arxiv.org/abs/1903.04197</a></p><p>15、Knowledge Adaptation for Efficient Semantic Segmentation（语义分割）<br>作者：Tong He, Chunhua Shen, Zhi Tian, Dong Gong, Changming Sun, Youliang Yan<br>论文链接：<a href="https://arxiv.org/abs/1903.04688">https://arxiv.org/abs/1903.04688</a></p><p>16、Improving Semantic Segmentation via Video Propagation and Label Relaxation(oral)<br>作者：Yi Zhu, Karan Sapra, Fitsum A. Reda, Kevin J. Shih, Shawn Newsam, Andrew Tao, Bryan Catanzaro<br>论文链接：<a href="https://arxiv.org/abs/1812.01593">https://arxiv.org/abs/1812.01593</a></p><p>17、In Defense of Pre-trained ImageNet Architectures for Real-time Semantic Segmentation of Road-driving Images<br>作者：Marin Oršić, Ivan Krešo, Petra Bevandić, Siniša Šegvić<br>论文链接：<a href="https://arxiv.org/abs/1903.08469">https://arxiv.org/abs/1903.08469</a><br>代码链接：<a href="https://github.com/orsic/swiftnet">https://github.com/orsic/swiftnet</a></p><p>18、Large-scale interactive object segmentation with human annotators<br>作者：Rodrigo Benenson, Stefan Popov, Vittorio Ferrari<br>论文链接：<a href="https://arxiv.org/abs/1903.10830">https://arxiv.org/abs/1903.10830</a><br>BubbleNets: Learning to Select the Guidance Frame in Video Object Segmentation by Deep Sorting Frames<br>作者：Brent A. Griffin, Jason J. Corso<br>论文链接：<a href="https://arxiv.org/abs/1903.11779">https://arxiv.org/abs/1903.11779</a></p><p>19、Pose2Seg: Detection Free Human Instance Segmentation<br>作者：Song-Hai Zhang, Ruilong Li, Xin Dong, Paul L. Rosin, Zixi Cai, Han Xi, Dingcheng Yang, Hao-Zhi Huang, Shi-Min Hu<br>论文链接：<a href="https://arxiv.org/abs/1803.10683">https://arxiv.org/abs/1803.10683</a><br>项目链接：<a href="https://www.liruilong.cn/Pose2Seg/index.html">https://www.liruilong.cn/Pose2Seg/index.html</a><br>代码链接：<a href="https://github.com/liruilong940607/OCHumanApi">https://github.com/liruilong940607/OCHumanApi</a></p><p>20、BubbleNets: Learning to Select the Guidance Frame in Video Object Segmentation by Deep Sorting Frames<br>作者：Brent A. Griffin, Jason J. Corso<br>论文链接：<a href="https://arxiv.org/abs/1903.11779">https://arxiv.org/abs/1903.11779</a></p><p>21、JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds with Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields（Oral)<br>作者：Quang-Hieu Pham, Duc Thanh Nguyen, Binh-Son Hua, Gemma Roig, Sai-Kit Yeung<br>论文链接：<a href="https://arxiv.org/abs/1904.00699">https://arxiv.org/abs/1904.00699</a><br>项目链接：<a href="https://pqhieu.github.io/cvpr19.html">https://pqhieu.github.io/cvpr19.html</a></p><p>22、Spatiotemporal CNN for Video Object Segmentation<br>作者：Kai Xu, Longyin Wen, Guorong Li, Liefeng Bo, Qingming Huang<br>论文链接：<a href="https://arxiv.org/abs/1904.02363">https://arxiv.org/abs/1904.02363</a><br>代码链接：<a href="https://github.com/longyin880815/STCNN">https://github.com/longyin880815/STCNN</a></p><p>23、Data augmentation using learned transformsfor one-shot medical image segmentation<br>作者：Amy Zhao, Guha Balakrishnan, Frédo Durand, John V. Guttag, Adrian V. Dalca<br>论文链接：<a href="https://arxiv.org/pdf/1902.09383.pdf">https://arxiv.org/pdf/1902.09383.pdf</a><br>源码链接：<a href="https://github.com/xamyzhao/brainstorm">https://github.com/xamyzhao/brainstorm</a></p><p>24、DeepCO3: Deep Instance Co-segmentation by Co-peak Search and Co-saliency (Oral )<br>作者：Kuang-Jui Hsu, Yen-Yu Lin, Yung-Yu Chuang<br>论文链接：<a href="https://cvlab.citi.sinica.edu.tw/images/paper/cvpr-hsu19.pdf">https://cvlab.citi.sinica.edu.tw/images/paper/cvpr-hsu19.pdf</a><br>源码链接：<a href="https://github.com/KuangJuiHsu/DeepCO3">https://github.com/KuangJuiHsu/DeepCO3</a></p><p>25、Cross-Modal Self-Attention Network for Referring Image Segmentation<br>作者：Linwei Ye, Mrigank Rochan, Zhi Liu, Yang Wang<br>论文链接：<a href="https://arxiv.org/abs/1904.04745">https://arxiv.org/abs/1904.04745</a></p><p>26、Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations(Oral)<br>作者：Jiwoon Ahn, Sunghyun Cho, Suha Kwak<br>论文链接：<a href="https://arxiv.org/abs/1904.05044">https://arxiv.org/abs/1904.05044</a></p><p>27、Adaptive Weighting Multi-Field-of-View CNN for Semantic Segmentation in Pathology<br>作者：Hiroki Tokunaga, Yuki Teramoto, Akihiko Yoshizawa, Ryoma Bise<br>论文链接：<a href="https://arxiv.org/abs/1904.06040">https://arxiv.org/abs/1904.06040</a></p><p>28、A Relation-Augmented Fully Convolutional Network for Semantic Segmentationin Aerial Scenes<br>作者：Lichao Mou, Yuansheng Hua, Xiao Xiang Zhu<br>论文链接：<a href="https://arxiv.org/abs/1904.05730">https://arxiv.org/abs/1904.05730</a></p><p>29、DFANet：Deep Feature Aggregation for Real-Time Semantic Segmentation（旷视）<br>作者：Hanchao Li, Pengfei Xiong,Haoqiang Fan,Jian Sun<br>论文链接：<a href="https://share.weiyun.com/5NgHbWH">https://share.weiyun.com/5NgHbWH</a></p><p>30、Exploiting Computation Power of Blockchain for Biomedical Image Segmentation<br>作者：Boyang Li, Changhao Chenli, Xiaowei Xu, Taeho Jung, Yiyu Shi<br>论文链接：<a href="https://arxiv.org/abs/1904.07349">https://arxiv.org/abs/1904.07349</a></p><p>31、MHP-VOS: Multiple Hypotheses Propagation for Video Object Segmentation(Oral)<br>作者：Shuangjie Xu, Daizong Liu, Linchao Bao, Wei Liu, Pan Zhou<br>论文链接：<a href="https://arxiv.org/abs/1904.08141">https://arxiv.org/abs/1904.08141</a></p><p>32、Machine Vision Guided 3D Medical Image Compression for Efficient Transmission and Accurate Segmentation in the Clouds<br>作者：Zihao Liu, Xiaowei Xu, Tao Liu, Qi Liu, Yanzhi Wang, Yiyu Shi, Wujie Wen, Meiping Huang, Haiyun Yuan, Jian Zhuang<br>论文链接：<a href="https://arxiv.org/abs/1904.08487">https://arxiv.org/abs/1904.08487</a></p><p>33、Fast User-Guided Video Object Segmentation by Interaction-and-Propagation Networks<br>作者：Seoung Wug Oh, Joon-Young Lee, Ning Xu, Seon Joo Kim<br>论文链接：<a href="https://arxiv.org/abs/1904.09791">https://arxiv.org/abs/1904.09791</a></p><p>34、Box-driven Class-wise Region Masking and Filling Rate Guided Loss for Weakly Supervised Semantic Segmentation<br>作者：Chunfeng Song, Yan Huang, Wanli Ouyang, Liang Wang<br>论文链接：<a href="https://arxiv.org/abs/1904.11693">https://arxiv.org/abs/1904.11693</a></p><p>35、Bidirectional Learning for Domain Adaptation of Semantic Segmentation<br>作者：Yunsheng Li, Lu Yuan, Nuno Vasconcelos<br>论文链接：<a href="https://arxiv.org/abs/1904.10620">https://arxiv.org/abs/1904.10620</a><br>源码链接：<a href="https://github.com/liyunsheng13/BDL">https://github.com/liyunsheng13/BDL</a></p><p>36、Learning Unsupervised Video Primary Object Segmentation through Visual Attention<br>作者：Wenguan Wang, Hongmei Song, Shuyang Zhao, Jianbing Shen, Sanyuan Zhao, Steven Chu Hong Hoi, and Haibin Ling<br>论文链接：<a href="https://www.dabi.temple.edu/~hbling/publication/UVOS-cvpr19.pdf">https://www.dabi.temple.edu/~hbling/publication/UVOS-cvpr19.pdf</a><br>源码链接：<a href="https://github.com/wenguanwang/AGS">https://github.com/wenguanwang/AGS</a></p><p>37、Elastic Boundary Projection for 3D Medical Image Segmentation<br>作者：Tianwei Ni, Lingxi Xie, Huangjie Zheng, Elliot K. Fishman, Alan L. Yuille<br>论文链接：<a href="https://arxiv.org/abs/1812.00518">https://arxiv.org/abs/1812.00518</a><br>源码链接：<a href="https://github.com/twni2016/Elastic-Boundary-Projection">https://github.com/twni2016/Elastic-Boundary-Projection</a></p><p>38、Seamless Scene Segmentation<br>作者：Lorenzo Porzi, Samuel Rota Bulò, Aleksander Colovic, Peter Kontschieder<br>论文链接：<a href="https://arxiv.org/abs/1905.01220">https://arxiv.org/abs/1905.01220</a></p><p>39、SCOPS: Self-Supervised Co-Part Segmentation<br>作者：Wei-Chih Hung, Varun Jampani, Sifei Liu, Pavlo Molchanov, Ming-Hsuan Yang, Jan Kautz<br>论文链接：<a href="https://arxiv.org/abs/1905.01298">https://arxiv.org/abs/1905.01298</a><br>项目链接：<a href="https://varunjampani.github.io/scops/">https://varunjampani.github.io/scops/</a></p><p>40、Interactive Full Image Segmentation by Considering All Regions Jointly<br>作者：Eirikur Agustsson, Jasper R. R. Uijlings, Vittorio Ferrari<br>论文链接：<a href="https://arxiv.org/pdf/1812.01888.pdf">https://arxiv.org/pdf/1812.01888.pdf</a></p><p>41、An End-to-End Network for Panoptic Segmentation<br>作者：Huanyu Liu, Chao Peng, Changqian Yu, Jingbo Wang, Xu Liu, Gang Yu, Wei Jiang<br>论文链接：<a href="https://arxiv.org/pdf/1903.05027.pdf">https://arxiv.org/pdf/1903.05027.pdf</a></p><p>42、Panoptic Feature Pyramid Networks（Oral）<br>作者：Alexander Kirillov, Ross Girshick, Kaiming He, Piotr Dollár<br>论文链接：<a href="https://arxiv.org/pdf/1901.02446.pdf">https://arxiv.org/pdf/1901.02446.pdf</a></p><p>43、Panoptic Segmentation<br>作者：Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár<br>论文链接：<a href="https://arxiv.org/pdf/1801.00868.pdf">https://arxiv.org/pdf/1801.00868.pdf</a></p><p>44、DARNet: Deep Active Ray Network for Building Segmentation<br>作者：Dominic Cheng, Renjie Liao, Sanja Fidler, Raquel Urtasun<br>论文链接：<a href="https://arxiv.org/abs/1905.05889">https://arxiv.org/abs/1905.05889</a></p><p>45、Amodal Instance Segmentation through KINS Dataset<br>作者：Lu Qi, Li Jiang, Shu Liu, Xiaoyong Shen, Jiaya Jia<br>论文链接：<a href="https://jiaya.me/papers/amodel_cvpr19.pdf">https://jiaya.me/papers/amodel_cvpr19.pdf</a></p><p>46、DeepCO3: Deep Instance Co-segmentation by Co-peak Search and Co-saliency Detection(Oral)<br>作者：Kuang-Jui Hsu, Yen-Yu Lin, Yung-Yu Chuang<br>论文链接：<a href="https://www.csie.ntu.edu.tw/~cyy/publications/papers/Hsu2019DCO.pdf">https://www.csie.ntu.edu.tw/~cyy/publications/papers/Hsu2019DCO.pdf</a><br>源码链接：<a href="https://github.com/KuangJuiHsu/DeepCO3">https://github.com/KuangJuiHsu/DeepCO3</a></p><p>47、ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation<br>作者：Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, Patrick Pérez<br>论文链接：<a href="https://arxiv.org/abs/1811.12833">https://arxiv.org/abs/1811.12833</a><br>源码链接：<a href="https://github.com/valeoai/ADVENT">https://github.com/valeoai/ADVENT</a></p><p>48、Collaborative Global-Local Networks for Memory-Efﬁcient Segmentation of Ultra-High Resolution Images(Oral)<br>作者：Wuyang Chen, Ziyu Jiang, Zhangyang Wang, Kexin Cui, and Xiaoning Qian<br>论文链接：<a href="https://arxiv.org/abs/1905.06368">https://arxiv.org/abs/1905.06368</a><br>源码链接：<a href="https://github.com/chenwydj/ultra_high_resolution_segmentation">https://github.com/chenwydj/ultra_high_resolution_segmentation</a></p><p>49、S4Net: Single Stage Salient-Instance Segmentation（南开大学）<br>作者：Ruochen Fan, Ming-Ming Cheng, Qibin Hou, Tai-Jiang Mu, Jingdong Wang, Shi-Min Hu<br>论文链接：<a href="https://mftp.mmcheng.net/Papers/19cvprS4Net.pdf">https://mftp.mmcheng.net/Papers/19cvprS4Net.pdf</a><br>源码链接：<a href="https://github.com/RuochenFan/S4Net">https://github.com/RuochenFan/S4Net</a><br>项目链接：<a href="https://mmcheng.net/s4net/">https://mmcheng.net/s4net/</a></p><p>50、Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation(Oral)<br>作者：Peng Zhang, Fuhao Zou, Zhiwen Wu, Nengli Dai, Skarpness Mark, Michael Fu, Juan Zhao, Kai Li<br>论文链接：<a href="https://arxiv.org/abs/1901.02985">https://arxiv.org/abs/1901.02985</a><br>源码链接：<a href="https://github.com/tensorflow/models/tree/master/research/deeplab">https://github.com/tensorflow/models/tree/master/research/deeplab</a></p><h3 id="分类、识别-19"><a href="#分类、识别-19" class="headerlink" title="分类、识别 19"></a>分类、识别 19</h3><p>1、Learning a Deep ConvNet for Multi-label Classification with Partial Labels(分类)<br>作者：Thibaut Durand, Nazanin Mehrasa, Greg Mori<br>论文链接：<a href="https://arxiv.org/abs/1902.09720">https://arxiv.org/abs/1902.09720</a></p><p>2、Efficient Video Classification Using Fewer Frames<br>作者：Shweta Bhardwaj, Mukundhan Srinivasan, Mitesh M. Khapra<br>论文链接：<a href="https://arxiv.org/abs/1902.10640">https://arxiv.org/abs/1902.10640</a></p><p>3、Weakly Supervised Complementary Parts Models for Fine-Grained Image Classification from the Bottom Up<br>作者：Weifeng Ge, Xiangru Lin, Yizhou Yu<br>论文链接：<a href="https://arxiv.org/abs/1903.02827">https://arxiv.org/abs/1903.02827</a></p><p>4、All You Need is a Few Shifts: Designing Efficient Convolutional Neural Networks for Image Classification（分类）<br>作者：Weijie Chen, Di Xie, Yuan Zhang, Shiliang Pu<br>论文链接：<a href="https://arxiv.org/abs/1903.05285">https://arxiv.org/abs/1903.05285</a></p><p>5、Bag of Tricks for Image Classification with Convolutional Neural Networks<br>作者：Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li<br>论文链接：<a href="https://arxiv.org/abs/1812.01187">https://arxiv.org/abs/1812.01187</a><br>源码链接：<a href="https://github.com/dmlc/gluon-cv">https://github.com/dmlc/gluon-cv</a><br>论文解读：<a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid=2247486778&amp;idx=2&amp;sn=23582d015eff1d0d5ba0c6f71ca86296&amp;chksm=ec1fe0c3db6869d588af077e6041377193cee8c8eeb069f283bdf6b9a2613bb7dc6b4c7b365a&amp;mpshare=1&amp;scene=1&amp;srcid=0318W4ZKu3BiCVeuKx6lLMrc#rd">图像分类技巧：Bag of Tricks for Image Classification with Convolutional Neural Networks</a></p><p>6、Direct Object Recognition Without Line-of-Sight Using Optical Coherence(目标识别）<br>作者：Xin Lei, Liangyu He, Yixuan Tan, Ken Xingze Wang, Xinggang Wang, Yihan Du, Shanhui Fan, Zongfu Yu<br>论文链接：<a href="https://arxiv.org/abs/1903.07705">https://arxiv.org/abs/1903.07705</a></p><p>7、Direct Object Recognition Without Line-of-Sight Using Optical Coherence(非视距物体识别技术)<br>作者：Xin Lei, Liangyu He, Yixuan Tan, Ken Xingze Wang, Xinggang Wang, Yihan Du, Shanhui Fan, Zongfu Yu<br>论文链接：<a href="https://arxiv.org/abs/1903.07705">https://arxiv.org/abs/1903.07705</a></p><p>8、C2AE: Class Conditioned Auto-Encoder for Open-set Recognition(Oral)<br>作者：Poojan Oza, Vishal M Patel<br>论文链接：<a href="https://arxiv.org/abs/1904.01198">https://arxiv.org/abs/1904.01198</a></p><p>9、Multispectral Imaging for Fine-Grained Recognition of Powders on Complex Backgrounds(Oral)<br>作者：Tiancheng Zhi, Bernardo R. Pires, Martial Hebert and Srinivasa G. Narasimhan<br>论文链接：<a href="https://www.cs.cmu.edu/~ILIM/projects/IM/MSPowder/files/ZPHN-CVPR19.pdf">https://www.cs.cmu.edu/~ILIM/projects/IM/MSPowder/files/ZPHN-CVPR19.pdf</a><br>代码链接：<a href="https://github.com/tiancheng-zhi/ms-powder">https://github.com/tiancheng-zhi/ms-powder</a><br>项目链接：<a href="https://www.cs.cmu.edu/~ILIM/projects/IM/MSPowder/">https://www.cs.cmu.edu/~ILIM/projects/IM/MSPowder/</a></p><p>10、Large-Scale Long-Tailed Recognition in an Open World（Oral)<br>作者：Ziwei Liu<em>, Zhongqi Miao</em>, Xiaohang Zhan, Jiayun Wang, Boqing Gong, Stella X. Yu<br>论文链接：<a href="https://github.com/ofsoundof/3D_Appearance_SR/blob/master/code/scripts/3d_appearance_sr.pdf">https://github.com/ofsoundof/3D_Appearance_SR/blob/master/code/scripts/3d_appearance_sr.pdf</a><br>源码链接：<a href="https://github.com/zhmiao/OpenLongTailRecognition-OLTR">https://github.com/zhmiao/OpenLongTailRecognition-OLTR</a></p><p>11、Multi-Label Image Recognition with Graph Convolutional Networks（多标签图像识别）<br>作者：Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, Yanwen Guo<br>论文链接：<a href="https://arxiv.org/abs/1904.03582">https://arxiv.org/abs/1904.03582</a><br>源码链接：<a href="https://github.com/chenzhaomin123/ML_GCN">https://github.com/chenzhaomin123/ML_GCN</a><br>简介：本工作针对多标记识别的核心问题，即“如何有效建模标记间的协同关系”进行探索，提出基于图卷积（GCN）的端到端系统，通过data-driven方式建立标记间有向图（directed graph）并由GCN将类别标记映射（mapping）为对应类别分类器，以此建模类别关系，同时可提升表示学习能力。此外针对GCN中的关键元素correlation matrix进行了深入分析和重设计，使其更胜任多标记问题。</p><p>12、Gait Recognition via Disentangled Representation Learning（Oral 步态识别）<br>作者：Ziyuan Zhang, Luan Tran, Xi Yin, Yousef Atoum, Xiaoming Liu, Jian Wan, Nanxin Wang<br>论文链接：<a href="https://arxiv.org/abs/1904.04925">https://arxiv.org/abs/1904.04925</a></p><p>13、Adaptively Connected Neural Networks（分类）<br>作者：Guangrun Wang, Keze Wang, Liang Lin<br>论文链接：<a href="https://arxiv.org/abs/1904.03579">https://arxiv.org/abs/1904.03579</a><br>源码链接：<a href="https://github.com/wanggrun/Adaptively-Connected-Neural-Networks">https://github.com/wanggrun/Adaptively-Connected-Neural-Networks</a></p><p>14、Aggregation Cross-Entropy for Sequence Recognition<br>作者：Zecheng Xie, Yaoxiong Huang, Yuanzhi Zhu, Lianwen Jin, Yuliang Liu, Lele Xie<br>论文链接：<a href="https://arxiv.org/abs/1904.08364">https://arxiv.org/abs/1904.08364</a></p><p>15、Meta-learning Convolutional Neural Architectures for Multi-target Concrete Defect Classification with the COncrete DEfect BRidge IMage Dataset<br>作者：Martin Mundt, Sagnik Majumder, Sreenivas Murali, Panagiotis Panetsos, Visvanathan Ramesh<br>论文链接：<a href="https://arxiv.org/abs/1904.08486">https://arxiv.org/abs/1904.08486</a></p><p>16、Unsupervised Open Domain Recognition by Semantic Discrepancy Minimization<br>作者：Junbao Zhuo, Shuhui Wang, Shuhao Cui, Qingming Huang<br>论文链接：<a href="https://arxiv.org/abs/1904.08631">https://arxiv.org/abs/1904.08631</a></p><p>17、Translate-to-Recognize Networks for RGB-D Scene Recognition<br>作者：Dapeng Du, Limin Wang, Huiling Wang, Kai Zhao, Gangshan Wu<br>论文链接：<a href="https://arxiv.org/abs/1904.12254">https://arxiv.org/abs/1904.12254</a><br>源码链接：<a href="https://ownstyledu.github.io/Translate-to-Recognize-Networks/">https://ownstyledu.github.io/Translate-to-Recognize-Networks/</a></p><p>18、Progressive Ensemble Networks for Zero-Shot Recognition<br>作者：Meng Ye, Yuhong Guo<br>论文链接：<a href="https://arxiv.org/pdf/1805.07473.pdf">https://arxiv.org/pdf/1805.07473.pdf</a></p><p>19、Generalized Zero-Shot Recognition based on Visually Semantic Embedding<br>作者：Pengkai Zhu, Hanxiao Wang, Venkatesh Saligrama<br>论文链接：<a href="https://arxiv.org/pdf/1811.07993.pdf">https://arxiv.org/pdf/1811.07993.pdf</a></p><h3 id="跟踪-20"><a href="#跟踪-20" class="headerlink" title="跟踪 20"></a>跟踪 20</h3><p>1、Fast Online Object Tracking and Segmentation: A Unifying Approach(SiamMask,目标跟踪）<br>作者：Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, Philip H.S. Torr<br>论文链接：<a href="https://arxiv.org/abs/1812.05050">https://arxiv.org/abs/1812.05050</a><br>Github链接：<a href="https://github.com/foolwood/SiamMask">https://github.com/foolwood/SiamMask</a><br>project链接：<a href="https://www.robots.ox.ac.uk/~qwang/SiamMask/">https://www.robots.ox.ac.uk/~qwang/SiamMask/</a><br>论文解读：<a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid=2247487638&amp;idx=1&amp;sn=5d3891860d29e994dd1241e19fd2cb19&amp;chksm=ec1ffd6fdb68747980ff7ae61094e708e7e09059daddac75670b2e0cb41ec72dcf5f325ff3ae&amp;token=1417050475&amp;lang=zh_CN#rd">CVPR2019 | SiamMask：视频跟踪最高精度</a></p><p>2、Deeper and Wider Siamese Networks for Real-Time Visual Tracking(CIR,目标跟踪）<br>作者：Zhipeng Zhang, Houwen Peng<br>论文链接：<a href="https://arxiv.org/pdf/1901.01660.pdf">https://arxiv.org/pdf/1901.01660.pdf</a><br>Code链接：<a href="https://gitlab.com/MSRA_NLPR/deeper_wider_siamese_trackers">https://gitlab.com/MSRA_NLPR/deeper_wider_siamese_trackers</a></p><p>3、SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks(目标跟踪）<br>作者：Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, Junjie Yan<br>论文链接：<a href="https://arxiv.org/pdf/1812.11703.pdf">https://arxiv.org/pdf/1812.11703.pdf</a><br>Project链接：<a href="https://bo-li.info/SiamRPN++/">https://bo-li.info/SiamRPN++/</a><br>论文解读：<a href="https://mp.weixin.qq.com/s/dB5u2No8eakLnrjto0kvyQ">https://mp.weixin.qq.com/s/dB5u2No8eakLnrjto0kvyQ</a></p><p>4、Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking(CRPN,目标跟踪）<br>作者：Heng Fan, Haibin Ling<br>论文链接：<a href="https://arxiv.org/pdf/1812.06148.pdf">https://arxiv.org/pdf/1812.06148.pdf</a></p><p>5、LaSOT: A High-quality Benchmark for Large-scale Single Object Tracking(目标跟踪）<br>作者：Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, Haibin Ling<br>论文链接：<a href="https://arxiv.org/pdf/1809.07845.pdf">https://arxiv.org/pdf/1809.07845.pdf</a><br>project链接：<a href="https://cis.temple.edu/lasot/">https://cis.temple.edu/lasot/</a></p><p>6、Leveraging Shape Completion for 3D Siamese Tracking<br>作者：Silvio Giancola, Jesus Zarzar, Bernard Ghanem<br>论文链接：<a href="https://arxiv.org/abs/1903.01784">https://arxiv.org/abs/1903.01784</a></p><p>7、Cross-Classification Clustering: An Efficient Multi-Object Tracking Technique for 3-D Instance Segmentation in Connectomics（多目标跟踪)<br>作者：Yaron Meirovitch, Lu Mi, Hayk Saribekyan, Alexander Matveev, David Rolnick, Casimir Wierzynski, Nir Shavit<br>论文链接：<a href="https://arxiv.org/abs/1812.01157">https://arxiv.org/abs/1812.01157</a></p><p>8、Multiview 2D/3D Rigid Registration via a Point-Of-Interest Network for Tracking and Triangulation (POINT^2)<br>作者：Haofu Liao, Wei-An Lin, Jiarui Zhang, Jingdan Zhang, Jiebo Luo, S. Kevin Zhou<br>论文链接：<a href="https://arxiv.org/abs/1903.03896">https://arxiv.org/abs/1903.03896</a></p><p>9、Inverse Path Tracing for Joint Material and Lighting Estimation(Oral)<br>作者：Jiaxin Cheng, Yue Wu, Wael Abd-Almageed, Premkumar Natarajan<br>论文链接：<a href="https://arxiv.org/abs/1903.07145">https://arxiv.org/abs/1903.07145</a></p><p>10、Inverse Path Tracing for Joint Material and Lighting Estimation(Oral)<br>作者：Jiaxin Cheng, Yue Wu, Wael Abd-Almageed, Premkumar Natarajan<br>论文链接：<a href="https://arxiv.org/abs/1903.07145">https://arxiv.org/abs/1903.07145</a></p><p>11、Multi-person Articulated Tracking with Spatial and Temporal Embeddings<br>作者：Sheng Jin, Wentao Liu, Wanli Ouyang, Chen Qian<br>论文链接：<a href="https://arxiv.org/abs/1903.09214">https://arxiv.org/abs/1903.09214</a></p><p>12、CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification<br>作者：Zheng Tang, Milind Naphade, Ming-Yu Liu, Xiaodong Yang, Stan Birchfield, Shuo Wang, Ratnesh Kumar, David Anastasiu, Jenq-Neng Hwang<br>论文链接：<a href="https://arxiv.org/abs/1903.09254">https://arxiv.org/abs/1903.09254</a></p><p>13、MOTS: Multi-Object Tracking and Segmentation<br>作者：Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, Bastian Leibe<br>论文链接：<a href="https://arxiv.org/abs/1902.03604">https://arxiv.org/abs/1902.03604</a></p><p>14、Target-Aware Deep Tracking<br>作者：Xin Li, Chao Ma, Baoyuan Wu, Zhenyu He, Ming-Hsuan Yang<br>论文链接：<a href="https://arxiv.org/pdf/1904.01772.pdf">https://arxiv.org/pdf/1904.01772.pdf</a></p><p>15、Unsupervised Deep Tracking<br>作者：Ning Wang, Yibing Song, Chao Ma, Wengang Zhou, Wei Liu, Houqiang Li<br>论文链接：<a href="https://arxiv.org/pdf/1904.01828.pdf">https://arxiv.org/pdf/1904.01828.pdf</a></p><p>16、Beyond Tracking: Selecting Memory and Refining Poses for Deep Visual Odometry（Oral)<br>作者：Fei Xue, Xin Wang, Shunkai Li, Qiuyuan Wang, Junqiu Wang, Hongbin Zha<br>论文链接：<a href="https://arxiv.org/abs/1904.01892">https://arxiv.org/abs/1904.01892</a></p><p>17、SPM-Tracker: Series-Parallel Matching for Real-Time Visual Object Tracking（视觉跟踪）<br>作者：Guangting Wang, Chong Luo, Zhiwei Xiong, Wenjun Zeng<br>论文链接：<a href="https://arxiv.org/abs/1904.04452">https://arxiv.org/abs/1904.04452</a></p><p>18、Graph Convolutional Tracking<br>作者：Junyu Gao，Tianzhu Zhang，Changsheng Xu<br>论文链接：<a href="https://nlpr-web.ia.ac.cn/mmc/homepage/jygao/gct_cvpr2019.html">https://nlpr-web.ia.ac.cn/mmc/homepage/jygao/gct_cvpr2019.html</a></p><p>19、ATOM: Accurate Tracking by Overlap Maximization(Oral,目标跟踪)<br>作者：Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, Michael Felsberg<br>论文链接：<a href="https://arxiv.org/abs/1811.07628">https://arxiv.org/abs/1811.07628</a><br>源码链接：<a href="https://github.com/visionml/pytracking">https://github.com/visionml/pytracking</a></p><p>20、Visual Tracking via Adaptive Spatially-Regularized Correlation Filters（视觉跟踪）<br>作者：Kenan Dai, Dong Wang, Huchuan Lu, Chong Sun, Jianhua Li<br>论文链接：<a href="https://pan.baidu.com/s/1sD-w1tHfA5ZwIbrnCtUI2Q">https://pan.baidu.com/s/1sD-w1tHfA5ZwIbrnCtUI2Q</a><br>源码链接：<a href="https://github.com/Daikenan/ASRCF">https://github.com/Daikenan/ASRCF</a></p><h3 id="人脸-12"><a href="#人脸-12" class="headerlink" title="人脸 12"></a>人脸 12</h3><p>1、Disentangled Representation Learning for 3D Face Shape<br>作者：Baris Gecer, Stylianos Ploumpis, Irene Kotsia, Stefanos Zafeiriou<br>论文链接：<a href="https://arxiv.org/abs/1902.05978">https://arxiv.org/abs/1902.05978</a></p><p>2、Joint Face Detection and Facial Motion Retargeting for Multiple Faces<br>作者：Bindita Chaudhuri, Noranart Vesdapunt, Baoyuan Wang<br>论文链接：<a href="https://arxiv.org/abs/1902.10744">https://arxiv.org/abs/1902.10744</a></p><p>3、ArcFace: Additive Angular Margin Loss for Deep Face Recognition（人脸识别）<br>作者：Jiankang Deng, Jia Guo, Niannan Xue, Stefanos Zafeiriou<br>论文链接：<a href="https://arxiv.org/abs/1801.07698">https://arxiv.org/abs/1801.07698</a><br>Demo链接：<a href="https://github.com/vita-epfl/openpifpafwebdemo">https://github.com/vita-epfl/openpifpafwebdemo</a></p><p>4、Linkage Based Face Clustering via Graph Convolution Network<br>作者：Zhongdao Wang, Liang Zheng, Yali Li, Shengjin Wang<br>论文链接：<a href="https://arxiv.org/abs/1903.11306">https://arxiv.org/abs/1903.11306</a></p><p>5、Learning to Cluster Faces on an Affinity Graph<br>作者：Lei Yang, Xiaohang Zhan, Dapeng Chen, Junjie Yan, Chen Change Loy, Dahua Lin<br>论文链接：<a href="https://arxiv.org/abs/1904.02749">https://arxiv.org/abs/1904.02749</a></p><p>6、Deep Tree Learning for Zero-shot Face Anti-Spoofing(Oral)<br>作者：Yaojie Liu, Joel Stehouwer, Amin Jourabloo, Xiaoming Liu<br>论文链接：<a href="https://arxiv.org/abs/1904.02860">https://arxiv.org/abs/1904.02860</a></p><p>7、Efficient Decision-based Black-box Adversarial Attacks on Face Recognition（人脸识别）<br>作者：Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, Jun Zhu<br>论文链接：<a href="https://arxiv.org/abs/1904.04433">https://arxiv.org/abs/1904.04433</a></p><p>8、Towards High-fidelity Nonlinear 3D Face Morphable Model<br>作者：Luan Tran, Feng Liu, Xiaoming Liu<br>论文链接：<a href="https://arxiv.org/abs/1904.04933">https://arxiv.org/abs/1904.04933</a><br>项目链接：<a href="https://cvlab.cse.msu.edu/project-nonlinear-3dmm.html">https://cvlab.cse.msu.edu/project-nonlinear-3dmm.html</a></p><p>9、LBVCNN: Local Binary Volume Convolutional Neural Network for Facial Expression Recognition from Image Sequences<br>作者：Sudhakar Kumawat, Manisha Verma, Shanmuganathan Raman<br>论文链接：<a href="https://arxiv.org/abs/1904.07647">https://arxiv.org/abs/1904.07647</a></p><p>10、Hierarchical Cross-Modal Talking Face Generationwith Dynamic Pixel-Wise Loss<br>作者：Lele Chen, Ross K. Maddox, Zhiyao Duan, Chenliang Xu<br>论文链接：<a href="https://arxiv.org/abs/1905.03820">https://arxiv.org/abs/1905.03820</a></p><p>11、Deep Face Recognition via Exclusive Regularization(南开大学，人脸识别)<br>作者：Kai Zhao, Jingyi Xu, Ming-Ming Cheng<br>论文链接：<a href="https://mftp.mmcheng.net/Papers/19cvprRegularFace.pdf">https://mftp.mmcheng.net/Papers/19cvprRegularFace.pdf</a></p><p>12、A Dataset and Benchmark for Large-scale Multi-modal Face Anti-spoofing<br>作者：Sachin Mehta, Mohammad Rastegari, Linda Shapiro, Hannaneh Hajishirzi<br>论文链接：<a href="https://arxiv.org/abs/1811.11431">https://arxiv.org/abs/1811.11431</a><br>源码链接：<a href="https://github.com/sacmehta/ESPNetv2">https://github.com/sacmehta/ESPNetv2</a></p><p>13、MVF-Net: Multi-View 3D Face Morphable Model Regression</p><p>MVF-Net：多视图3D面部可变模型回归</p><p>作者：Fanzi Wu, Linchao Bao, Yajing Chen, Yonggen Ling, Yibing Song, Songnan Li, King Ngi Ngan, Wei Liu<br>论文链接：<a href="https://arxiv.org/abs/1904.04473">https://arxiv.org/abs/1904.04473</a><br>源码链接：<a href="https://github.com/Fanziapril/mvfnet">https://github.com/Fanziapril/mvfnet</a></p><h3 id="人体姿态估计-手势姿态估计、位姿估计-26"><a href="#人体姿态估计-手势姿态估计、位姿估计-26" class="headerlink" title="人体姿态估计/手势姿态估计、位姿估计 26"></a>人体姿态估计/手势姿态估计、位姿估计 26</h3><p>1、Deep High-Resolution Representation Learning for Human Pose Estimation(目前SOTA,已经开源)<br>作者：Ke Sun, Bin Xiao, Dong Liu, Jingdong Wang<br>论文链接：<a href="https://128.84.21.199/abs/1902.09212">https://128.84.21.199/abs/1902.09212</a><br>代码链接：<a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch">https://github.com/leoxiaobin/deep-high-resolution-net.pytorch</a><br>论文解读：<a href="https://mp.weixin.qq.com/s/ZRCzBTBmlEzQCVo1HLWtbQ">https://mp.weixin.qq.com/s/ZRCzBTBmlEzQCVo1HLWtbQ</a></p><p>2、DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion<br>作者：Chen Wang, Danfei Xu, Yuke Zhu, Roberto Martín-Martín, Cewu Lu, Li Fei-Fei, Silvio Savarese<br>论文链接：<a href="https://arxiv.org/abs/1901.04780">https://arxiv.org/abs/1901.04780</a><br>论文解读：<a href="https://mp.weixin.qq.com/s/wrND2cocWlPPVXPqpq-Glg">https://mp.weixin.qq.com/s/wrND2cocWlPPVXPqpq-Glg</a></p><p>3、RepNet: Weakly Supervised Training of an Adversarial Reprojection Network for 3D Human Pose Estimation<br>作者：Bastian Wandt, Bodo Rosenhahn<br>论文链接：<a href="https://arxiv.org/abs/1902.09868">https://arxiv.org/abs/1902.09868</a></p><p>4、3D Hand Shape and Pose Estimation from a Single RGB Image<br>作者：Liuhao Ge, Zhou Ren, Yuncheng Li, Zehao Xue, Yingying Wang, Jianfei Cai, Junsong Yuan<br>论文链接：<a href="https://arxiv.org/abs/1903.00812">https://arxiv.org/abs/1903.00812</a></p><p>5、Self-Supervised Learning of 3D Human Pose using Multi-view Geometry<br>作者：Muhammed Kocabas, Salih Karagoz, Emre Akbas<br>论文链接：<a href="https://arxiv.org/abs/1903.02330">https://arxiv.org/abs/1903.02330</a><br>Github链接：<a href="https://github.com/mkocabas/EpipolarPose">https://github.com/mkocabas/EpipolarPose</a></p><p>6、Fast and Robust Multi-Person 3D Pose Estimation from Multiple Views<br>作者：Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, Xiaowei Zhou<br>论文链接：<a href="https://arxiv.org/abs/1901.04111">https://arxiv.org/abs/1901.04111</a><br>项目链接：<a href="https://zju-3dv.github.io/mvpose/">https://zju-3dv.github.io/mvpose/</a><br>代码链接：<a href="https://github.com/zju-3dv/mvpose">https://github.com/zju-3dv/mvpose</a></p><p>7、Extreme Relative Pose Estimation for RGB-D Scans via Scene Completion (Oral)<br>作者：Zhenpei Yang, Jeffrey Z.Pan, Linjie Luo, Xiaowei Zhou, Kristen Grauman and Qixing Huang<br>论文链接：<a href="https://arxiv.org/pdf/1901.00063.pdf">https://arxiv.org/pdf/1901.00063.pdf</a><br>代码链接: <a href="https://github.com/zhenpeiyang/RelativePose">https://github.com/zhenpeiyang/RelativePose</a></p><p>8、PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation<br>作者：Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou<br>论文链接：<a href="https://arxiv.org/pdf/1812.11788.pdf">https://arxiv.org/pdf/1812.11788.pdf</a></p><p>9、PoseFix: Model-agnostic General Human Pose Refinement Network<br>作者：Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee<br>论文链接：<a href="https://arxiv.org/abs/1812.03595">https://arxiv.org/abs/1812.03595</a><br>源码链接：<a href="https://github.com/mks0601/PoseFix_RELEASE">https://github.com/mks0601/PoseFix_RELEASE</a></p><p>10、Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation(oral)<br>作者：He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, Leonidas J. Guibas<br>论文链接：<a href="https://arxiv.org/abs/1901.02970">https://arxiv.org/abs/1901.02970</a></p><p>11、PifPaf: Composite Fields for Human Pose Estimation(姿态估计）<br>作者：Sven Kreiss, Lorenzo Bertoni, Alexandre Alahi<br>论文链接：<a href="https://arxiv.org/abs/1903.06593">https://arxiv.org/abs/1903.06593</a><br>Demo链接：<a href="https://github.com/vita-epfl/openpifpafwebdemo">https://github.com/vita-epfl/openpifpafwebdemo</a></p><p>12、Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose Estimation(Oral，3D姿态估计)<br>作者：Xipeng Chen, Kwan-Yee Lin, Wentao Liu, Chen Qian, Liang Lin<br>论文链接：<a href="https://arxiv.org/abs/1903.08839">https://arxiv.org/abs/1903.08839</a></p><p>13、CrowdPose: Efficient Crowded Scenes Pose Estimation and A New Benchmark<br>作者：Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, Cewu Lu<br>论文链接：<a href="https://arxiv.org/abs/1812.00324">https://arxiv.org/abs/1812.00324</a><br>代码链接：<a href="https://github.com/Jeff-sjtu/CrowdPose">https://github.com/Jeff-sjtu/CrowdPose</a></p><p>14、Dense Intrinsic Appearance Flow for Human Pose Transfer<br>作者：Yining Li, Chen Huang, Chen Change Loy<br>论文链接：<a href="https://arxiv.org/abs/1903.11326">https://arxiv.org/abs/1903.11326</a></p><p>15、Generating Multiple Hypotheses for 3D Human Pose Estimation with Mixture Density Network<br>作者：Chen Li, Gim Hee Lee<br>论文链接：<a href="https://arxiv.org/abs/1904.05547">https://arxiv.org/abs/1904.05547</a></p><p>16、FSA-Net: Learning Fine-Grained Structure Aggregation for Head Pose Estimation from a Single Image<br>作者：Tsun-Yi Yang, Yi-Ting Chen, Yen-Yu Lin, and Yung-Yu Chuang<br>论文链接：<a href="https://github.com/shamangary/FSA-Net/blob/master/0191.pdf">https://github.com/shamangary/FSA-Net/blob/master/0191.pdf</a><br>源码链接：<a href="https://github.com/shamangary/FSA-Net">https://github.com/shamangary/FSA-Net</a></p><p>17、Segmentation-driven 6D Object Pose Estimation<br>作者：Yinlin Hu, Joachim Hugonot, Pascal Fua, Mathieu Salzmann<br>论文链接：<a href="https://arxiv.org/abs/1812.02541">https://arxiv.org/abs/1812.02541</a><br>源码链接：<a href="https://github.com/cvlab-epfl/segmentation-driven-pose">https://github.com/cvlab-epfl/segmentation-driven-pose</a></p><p>18、Progressive Pose Attention Transfer for Person Image Generation<br>作者：Zhen Zhu, Tengteng Huang, Baoguang Shi, Miao Yu, Bofei Wang, Xiang Bai<br>论文链接：<a href="https://arxiv.org/abs/1904.03349">https://arxiv.org/abs/1904.03349</a><br>源码链接：<a href="https://github.com/tengteng95/Pose-Transfer">https://github.com/tengteng95/Pose-Transfer</a></p><p>19、Multi-Person Pose Estimation with Enhanced Channel-wise and Spatial Information<br>作者：Kai Su, Dongdong Yu, Zhenqi Xu, Xin Geng, Changhu Wang<br>论文链接：<a href="https://arxiv.org/abs/1905.03466">https://arxiv.org/abs/1905.03466</a></p><p>20、Exploiting temporal context for 3D human pose estimation in the wild<br>作者：Anurag Arnab, Carl Doersch, Andrew Zisserman<br>论文链接：<a href="https://arxiv.org/abs/1905.04266">https://arxiv.org/abs/1905.04266</a></p><p>21、In the Wild Human Pose Estimation Using Explicit 2D Features and Intermediate 3D Representations<br>作者：Ikhsanul Habibie, Weipeng Xu, Dushyant Mehta, Gerard Pons-Moll, Christian Theobalt<br>论文链接：<a href="https://arxiv.org/abs/1904.03289">https://arxiv.org/abs/1904.03289</a></p><p>22、CrossInfoNet: Multi-Task Information Sharing Based Hand Pose Estimation（手势姿态估计）<br>作者：Kuo Du, Xiangbo Lin, Yi Sun, Xiaohong Ma<br>论文链接：待更新<br>源码链接：<a href="https://github.com/dumyy/handpose">https://github.com/dumyy/handpose</a></p><p>23、Self supervised 3D hand pose estimation（Oral，手势姿态估计）<br>作者：Shile Li, Dongheui Lee<br>论文链接：<a href="https://www.vision.ee.ethz.ch/~wanc/papers/cvpr2019.pdf">https://www.vision.ee.ethz.ch/~wanc/papers/cvpr2019.pdf</a><br>源码链接：<a href="https://github.com/melonwan/sphereHand">https://github.com/melonwan/sphereHand</a></p><p>24、Point-to-Pose Voting based Hand Pose Estimation using Residual Permutation Equivariant Layer（手势姿态估计）<br>作者：Shile Li, Dongheui Lee<br>论文链接：<a href="https://arxiv.org/pdf/1812.02050.pdf">https://arxiv.org/pdf/1812.02050.pdf</a></p><p>25、Disentangling Latent Hands for Image Synthesis and Pose Estimation<br>作者：Linlin Yang, Angela Yao<br>论文链接：<a href="https://arxiv.org/abs/1812.01002">https://arxiv.org/abs/1812.01002</a></p><p>26、3D human pose estimation in video with temporal convolutions and semi-supervised training(Facebook)<br>作者：Dario Pavllo, Christoph Feichtenhofer, David Grangier, Michael Auli<br>论文链接：<a href="https://research.fb.com/wp-content/uploads/2019/05/3D-human-pose-estimation-in-video-with-temporal-convolutions-and-semi-supervised-training.pdf">https://research.fb.com/wp-content/uploads/2019/05/3D-human-pose-estimation-in-video-with-temporal-convolutions-and-semi-supervised-training.pdf</a></p><h3 id="行为-动作识别、手势识别-13"><a href="#行为-动作识别、手势识别-13" class="headerlink" title="行为/动作识别、手势识别 13"></a>行为/动作识别、手势识别 13</h3><p>1、An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition<br>作者：Chenyang Si, Wentao Chen, Wei Wang, Liang Wang, Tieniu Tan<br>论文链接：<a href="https://arxiv.org/abs/1902.09130">https://arxiv.org/abs/1902.09130</a></p><p>2、Improving the Performance of Unimodal Dynamic Hand-Gesture Recognition with Multimodal Training<br>作者：Mahdi Abavisani, Hamid Reza Vaezi Joze, Vishal M. Patel<br>链接：<a href="https://arxiv.org/abs/1812.06145">https://arxiv.org/abs/1812.06145</a></p><p>3、Collaborative Spatio-temporal Feature Learning for Video Action Recognition<br>作者：Chao Li, Qiaoyong Zhong, Di Xie, Shiliang Pu<br>论文链接：<a href="https://arxiv.org/abs/1903.01197">https://arxiv.org/abs/1903.01197</a></p><p>4、Peeking into the Future: Predicting Future Person Activities and Locations in Videos(行为预测）<br>作者：Junwei Liang, Lu Jiang, Juan Carlos Niebles, Alexander Hauptmann, Li Fei-Fei<br>论文链接：<a href="https://arxiv.org/abs/1902.03748">https://arxiv.org/abs/1902.03748</a></p><p>5、Neural Scene Decomposition for Multi-Person Motion Capture<br>作者：Helge Rhodin, Victor Constantin, Isinsu Katircioglu, Mathieu Salzmann, Pascal Fua<br>论文链接：<a href="https://arxiv.org/abs/1903.05684">https://arxiv.org/abs/1903.05684</a></p><p>6、Action Recognition from Single Timestamp Supervision in Untrimmed Videos（动作识别）<br>作者：Davide Moltisanti, Sanja Fidler, Dima Damen<br>论文链接：<a href="https://arxiv.org/abs/1904.04689">https://arxiv.org/abs/1904.04689</a></p><p>7、Pushing the Envelope for RGB-based Dense 3D Hand Pose Estimation via Neural Rendering<br>作者：Seungryul Baek, Kwang In Kim, Tae-Kyun Kim<br>论文链接：<a href="https://arxiv.org/abs/1904.04196">https://arxiv.org/abs/1904.04196</a></p><p>8、Relational Action Forecasting(oral)<br>作者：Chen Sun, Abhinav Shrivastava, Carl Vondrick, Rahul Sukthankar, Kevin Murphy, Cordelia Schmid<br>论文链接：<a href="https://arxiv.org/abs/1904.04231">https://arxiv.org/abs/1904.04231</a></p><p>9、H+O: Unified Egocentric Recognition of 3D Hand-Object Poses and Interactions(Oral)<br>作者：Bugra Tekin, Federica Bogo, Marc Pollefeys<br>论文链接：<a href="https://arxiv.org/abs/1904.05349">https://arxiv.org/abs/1904.05349</a></p><p>10、Out-of-Distribution Detection for Generalized Zero-Shot Action Recognition<br>作者：Devraj Mandal, Sanath Narayan, Saikumar Dwivedi, Vikram Gupta, Shuaib Ahmed, Fahad Shahbaz Khan, Ling Shao<br>论文链接：<a href="https://arxiv.org/abs/1904.08703">https://arxiv.org/abs/1904.08703</a></p><p>11、Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition<br>作者：Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng Wang, and Qi Tian<br>论文链接：<a href="https://arxiv.org/pdf/1904.12659.pdf">https://arxiv.org/pdf/1904.12659.pdf</a></p><p>12、A neural network based on SPD manifold learning for skeleton-based hand gesture recognition<br>作者：Xuan Son Nguyen, Luc Brun, Olivier Lézoray, Sébastien Bougleux<br>论文链接：<a href="https://arxiv.org/abs/1904.12970">https://arxiv.org/abs/1904.12970</a></p><p>13、DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video Action Recognition(Facebook)<br>作者：Zheng Shou, Xudong Lin, Yannis Kalantidis, Laura Sevilla-Lara, Marcus Rohrbach, Shih-Fu Chang, Zhicheng Yan<br>论文链接：<a href="https://research.fb.com/wp-content/uploads/2019/05/DMC-Net-Generating-Discriminative-Motion-Cues-for-Fast-Compressed-Video-Action-Recognition.pdf">https://research.fb.com/wp-content/uploads/2019/05/DMC-Net-Generating-Discriminative-Motion-Cues-for-Fast-Compressed-Video-Action-Recognition.pdf</a></p><h3 id="时序动作检测、视频相关-19"><a href="#时序动作检测、视频相关-19" class="headerlink" title="时序动作检测、视频相关 19"></a>时序动作检测、视频相关 19</h3><p>1、Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning<br>作者：Nayyer Aafaq, Naveed Akhtar, Wei Liu, Syed Zulqarnain Gilani, Ajmal Mian<br>论文链接：<a href="https://arxiv.org/abs/1902.10322">https://arxiv.org/abs/1902.10322</a><br>来源：<a href="https://mp.weixin.qq.com/s/61C-k3Ijy_7ry5B5lRML6Q">https://mp.weixin.qq.com/s/61C-k3Ijy_7ry5B5lRML6Q</a></p><p>2、Single-frame Regularization for Temporally Stable CNNs（视频处理）<br>作者：Gabriel Eilertsen, Rafał K. Mantiuk, Jonas Unger<br>论文链接：<a href="https://arxiv.org/abs/1902.10424">https://arxiv.org/abs/1902.10424</a><br>来源：<a href="https://mp.weixin.qq.com/s/61C-k3Ijy_7ry5B5lRML6Q">https://mp.weixin.qq.com/s/61C-k3Ijy_7ry5B5lRML6Q</a></p><p>3、Neural RGB-D Sensing: Depth estimation from a video<br>作者：Chao Liu, Jinwei Gu, Kihwan Kim, Srinivasa Narasimhan, Jan Kautz<br>论文链接：<a href="https://arxiv.org/pdf/1901.02571.pdf">https://arxiv.org/pdf/1901.02571.pdf</a><br>project链接：<a href="https://research.nvidia.com/publication/2019-06_Neural-RGBD">https://research.nvidia.com/publication/2019-06_Neural-RGBD</a></p><p>4、Competitive Collaboration: Joint Unsupervised Learning of Depth, CameraMotion, Optical Flow and Motion Segmentation<br>作者：Anurag Ranjan, Varun Jampani, Kihwan Kim, Deqing Sun, Jonas Wulff, Michael J. Black<br>论文链接：<a href="https://arxiv.org/pdf/1805.09806.pdf">https://arxiv.org/pdf/1805.09806.pdf</a></p><p>5、Representation Flow for Action Recognition<br>作者：AJ Piergiovanni, Michael S. Ryoo<br>论文链接：<a href="https://arxiv.org/abs/1810.01455">https://arxiv.org/abs/1810.01455</a><br>项目链接：<a href="https://piergiaj.github.io/rep-flow-site/">https://piergiaj.github.io/rep-flow-site/</a><br>代码链接：<a href="https://github.com/piergiaj/representation-flow-cvpr19">https://github.com/piergiaj/representation-flow-cvpr19</a></p><p>6、Learning Regularity in Skeleton Trajectories for Anomaly Detection in Videos<br>作者：Romero Morais, Vuong Le, Truyen Tran, Budhaditya Saha, Moussa Mansour, Svetha Venkatesh<br>论文链接：<a href="https://arxiv.org/abs/1903.03295">https://arxiv.org/abs/1903.03295</a></p><p>7、Video Generation from Single Semantic Label Map<br>作者：Junting Pan, Chengyu Wang, Xu Jia, Jing Shao, Lu Sheng, Junjie Yan, Xiaogang Wang<br>论文链接：<a href="https://arxiv.org/abs/1903.04480">https://arxiv.org/abs/1903.04480</a><br>源码链接：<a href="https://github.com/junting/seg2vid/tree/master">https://github.com/junting/seg2vid/tree/master</a></p><p>8、Inserting Videos into Videos<br>作者：Donghoon Lee, Tomas Pfister, Ming-Hsuan Yang<br>论文链接：<a href="https://arxiv.org/abs/1903.06571">https://arxiv.org/abs/1903.06571</a></p><p>9、Recurrent Back-Projection Network for Video Super-Resolution<br>作者：Muhammad Haris, Greg Shakhnarovich, Norimichi Ukita<br>论文链接：<a href="https://alterzero.github.io/projects/rbpn_cvpr2019.pdf">https://alterzero.github.io/projects/rbpn_cvpr2019.pdf</a><br>代码链接：<a href="https://github.com/alterzero/RBPN-PyTorch">https://github.com/alterzero/RBPN-PyTorch</a><br>项目链接：<a href="https://alterzero.github.io/projects/RBPN.html">https://alterzero.github.io/projects/RBPN.html</a></p><p>10、Depth-Aware Video Frame Interpolation<br>作者：Wenbo Bao Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang<br>论文链接：<a href="https://sites.google.com/view/wenbobao/dain">https://sites.google.com/view/wenbobao/dain</a><br>代码链接：<a href="https://github.com/baowenbo/DAIN">https://github.com/baowenbo/DAIN</a></p><p>11、Video Relationship Reasoning using Gated Spatio-Temporal Energy Graph<br>作者：Yao-Hung Hubert Tsai, Santosh Divvala, Louis-Philippe Morency, Ruslan Salakhutdinov, Ali Farhadi<br>论文链接：<a href="https://arxiv.org/abs/1903.10547">https://arxiv.org/abs/1903.10547</a></p><p>12、Dual Encoding for Zero-Example Video Retrieval<br>作者：Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan He, Gang Yang and Xun Wang<br>论文链接：<a href="https://arxiv.org/abs/1809.06181">https://arxiv.org/abs/1809.06181</a><br>代码链接：<a href="https://github.com/danieljf24/dual_encoding">https://github.com/danieljf24/dual_encoding</a></p><p>13、Rethinking the Evaluation of Video Summaries<br>作者：Jacques Manderscheid, Amos Sironi, Nicolas Bourdis, Davide Migliore, Vincent Lepetit<br>论文链接：<a href="https://arxiv.org/abs/1903.11328">https://arxiv.org/abs/1903.11328</a></p><p>14、End-to-End Time-Lapse Video Synthesis from a Single Outdoor Image<br>作者：Seonghyeon Nam, Chongyang Ma, Menglei Chai, William Brendel, Ning Xu, Seon Joo Kim<br>论文链接：<a href="https://arxiv.org/abs/1904.00680">https://arxiv.org/abs/1904.00680</a></p><p>15、GolfDB: A Video Database for Golf Swing Sequencing<br>作者：William McNally, Kanav Vats, Tyler Pinto, Chris Dulhanty, John McPhee, Alexander Wong<br>论文链接：<a href="https://arxiv.org/abs/1903.06528v1">https://arxiv.org/abs/1903.06528v1</a></p><p>16、VORNet: Spatio-temporally Consistent Video Inpainting for Object Removal<br>作者：Ya-Liang Chang, Zhe Yu Liu, Winston Hsu<br>论文链接：<a href="https://arxiv.org/abs/1904.06726">https://arxiv.org/abs/1904.06726</a></p><p>17、STEP: Spatio-Temporal Progressive Learning for Video Action Detection（Oral）<br>作者：Xitong Yang, Xiaodong Yang, Ming-Yu Liu, Fanyi Xiao, Larry Davis, Jan Kautz<br>论文链接：<a href="https://arxiv.org/abs/1904.09288">https://arxiv.org/abs/1904.09288</a></p><p>18、UnOS: Unified Unsupervised Optical-flow and Stereo-depth Estimation by Watching Videos<br>作者：Yang Wang, Peng Wang, Zhenheng Yang, Chenxu Luo, Yi Yang, and Wei Xu<br>论文链接：<a href="https://arxiv.org/abs/1810.03654">https://arxiv.org/abs/1810.03654</a></p><p>19、Memory-Attended Recurrent Network for Video Captioning<br>作者：Wenjie Pei, Jiyuan Zhang, Xiangrong Wang, Lei Ke, Xiaoyong Shen, Yu-Wing Tai<br>论文链接：<a href="https://arxiv.org/abs/1905.03966">https://arxiv.org/abs/1905.03966</a></p><h3 id="Related-to-Networks-39"><a href="#Related-to-Networks-39" class="headerlink" title="Related to Networks 39"></a>Related to Networks 39</h3><p>1、RePr: Improved Training of Convolutional Filters<br>作者：Aaditya Prakash, James Storer, Dinei Florencio, Cha Zhang<br>论文链接：<a href="https://arxiv.org/abs/1811.07275">https://arxiv.org/abs/1811.07275</a></p><p>2、Iterative Residual CNNs for Burst Photography Applications<br>作者：Filippos Kokkinos   Stamatis Lefkimmiatis<br>论文链接：<a href="https://arxiv.org/abs/1811.12197">https://arxiv.org/abs/1811.12197</a></p><p>3、SpherePHD: Applying CNNs on a Spherical PolyHeDron Representation of 360 degree Images<br>作者：Yeon Kun Lee, Jaeseok Jeong, Jong Seob Yun, Cho Won June, Kuk-Jin Yoon<br>论文链接：<a href="https://arxiv.org/abs/1811.08196">https://arxiv.org/abs/1811.08196</a></p><p>4、On the Continuity of Rotation Representations in Neural Networks<br>作者：Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, Hao Li<br>论文链接：<a href="https://arxiv.org/pdf/1812.07035.pdf">https://arxiv.org/pdf/1812.07035.pdf</a></p><p>5、Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?<br>作者：Shilin Zhu, Xin Dong, Hao Su<br>论文链接：<a href="https://arxiv.org/abs/1806.07550">https://arxiv.org/abs/1806.07550</a><br>简要：Ensemble of binary neural networks has better stability and robustness, and may perform as well as floating-point networks.</p><p>6、A Neurobiological Evaluation Metric for Neural Network Model Search<br>作者：Nathaniel Blanchard, Jeffery Kinnison, Brandon RichardWebster, Pouya Bashivan, Walter J. Scheirer<br>论文链接：<a href="https://arxiv.org/pdf/1805.10726.pdf">https://arxiv.org/pdf/1805.10726.pdf</a></p><p>7、MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment<br>作者：Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang, Larry S. Davis<br>论文链接：<a href="https://arxiv.org/pdf/1812.00087.pdf">https://arxiv.org/pdf/1812.00087.pdf</a></p><p>8、Multi-Step Prediction of Occupancy Grid Maps with Recurrent Neural Networks<br>作者：Nima Mohajerin, Mohsen Rohani<br>论文链接：<a href="https://arxiv.org/pdf/1812.09395.pdf">https://arxiv.org/pdf/1812.09395.pdf</a></p><p>9、Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem（oral)<br>作者：Matthias Hein, Maksym Andriushchenko, Julian Bitterwolf<br>论文链接：<a href="https://arxiv.org/abs/1812.05720">https://arxiv.org/abs/1812.05720</a><br>Reading Note:In the paper, we give a theoretical argument of why ReLU activation can lead to models with overconfident predictions. Moreover, we propose a robust optimization training scheme that mitigates this problem.</p><p>10、RGBD Based Dimensional Decomposition Residual Network for 3D Semantic Scene Completion<br>作者：Jie Li, Yu Liu, Dong Gong, Qinfeng Shi, Xia Yuan, Chunxia Zhao, Ian Reid<br>论文链接：<a href="https://arxiv.org/abs/1903.00620">https://arxiv.org/abs/1903.00620</a></p><p>11、PartNet: A Recursive Part Decomposition Network for Fine-grained and Hierarchical Shape Segmentation<br>作者：Fenggen Yu, Kun Liu, Yan Zhang, Chenyang Zhu, Kai Xu<br>论文链接：<a href="https://arxiv.org/abs/1903.00709">https://arxiv.org/abs/1903.00709</a></p><p>12、3D Point-Capsule Networks<br>作者：Yongheng Zhao, Tolga Birdal, Haowen Deng, Federico Tombari<br>论文链接：<a href="https://arxiv.org/abs/1812.10775">https://arxiv.org/abs/1812.10775</a></p><p>13、CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning<br>作者：Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, Chunhua Shen<br>论文链接：<a href="https://arxiv.org/abs/1903.02351">https://arxiv.org/abs/1903.02351</a></p><p>14、Path-Invariant Map Networks (Oral)<br>作者：Zaiwei Zhang, Zhenxiao Liang, Lemeng Wu, Xiaowei Zhou and Qixing Huang<br>论文链接：<a href="https://arxiv.org/pdf/1812.11647.pdf">https://arxiv.org/pdf/1812.11647.pdf</a><br>代码链接: <a href="https://github.com/zaiweizhang/path_invariance_map_network">https://github.com/zaiweizhang/path_invariance_map_network</a></p><p>15、A Main/Subsidiary Network Framework for Simplifying Binary Neural Network<br>作者：Yinghao Xu, Xin Dong, Yudian Li, Hao Su<br>论文链接：<a href="https://arxiv.org/abs/1812.04210">https://arxiv.org/abs/1812.04210</a><br>简要：A simple learning-based binary neural network pruning scheme.</p><p>16、Knowledge-Embedded Routing Network for Scene Graph Generation<br>作者：Tianshui Chen, Weihao Yu, Riquan Chen, Liang Lin<br>论文链接：<a href="https://arxiv.org/abs/1903.03326">https://arxiv.org/abs/1903.03326</a></p><p>17、Knowledge-Embedded Routing Network for Scene Graph Generation<br>作者：Tianshui Chen, Weihao Yu, Riquan Chen, Liang Lin<br>论文链接：<a href="https://arxiv.org/abs/1903.03326">https://arxiv.org/abs/1903.03326</a></p><p>18、HetConv: Heterogeneous Kernel-Based Convolutions for Deep CNNs<br>作者：Pravendra Singh, Vinay Kumar Verma, Piyush Rai, Vinay P. Namboodiri<br>论文链接：<a href="https://arxiv.org/abs/1903.04120">https://arxiv.org/abs/1903.04120</a></p><p>19、Large-scale Distributed Second-order Optimization Using Kronecker-factored Approximate Curvature for Deep Convolutional Neural Networks<br>作者：Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, Satoshi Matsuoka<br>论文链接：<a href="https://arxiv.org/abs/1811.12019">https://arxiv.org/abs/1811.12019</a></p><p>20、ADCrowdNet: An Attention-injective Deformable Convolutional Network for Crowd Understanding<br>作者：Ning Liu, Yongchao Long, Changqing Zou, Qun Niu, Li Pan, Hefeng Wu<br>论文链接：<a href="https://arxiv.org/abs/1811.11968">https://arxiv.org/abs/1811.11968</a></p><p>21、LaSO: Label-Set Operations networks for multi-label few-shot learning(oral)<br>作者：Amit Alfassy, Leonid Karlinsky, Amit Aides, Joseph Shtok, Sivan Harary, Rogerio Feris, Raja Giryes, Alex M. Bronstein<br>论文链接：<a href="https://arxiv.org/abs/1902.09811">https://arxiv.org/abs/1902.09811</a></p><p>22、Selective Kernel Networks<br>作者：Xiang Li, Wenhai Wang, Xiaolin Hu, Jian Yang<br>论文链接：<a href="https://arxiv.org/abs/1903.06586">https://arxiv.org/abs/1903.06586</a><br>源码链接：<a href="https://github.com/implus/SKNet">https://github.com/implus/SKNet</a></p><p>23、Self-calibrating Deep Photometric Stereo Networks(Oral)<br>作者：Guanying Chen, Kai Han, Boxin Shi, Yasuyuki Matsushita, Kwan-Yee K. Wong<br>论文链接：<a href="https://arxiv.org/abs/1903.07366">https://arxiv.org/abs/1903.07366</a><br>项目链接：<a href="https://gychen.org/SDPS-Net/">https://gychen.org/SDPS-Net/</a><br>代码链接：<a href="https://github.com/guanyingc/SDPS-Net">https://github.com/guanyingc/SDPS-Net</a></p><p>24、Self-calibrating Deep Photometric Stereo Networks(Oral)<br>作者：Guanying Chen, Kai Han, Boxin Shi, Yasuyuki Matsushita, Kwan-Yee K. Wong<br>论文链接：<a href="https://arxiv.org/abs/1903.07366">https://arxiv.org/abs/1903.07366</a><br>项目链接：<a href="https://gychen.org/SDPS-Net/">https://gychen.org/SDPS-Net/</a><br>代码链接：<a href="https://github.com/guanyingc/SDPS-Net">https://github.com/guanyingc/SDPS-Net</a></p><p>25、Networks for Joint Affine and Non-parametric Image Registration<br>作者：Zhengyang Shen, Xu Han, Zhenlin Xu, Marc Niethammer<br>论文链接：<a href="https://arxiv.org/abs/1903.08811">https://arxiv.org/abs/1903.08811</a></p><p>26、Learning for Single-Shot Confidence Calibration in Deep Neural Networks through Stochastic Inferences<br>作者：Seonguk Seo, Paul Hongsuck Seo, Bohyung Han<br>论文链接：<a href="https://arxiv.org/abs/1810.02358">https://arxiv.org/abs/1810.02358</a></p><p>27、Towards Optimal Structured CNN Pruning via Generative Adversarial Learning<br>作者：Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan Cao, Qixiang Ye, Feiyue Huang, David Doermann<br>论文链接：<a href="https://arxiv.org/abs/1903.09291">https://arxiv.org/abs/1903.09291</a></p><p>28、TIN: Transferable Interactiveness Network<br>作者：Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma, Hao-Shu Fang, Yan-Feng Wang, Cewu Lu<br>论文链接：<a href="https://arxiv.org/abs/1811.08264">https://arxiv.org/abs/1811.08264</a><br>代码链接：<a href="https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network">https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network</a></p><p>29、Convolutional Neural Networks Deceived by Visual Illusions<br>作者：Alexander Gomez-Villa, Adrián Martín, Javier Vazquez-Corral, Marcelo Bertalmío<br>论文链接：<a href="https://arxiv.org/abs/1811.10565">https://arxiv.org/abs/1811.10565</a></p><p>30、Fully Learnable Group Convolution for Acceleration of Deep Neural Networks<br>作者：Xijun Wang, Meina Kan, Shiguang Shan, Xilin Chen<br>论文链接：<a href="https://arxiv.org/abs/1904.00346">https://arxiv.org/abs/1904.00346</a></p><p>31、Kervolutional Neural Networks<br>作者：Chen Wang, Jianfei Yang, Lihua Xie, Junsong Yuan<br>论文链接：<a href="https://arxiv.org/abs/1904.03955">https://arxiv.org/abs/1904.03955</a></p><p>32、Pixel-Adaptive Convolutional Neural Networks<br>作者：Hang Su, Varun Jampani, Deqing Sun, Orazio Gallo, Erik Learned-Miller, Jan Kautz<br>论文链接：<a href="https://arxiv.org/abs/1904.05373">https://arxiv.org/abs/1904.05373</a></p><p>33、Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?<br>作者：Shilin Zhu, Xin Dong, Hao Su<br>论文链接：<a href="https://arxiv.org/abs/1806.07550">https://arxiv.org/abs/1806.07550</a></p><p>34、Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration(Oral)<br>作者：Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, Yi Yang<br>论文链接：<a href="https://arxiv.org/abs/1811.00250">https://arxiv.org/abs/1811.00250</a><br>源码链接：<a href="https://github.com/he-y/filter-pruning-geometric-median">https://github.com/he-y/filter-pruning-geometric-median</a></p><p>35、D2-Net: A Trainable CNN for Joint Detection and Description of Local Features<br>作者：Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torii, Torsten Sattler<br>论文链接：<a href="https://dsmn.ml/publications/d2-net.html">https://dsmn.ml/publications/d2-net.html</a><br>源码链接：<a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></p><p>36、On Implicit Filter Level Sparsity in Convolutional Neural Networks<br>作者：Dushyant Mehta, Kwang In Kim, Christian Theobalt<br>论文链接：<a href="https://arxiv.org/abs/1811.12495">https://arxiv.org/abs/1811.12495</a></p><p>37、Graph-Based Global Reasoning Networks(Facebook)<br>作者：Yunpeng Chen, Marcus Rohrbach, Zhicheng Yan, Shuicheng Yan, Jiashi Feng, Yannis Kalantidis<br>论文链接：<a href="https://research.fb.com/wp-content/uploads/2019/05/Graph-Based-Global-Reasoning-Networks.pdf">https://research.fb.com/wp-content/uploads/2019/05/Graph-Based-Global-Reasoning-Networks.pdf</a></p><p>38、FeatherNets: Convolutional Neural Networks as Light as Feather for Face Anti-spoofing<br>作者：Peng Zhang, Fuhao Zou, Zhiwen Wu, Nengli Dai, Skarpness Mark, Michael Fu, Juan Zhao, Kai Li<br>论文链接：<a href="https://arxiv.org/abs/1904.09290">https://arxiv.org/abs/1904.09290</a><br>源码链接：<a href="https://github.com/SoftwareGift/FeatherNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019">https://github.com/SoftwareGift/FeatherNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019</a></p><p>39、ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network<br>作者：Sachin Mehta, Mohammad Rastegari, Linda Shapiro, Hannaneh Hajishirzi<br>论文链接：<a href="https://arxiv.org/abs/1811.11431">https://arxiv.org/abs/1811.11431</a><br>源码链接：<a href="https://github.com/sacmehta/ESPNetv2">https://github.com/sacmehta/ESPNetv2</a></p><h3 id="GAN、图像文本生成-25"><a href="#GAN、图像文本生成-25" class="headerlink" title="GAN、图像文本生成 25"></a>GAN、图像文本生成 25</h3><p>1、Event-based High Dynamic Range Image and Very High Frame Rate Video Generation using Conditional Generative Adversarial Networks<br>作者：S. Mohammad Mostafavi I., Lin Wang, Yo-Sung Ho, Kuk-Jin Yoon<br>论文链接：<a href="https://arxiv.org/abs/1811.08230">https://arxiv.org/abs/1811.08230</a></p><p>2、Mixture Density Generative Adversarial Networks<br>作者：Hamid Eghbal-zadeh, Werner Zellinger, Gerhard Widmer<br>论文链接：<a href="https://arxiv.org/abs/1811.00152">https://arxiv.org/abs/1811.00152</a></p><p>3、GANFIT: Generative Adversarial Network Fitting for High Fidelity 3D Face Reconstruction<br>作者：Baris Gecer, Stylianos Ploumpis, Irene Kotsia, Stefanos Zafeiriou<br>论文链接：<a href="https://arxiv.org/abs/1902.05978">https://arxiv.org/abs/1902.05978</a><br>github链接：<a href="https://github.com/barisgecer/ganfit">https://github.com/barisgecer/ganfit</a></p><p>4、Self-Supervised Generative Adversarial Networks<br>作者：Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, Neil Houlsby<br>论文链接：<a href="https://arxiv.org/abs/1811.11212">https://arxiv.org/abs/1811.11212</a><br>Github链接：<a href="https://github.com/google/compare_gan">https://github.com/google/compare_gan</a></p><p>5、CollaGAN : Collaborative GAN for Missing Image Data Imputation<br>作者：Dongwook Lee, Junyoung Kim, Won-Jin Moon, Jong Chul Ye<br>论文链接：<a href="https://arxiv.org/abs/1901.09764">https://arxiv.org/abs/1901.09764</a></p><p>6、Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis<br>作者：Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, Ming-Hsuan Yang<br>论文链接：<a href="https://arxiv.org/abs/1903.05628">https://arxiv.org/abs/1903.05628</a><br>代码链接：<a href="https://github.com/HelenMao/MSGAN">https://github.com/HelenMao/MSGAN</a> （待更新）</p><p>7、MirrorGAN: Learning Text-to-image Generation by Redescription（图像文本生成）<br>作者：Tingting Qiao, Jing Zhang, Duanqing Xu, Dacheng Tao<br>论文链接：<a href="https://arxiv.org/abs/1903.05854">https://arxiv.org/abs/1903.05854</a></p><p>8、From Adversarial Training to Generative Adversarial Networks<br>作者：Xuanqing Liu, Cho-Jui Hsieh<br>论文链接：<a href="https://arxiv.org/pdf/1807.10454.pdf">https://arxiv.org/pdf/1807.10454.pdf</a></p><p>9、OCGAN: One-class Novelty Detection Using GANs with Constrained Latent Representations<br>作者：Pramuditha Perera, Ramesh Nallapati, Bing Xiang<br>论文链接：<a href="https://arxiv.org/abs/1903.08550">https://arxiv.org/abs/1903.08550</a></p><p>10、SalGAN: Visual Saliency Prediction with Generative Adversarial Networks（商汤/华为/港中文）<br>作者：Junting Pan, Cristian Canton Ferrer, Kevin McGuinness, Noel E. O’Connor, Jordi Torres, Elisa Sayrol, Xavier Giro-i-Nieto<br>论文链接：<a href="https://arxiv.org/abs/1701.01081">https://arxiv.org/abs/1701.01081</a><br>代码链接：<a href="https://github.com/junting/seg2vid">https://github.com/junting/seg2vid</a></p><p>11、StoryGAN: A Sequential Conditional GAN for Story Visualization（图像文本生成）<br>作者：Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carlson, Jianfeng Gao<br>论文链接：<a href="https://arxiv.org/abs/1812.02784">https://arxiv.org/abs/1812.02784</a><br>代码链接：<a href="https://github.com/yitong91/StoryGAN">https://github.com/yitong91/StoryGAN</a></p><p>12、Object-driven Text-to-Image Synthesis via Adversarial Training（图像文本生成）<br>作者：Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang, Xiaodong He, Siwei Lyu, Jianfeng Gao<br>论文链接：<a href="https://arxiv.org/abs/1902.10740">https://arxiv.org/abs/1902.10740</a></p><p>13、Text2Scene: Generating Compositional Scenes from Textual Descriptions<br>作者：Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carlson, Jianfeng Gao<br>论文链接：<a href="https://arxiv.org/abs/1809.01110">https://arxiv.org/abs/1809.01110</a><br>代码链接：<a href="https://github.com/uvavision/Text2Image">https://github.com/uvavision/Text2Image</a></p><p>14、Image Generation from Layout<br>作者：Bo Zhao, Lili Meng, Weidong Yin, Leonid Sigal<br>论文链接：<a href="https://arxiv.org/abs/1811.11389">https://arxiv.org/abs/1811.11389</a></p><p>15、DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis<br>作者：Minfeng Zhu, Pingbo Pan, Wei Chen, Yi Yang<br>论文链接：<a href="https://arxiv.org/abs/1904.01310">https://arxiv.org/abs/1904.01310</a></p><p>16、Semantics Disentangling for Text-to-Image Generation（Oral)<br>作者：Guojun Yin, Bin Liu, Lu Sheng, Nenghai Yu, Xiaogang Wang, Jing Shao<br>论文链接：<a href="https://arxiv.org/abs/1904.01480">https://arxiv.org/abs/1904.01480</a></p><p>17、Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks（Oral)<br>作者：Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu<br>论文链接：<a href="https://arxiv.org/abs/1904.02884">https://arxiv.org/abs/1904.02884</a></p><p>18、R2GAN: Cross-modal Recipe Retrieval with Generative Adversarial Network<br>作者：Bin Zhu, Chong-Wah Ngo, Jingjing Chen, and Yanbin Hao<br>论文链接：<a href="https://vireo.cs.cityu.edu.hk/papers/R2GAN.pdf">https://vireo.cs.cityu.edu.hk/papers/R2GAN.pdf</a></p><p>19、Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance for Cross-View Image Translation（Oral)<br>作者：Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J. Corso, Yan Yan<br>论文链接：<a href="https://arxiv.org/abs/1904.06807">https://arxiv.org/abs/1904.06807</a><br>源码链接：<a href="https://github.com/Ha0Tang/SelectionGAN">https://github.com/Ha0Tang/SelectionGAN</a></p><p>20、Text Guided Person Image Synthesis<br>作者：Xingran Zhou, Siyu Huang, Bin Li, Yingming Li, Jiachen Li, Zhongfei Zhang<br>论文链接：<a href="https://arxiv.org/abs/1904.05118">https://arxiv.org/abs/1904.05118</a></p><p>21、Max-Sliced Wasserstein Distance and its use for GANs<br>作者：Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen Zhao, David Forsyth, Alexander Schwing<br>论文链接：<a href="https://arxiv.org/abs/1904.05877">https://arxiv.org/abs/1904.05877</a></p><p>22、Fashion-AttGAN: Attribute-Aware Fashion Editing with Multi-Objective GAN<br>作者：Qing Ping, Jiangbo Yuan, Bing Wu, Wanying Ding<br>论文链接：<a href="https://arxiv.org/abs/1904.07460">https://arxiv.org/abs/1904.07460</a></p><p>23、Self-Supervised GANs via Auxiliary Rotation Loss<br>作者：Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, Neil Houlsby<br>论文链接：<a href="https://arxiv.org/abs/1811.11212">https://arxiv.org/abs/1811.11212</a></p><p>24、Sphere Generative Adversarial Network Based on Geometric Moment Matching<br>作者：Sung Woo Park and Junseok Kwon<br>论文链接：<a href="https://cau.ac.kr/~jskwon/paper/SphereGAN_CVPR2019.pdf">https://cau.ac.kr/~jskwon/paper/SphereGAN_CVPR2019.pdf</a><br>源码链接：<a href="https://github.com/taki0112/SphereGAN-Tensorflow">https://github.com/taki0112/SphereGAN-Tensorflow</a></p><p>25、A Style-Based Generator Architecture for Generative Adversarial Networks<br>作者：Tero Karras (NVIDIA), Samuli Laine (NVIDIA), Timo Aila (NVIDIA)<br>论文链接：<a href="https://stylegan.xyz/paper">https://stylegan.xyz/paper</a><br>源码链接：<a href="https://github.com/NVlabs/stylegan">https://github.com/NVlabs/stylegan</a></p><h3 id="图像-视频处理、超分辨-24"><a href="#图像-视频处理、超分辨-24" class="headerlink" title="图像/视频处理、超分辨 24"></a>图像/视频处理、超分辨 24</h3><p>1、Recurrent MVSNet for High-resolution Multi-view Stereo Depth Inference<br>作者：Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, Long Quan<br>论文链接：<a href="https://arxiv.org/abs/1902.10556">https://arxiv.org/abs/1902.10556</a><br>代码链接：<a href="https://github.com/YoYo000/MVSNet">https://github.com/YoYo000/MVSNet</a></p><p>2、Unprocessing Images for Learned Raw Denoising (Oral Presentation)<br>作者：Tim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen, Dillon Sharlet, Jonathan T. Barron<br>论文链接：<a href="https://arxiv.org/abs/1811.11127">https://arxiv.org/abs/1811.11127</a><br>project链接：<a href="https://timothybrooks.com/tech/unprocessing/">https://timothybrooks.com/tech/unprocessing/</a><br>Reading note:We can learn a better denoising model by processing and unprocessing images the same way a camera does.</p><p>3、Image Super-Resolution by Neural Texture Transfer<br>作者：Zhifei Zhang, Zhaowen Wang, Zhe Lin, Hairong Qi<br>论文链接：<a href="https://arxiv.org/pdf/1903.00834.pdf">https://arxiv.org/pdf/1903.00834.pdf</a><br>项目链接：<a href="https://web.eecs.utk.edu/~zzhang61/project_page/SRNTT/SRNTT.html">https://web.eecs.utk.edu/~zzhang61/project_page/SRNTT/SRNTT.html</a><br>代码链接：<a href="https://github.com/ZZUTK/SRNTT">https://github.com/ZZUTK/SRNTT</a></p><p>4、Toward Convolutional Blind Denoising of Real Photographs<br>作者：Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, Lei Zhang<br>论文链接：<a href="https://arxiv.org/abs/1807.04686">https://arxiv.org/abs/1807.04686</a><br>代码链接：<a href="https://github.com/GuoShi28/CBDNet">https://github.com/GuoShi28/CBDNet</a></p><p>5、Learning Parallax Attention for Stereo Image Super-Resolution(图像超分辨)<br>作者：Longguang Wang, Yingqian Wang, Zhengfa Liang, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo<br>论文链接：<a href="https://arxiv.org/abs/1903.05784">https://arxiv.org/abs/1903.05784</a></p><p>6、Dual Residual Networks Leveraging the Potential of Paired Operations for Image Restoration<br>作者：Xing Liu, Masanori Suganuma, Zhun Sun, Takayuki Okatani<br>论文链接：<a href="https://arxiv.org/abs/1903.08817">https://arxiv.org/abs/1903.08817</a></p><p>7、PASSRnet: Parallax Attention Stereo Super-Resolution Network<br>作者：Longguang Wang, Yingqian Wang, Zhengfa Liang, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo<br>论文链接：<a href="https://arxiv.org/abs/1903.05784">https://arxiv.org/abs/1903.05784</a><br>代码链接：<a href="https://github.com/LongguangWang/PASSRnet">https://github.com/LongguangWang/PASSRnet</a></p><p>8、Feedback Network for Image Super-Resolution<br>作者：Zhen Li, Jinglei Yang, Zheng Liu, Xiaomin Yang, Gwanggil Jeon, Wei Wu<br>论文链接：<a href="https://arxiv.org/abs/1903.09814">https://arxiv.org/abs/1903.09814</a></p><p>9、Meta-SR: A Magnification-Arbitrary Network for Super-Resolution （旷视，超分辨）<br>作者：Xuecai Hu, Haoyuan Mu, Xiangyu Zhang, Zilei Wang, Tieniu Tan, Jian Sun<br>论文链接：<a href="https://arxiv.org/abs/1903.00875">https://arxiv.org/abs/1903.00875</a><br>论文解读：<a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid=2247488018&amp;idx=2&amp;sn=a8cc6e44fe5857ab914da63d2ae239cc&amp;chksm=ec1fffebdb6876fd6e6080e6ddaaed57ca5202f0636f4a4065070ee2adcf21aa687df996782d&amp;token=1423816873&amp;lang=zh_CN&amp;scene=21#wechat_redirect">CVPR2019 | 旷视提出Meta-SR：单一模型实现超分辨率任意缩放因子</a></p><p>10、Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels<br>作者：Kai Zhang, Wangmeng Zuo, Lei Zhang<br>论文链接：<a href="https://arxiv.org/abs/1903.12529">https://arxiv.org/abs/1903.12529</a><br>代码链接：<a href="https://github.com/cszn/DPSR">https://github.com/cszn/DPSR</a></p><p>11、Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset<br>作者：Tianyu Wang<em>, Xin Yang</em>, Ke Xu, Shaozhe Chen, Qiang Zhang, Rynson Lau<br>论文链接：<a href="https://arxiv.org/abs/1904.01538">https://arxiv.org/abs/1904.01538</a><br>项目链接：<a href="https://stevewongv.github.io/derain-project.html">https://stevewongv.github.io/derain-project.html</a></p><p>12、DVC: An End-to-end Deep Video Compression Framework（Oral）<br>作者：Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Chunlei Cai, Zhiyong Gao<br>论文链接：<a href="https://arxiv.org/abs/1812.00101">https://arxiv.org/abs/1812.00101</a><br>代码链接：<a href="https://github.com/GuoLusjtu/DVC">https://github.com/GuoLusjtu/DVC</a></p><p>13、Blind Visual Motif Removal from a Single Image<br>作者：Amir Hertz, Sharon Fogel, Rana Hanocka, Raja Giryes, Daniel Cohen-Or<br>论文链接：<a href="https://arxiv.org/abs/1904.02756">https://arxiv.org/abs/1904.02756</a></p><p>14、Fast Spatio-Temporal Residual Network for Video Super-Resolution<br>作者：Sheng Li, Fengxiang He, Bo Du, Lefei Zhang, Yonghao Xu, Dacheng Tao<br>论文链接：<a href="https://arxiv.org/abs/1904.02870">https://arxiv.org/abs/1904.02870</a></p><p>15、3D Appearance Super-Resolution with Deep Learning<br>论文链接：<a href="https://github.com/ofsoundof/3D_Appearance_SR/blob/master/code/scripts/3d_appearance_sr.pdf">https://github.com/ofsoundof/3D_Appearance_SR/blob/master/code/scripts/3d_appearance_sr.pdf</a><br>源码链接：<a href="https://github.com/ofsoundof/3D_Appearance_SR">https://github.com/ofsoundof/3D_Appearance_SR</a></p><p>16、Camera Lens Super-Resolution<br>作者：Chang Chen, Zhiwei Xiong, Xinmei Tian, Zheng-Jun Zha, Feng Wu<br>论文链接：<a href="https://staff.ustc.edu.cn/~zwxiong/cameraSR.pdf">https://staff.ustc.edu.cn/~zwxiong/cameraSR.pdf</a><br>源码链接：<a href="https://github.com/ngchc/CameraSR">https://github.com/ngchc/CameraSR</a></p><p>17、Heavy Rain Image Restoration: Integrating Physics Model and Conditional Adversarial Learning<br>作者：Ruotent Li, Loong Fah Cheong, Robby T. Tan<br>论文链接：<a href="https://arxiv.org/abs/1904.05050">https://arxiv.org/abs/1904.05050</a></p><p>18、Learning Pyramid-Context Encoder Network for High-Quality Image Inpainting<br>作者：Yanhong Zeng, Jianlong Fu, Hongyang Chao, Baining Guo<br>论文链接：<a href="https://arxiv.org/abs/1904.07475">https://arxiv.org/abs/1904.07475</a></p><p>19、Attention-based Adaptive Selection of Operations for Image Restoration in the Presence of Unknown Combined Distortions<br>作者：Masanori Suganuma, Xing Liu, Takayuki Okatani<br>论文链接：<a href="https://arxiv.org/abs/1812.00733">https://arxiv.org/abs/1812.00733</a><br>源码链接：<a href="https://github.com/sg-nm/Operation-wise-attention-network">https://github.com/sg-nm/Operation-wise-attention-network</a></p><p>20、Deep Video Inpainting<br>作者：Dahun Kim, Sanghyun Woo, Joon-Young Lee, In So Kweon<br>论文链接：<a href="https://arxiv.org/abs/1905.01639">https://arxiv.org/abs/1905.01639</a></p><p>21、Deep Flow-Guided Video Inpaintinge<br>作者：Rui Xu, Xiaoxiao Li, Bolei Zhou, Chen Change Loy<br>论文链接：<a href="https://arxiv.org/abs/1905.02884">https://arxiv.org/abs/1905.02884</a></p><p>22、Deep Blind Video Decaptioning by Temporal Aggregation and Recurrence<br>作者：Dahun Kim, Sanghyun Woo, Joon-Young Lee, In So Kweon<br>论文链接：<a href="https://arxiv.org/abs/1905.02949">https://arxiv.org/abs/1905.02949</a></p><p>23、Dynamic Scene Deblurring with Parameter Selective Sharing and Nested Skip Connections<br>作者：Hongyun Gao, Xin Tao, Xiaoyong Shen, Jiaya Jia<br>论文链接：<a href="https://jiaya.me/papers/deblur_cvpr19.pdf">https://jiaya.me/papers/deblur_cvpr19.pdf</a></p><p>24、Underexposed Photo Enhancement using Deep Illumination Estimation<br>作者：Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen, Wei-shi Zheng, Jiaya Jia<br>论文链接：<a href="https://jiaya.me/papers/photoenhance_cvpr19.pdf">https://jiaya.me/papers/photoenhance_cvpr19.pdf</a></p><h3 id="点云、三维重建-44"><a href="#点云、三维重建-44" class="headerlink" title="点云、三维重建 44"></a>点云、三维重建 44</h3><p>1、The Perfect Match: 3D Point Cloud Matching with Smoothed Densities<br>作者：Zan Gojcic, Caifa Zhou, Jan D. Wegner, Andreas Wieser<br>论文链接：<a href="https://arxiv.org/abs/1811.06879">https://arxiv.org/abs/1811.06879</a></p><p>2、Octree guided CNN with Spherical Kernels for 3D Point Clouds<br>作者：Huan Lei, Naveed Akhtar, Ajmal Mian<br>论文链接：<a href="https://arxiv.org/abs/1903.00343">https://arxiv.org/abs/1903.00343</a></p><p>3、DeepMapping: Unsupervised Map Estimation From Multiple Point Clouds<br>作者：Li Ding, Chen Feng<br>论文链接：<a href="https://arxiv.org/abs/1811.11397">https://arxiv.org/abs/1811.11397</a></p><p>4、Generating 3D Adversarial Point Clouds<br>作者：Chong Xiang (1), Charles R. Qi (2), Bo Li (3) ((1) Shanghai Jiao Tong Univerisity, (2) Stanford University, (3) University of Illinois at Urbana-Champaign)<br>论文链接：<a href="https://arxiv.org/abs/1809.07016">https://arxiv.org/abs/1809.07016</a><br>简要：Proposed several novel algorithms to craft adversarial point clouds against 3D deep learning models with adversarial points perturbation and adversarial points generation.</p><p>5、FlowNet3D: Learning Scene Flow in 3D Point Clouds<br>作者：Xingyu Liu, Charles R. Qi, Leonidas J. Guibas<br>论文链接：<a href="https://arxiv.org/abs/1806.01411">https://arxiv.org/abs/1806.01411</a><br>简要：Proposed a novel deep neural network that learns scene flow from point clouds in an end-to-end fashion.</p><p>6、33.Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding（开源）<br>作者：Zehao Yu, Jia Zheng, Dongze Lian, Zihan Zhou, Shenghua Gao<br>论文链接：<a href="https://arxiv.org/abs/1902.09777">https://arxiv.org/abs/1902.09777</a><br>代码链接：<a href="https://github.com/svip-lab/PlanarReconstruction">https://github.com/svip-lab/PlanarReconstruction</a></p><p>7、FML: Face Model Learning from Videos(Oral)<br>作者：A. Tewari F. Bernard P. Garrido G. Bharaj M. Elgharib H-P. Seidel P. Perez M. Zollhöfer C.Theobalt<br>项目链接：<a href="https://gvv.mpi-inf.mpg.de/projects/FML19/">https://gvv.mpi-inf.mpg.de/projects/FML19/</a><br>论文链接：<a href="https://gvv.mpi-inf.mpg.de/projects/FML19/paper.pdf">https://gvv.mpi-inf.mpg.de/projects/FML19/paper.pdf</a></p><p>8、SceneCode: Monocular Dense Semantic Reconstruction using Learned Encoded Scene Representation<br>作者：Shuaifeng Zhi, Michael Bloesch, Stefan Leutenegger, Andrew J. Davison<br>论文链接：<a href="https://arxiv.org/abs/1903.06482">https://arxiv.org/abs/1903.06482</a></p><p>9、Photometric Mesh Optimization for Video-Aligned 3D Object Reconstruction<br>作者：Pelin Dogan, Leonid Sigal, Markus Gross<br>论文链接：<br><a href="https://chenhsuanlin.bitbucket.io/photometric-mesh-optim/paper.pdf">https://chenhsuanlin.bitbucket.io/photometric-mesh-optim/paper.pdf</a><br>代码链接：<br><a href="https://github.com/chenhsuanlin/photometric-mesh-optim">https://github.com/chenhsuanlin/photometric-mesh-optim</a><br>项目链接：<br><a href="https://chenhsuanlin.bitbucket.io/photometric-mesh-optim/">https://chenhsuanlin.bitbucket.io/photometric-mesh-optim/</a></p><p>10、Learning View Priors for Single-view 3D Reconstruction<br>作者：Hiroharu Kato, Tatsuya Harada<br>论文链接：<a href="https://arxiv.org/abs/1811.10719">https://arxiv.org/abs/1811.10719</a><br>项目链接：<a href="https://hiroharu-kato.com/projects_en/view_prior_learning.html">https://hiroharu-kato.com/projects_en/view_prior_learning.html</a></p><p>11、Patch-based Progressive 3D Point Set Upsampling<br>作者：Wang Yifan, Shihao Wu, Hui Huang, Daniel Cohen-Or, Olga Sorkine-Hornung<br>论文链接：<a href="https://arxiv.org/abs/1811.11286">https://arxiv.org/abs/1811.11286</a><br>代码链接：<a href="https://github.com/yifita/3PU">https://github.com/yifita/3PU</a></p><p>12、GeoNet: Deep Geodesic Networks for Point Cloud Analysis（Oral,旷视，根据测地间隔的点云剖析深度网络）<br>作者：Tong He, Haibin Huang, Li Yi, Yuqian Zhou, Chihao Wu, Jue Wang, Stefano Soatto<br>论文链接：<a href="https://arxiv.org/abs/1901.00680">https://arxiv.org/abs/1901.00680</a><br>论文解读：<a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid=2247487956&amp;idx=2&amp;sn=0b1dd72826412afb02bac769a624b709&amp;chksm=ec1ffc2ddb68753bcc1a5cedbc32caeedf08d2fba13770cef4fc06dcd43be4175e3452f337d5&amp;scene=21&amp;token=236193575&amp;lang=zh_CN#wechat_redirect">CVPR 2019 | 旷视等Oral论文提出GeoNet：基于测地距离的点云分析深度网络</a></p><p>13、JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds with Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields（Oral)<br>作者：Quang-Hieu Pham, Duc Thanh Nguyen, Binh-Son Hua, Gemma Roig, Sai-Kit Yeung<br>论文链接：<a href="https://arxiv.org/abs/1904.00699">https://arxiv.org/abs/1904.00699</a><br>项目链接：<a href="https://pqhieu.github.io/cvpr19.html">https://pqhieu.github.io/cvpr19.html</a></p><p>14、Point Cloud Oversegmentation with Graph-Structured Deep Metric Learning<br>作者：Loic Landrieu, Mohamed Boussaha<br>论文链接：<a href="https://arxiv.org/abs/1904.02113">https://arxiv.org/abs/1904.02113</a></p><p>15、Calibration of Asynchronous Camera Networks for Object Reconstruction Tasks<br>作者：Amy Tabb, Henry Medeiros<br>论文链接：<a href="https://arxiv.org/abs/1903.06811">https://arxiv.org/abs/1903.06811</a></p><p>16、StereoDRNet: Dilated Residual Stereo Net<br>作者：Rohan Chabra, Julian Straub, Chris Sweeny, Richard Newcombe, Henry Fuchs<br>论文链接：<a href="https://arxiv.org/abs/1904.02251">https://arxiv.org/abs/1904.02251</a></p><p>17、Conditional Single-view Shape Generation for Multi-view Stereo Reconstruction<br>作者：Yi Wei, Shaohui Liu, Wang Zhao, Jiwen Lu, Jie Zhou<br>论文链接：<a href="https://arxiv.org/abs/1904.06699">https://arxiv.org/abs/1904.06699</a></p><p>18、PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud<br>作者：Shaoshuai Shi, Xiaogang Wang, Hongsheng Li<br>论文链接：<a href="https://arxiv.org/abs/1812.04244">https://arxiv.org/abs/1812.04244</a><br>源码链接：<a href="https://github.com/sshaoshuai/PointRCNN">https://github.com/sshaoshuai/PointRCNN</a></p><p>19、Relation-Shape Convolutional Neural Network for Point Cloud Analysis<br>作者：Yongcheng Liu, Bin Fan, Shiming Xiang, Chunhong Pan<br>论文链接：<a href="https://arxiv.org/abs/1904.07601">https://arxiv.org/abs/1904.07601</a><br>项目链接：<a href="https://yochengliu.github.io/Relation-Shape-CNN/">https://yochengliu.github.io/Relation-Shape-CNN/</a><br>源码链接：<a href="https://github.com/Yochengliu/Relation-Shape-CNN">https://github.com/Yochengliu/Relation-Shape-CNN</a></p><p>20、A-CNN: Annularly Convolutional Neural Networks on Point Clouds<br>作者：Artem Komarichev, Zichun Zhong, Jing Hua<br>论文链接：<a href="https://arxiv.org/abs/1904.08017">https://arxiv.org/abs/1904.08017</a></p><p>21、PCAN: 3D Attention Map Learning Using Contextual Information for Point Cloud Based Retrieval<br>作者： Wenxiao Zhang, Chunxia Xiao<br>论文链接：<a href="https://arxiv.org/abs/1904.09793">https://arxiv.org/abs/1904.09793</a></p><p>22、Deep Convolutional Networks on 3D Point Clouds<br>作者：Satwik Acharyya, Zhengwu Zhang, Anirban Bhattacharya, Debdeep Pati<br>论文链接：<a href="https://arxiv.org/pdf/1811.07246.pdf">https://arxiv.org/pdf/1811.07246.pdf</a><br>源码链接：<a href="https://github.com/DylanWusee/pointconv">https://github.com/DylanWusee/pointconv</a></p><p>23、LBS Autoencoder: Self-supervised Fitting of Articulated Meshes to Point Clouds<br>作者：Chun-Liang Li, Tomas Simon, Jason Saragih, Barnabás Póczos, Yaser Sheikh<br>论文链接：<a href="https://arxiv.org/abs/1904.10037">https://arxiv.org/abs/1904.10037</a></p><p>24、Modeling Local Geometric Structure of 3D Point Clouds using Geo-CNN<br>作者：Shiyi Lan, Ruichi Yu, Gang Yu, Larry S. Davis<br>论文链接：<a href="https://arxiv.org/abs/1811.07782">https://arxiv.org/abs/1811.07782</a></p><p>25、RL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for Real-Time Point Cloud Shape Completion<br>作者：Muhammad Sarmad, Hyunjoo Jenny Lee, Young Min Kim<br>论文链接：<a href="https://arxiv.org/abs/1904.12304">https://arxiv.org/abs/1904.12304</a><br>源码链接：<a href="https://github.com/iSarmad/RL-GAN-Net">https://github.com/iSarmad/RL-GAN-Net</a></p><p>26、Occupancy Networks - Learning 3D Reconstruction in Function Space<br>作者：Lars Mescheder and Michael Oechsle and Michael Niemeyer and Sebastian Nowozin and Andreas Geiger<br>论文链接：<a href="https://avg.is.tuebingen.mpg.de/uploads_file/attachment/attachment/490/top.pdf">https://avg.is.tuebingen.mpg.de/uploads_file/attachment/attachment/490/top.pdf</a><br>源码链接：<a href="https://github.com/autonomousvision/occupancy_networks">https://github.com/autonomousvision/occupancy_networks</a></p><p>27、Graph Attention Convolution for Point Cloud Segmentation<br>作者：待更新<br>论文链接：<a href="https://engineering.purdue.edu/~jshan/publications/2018/Lei Wang Graph Attention Convolution for Point Cloud Segmentation CVPR2019.pdf">https://engineering.purdue.edu/~jshan/publications/2018/Lei%20Wang%20Graph%20Attention%20Convolution%20for%20Point%20Cloud%20Segmentation%20CVPR2019.pdf</a></p><p>28、GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud<br>作者：Li Yi, Wang Zhao, He Wang, Minhyuk Sung, Leonidas Guibas<br>论文链接：<a href="https://arxiv.org/abs/1812.03320">https://arxiv.org/abs/1812.03320</a></p><p>29、Nesti-Net: Normal Estimation for Unstructured 3D Point Clouds using Convolutional Neural Networks<br>作者：Yizhak Ben-Shabat, Michael Lindenbaum, Anath Fischer<br>论文链接：<a href="https://arxiv.org/abs/1812.00709">https://arxiv.org/abs/1812.00709</a><br>源码链接：<a href="https://github.com/sitzikbs/Nesti-Net">https://github.com/sitzikbs/Nesti-Net</a></p><p>30、Robust Point Cloud Based Reconstruction of Large-Scale Outdoor Scenes<br>作者：Ziquan Lan, Zi Jian Yew，Gim Hee Lee<br>论文链接：<a href="https://www.researchgate.net/publication/332240602_Robust_Point_Cloud_Based_Reconstruction_of_Large-Scale_Outdoor_Scenes">https://www.researchgate.net/publication/332240602_Robust_Point_Cloud_Based_Reconstruction_of_Large-Scale_Outdoor_Scenes</a><br>源码链接：<a href="https://github.com/ziquan111/RobustPCLReconstruction">https://github.com/ziquan111/RobustPCLReconstruction</a></p><p>31、PointNetLK: Robust &amp; Efficient Point Cloud Registration using PointNet<br>作者：Yasuhiro Aoki, Hunter Goforth, Rangaprasad Arun Srivatsan, Simon Lucey<br>论文链接：<a href="https://arxiv.org/abs/1903.05711">https://arxiv.org/abs/1903.05711</a><br>源码链接：<a href="https://github.com/hmgoforth/PointNetLK">https://github.com/hmgoforth/PointNetLK</a></p><p>32、PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing<br>作者：Hengshuang Zhao， Li Jiang， Chi-Wing Fu，Jiaya Jia<br>论文链接：<a href="https://jiaya.me/papers/pointweb_cvpr19.pdf">https://jiaya.me/papers/pointweb_cvpr19.pdf</a></p><p>33、ClusterNet: Deep Hierarchical Cluster Network with Rigorously Rotation-Invariant Representation for Point Cloud Analysis<br>作者：Chao Chen， Guanbin Li， Ruijia Xu， Tianshui Chen， Meng Wang， Liang Lin<br>论文链接：<a href="https://www.linliang.net/wp-content/uploads/2019/04/CVPR2019_PointClound.pdf">https://www.linliang.net/wp-content/uploads/2019/04/CVPR2019_PointClound.pdf</a></p><p>34、FilterReg: Robust and Efficient Probabilistic Point-Set Registration using Gaussian Filter and Twist Parameterization<br>作者：Wei Gao, Russ Tedrake<br>论文链接：<a href="https://arxiv.org/abs/1811.10136">https://arxiv.org/abs/1811.10136</a><br>源码链接：<a href="https://bitbucket.org/gaowei19951004/poser/src/master/">https://bitbucket.org/gaowei19951004/poser/src/master/</a></p><p>35、Embodied Question Answering in Photorealistic Environments with Point Cloud Perception<br>作者：Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, Dhruv Batra<br>论文链接：<a href="https://arxiv.org/abs/1904.03461">https://arxiv.org/abs/1904.03461</a></p><p>36、SDRSAC: Semidefinite-Based Randomized Approach for Robust Point Cloud Registration without Correspondences<br>作者：Huu Le, Thanh-Toan Do, Tuan Hoang, Ngai-Man Cheung<br>论文链接：<a href="https://arxiv.org/abs/1904.03483">https://arxiv.org/abs/1904.03483</a><br>源码链接：<a href="https://github.com/intellhave/SDRSAC">https://github.com/intellhave/SDRSAC</a> （matlab）</p><p>37、PointFlowNet: Learning Representations for Rigid Motion Estimation from Point Clouds<br>作者：Aseem Behl, Despoina Paschalidou, Simon Donné, Andreas Geiger<br>论文链接：<a href="https://arxiv.org/abs/1806.02170">https://arxiv.org/abs/1806.02170</a><br>源码链接：<a href="https://github.com/aseembehl/pointflownet">https://github.com/aseembehl/pointflownet</a></p><p>38、PointPillars: Fast Encoders for Object Detection from Point Clouds<br>作者：Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, Oscar Beijbom<br>论文链接：<a href="https://arxiv.org/abs/1812.05784">https://arxiv.org/abs/1812.05784</a><br>源码链接：<a href="https://github.com/nutonomy/second.pytorch">https://github.com/nutonomy/second.pytorch</a></p><p>40、Supervised Fitting of Geometric Primitives to 3D Point Clouds（Oral）<br>作者：Lingxiao Li, Minhyuk Sung, Anastasia Dubrovina, Li Yi, Leonidas Guibas<br>论文链接：<a href="https://arxiv.org/abs/1811.08988">https://arxiv.org/abs/1811.08988</a><br>源码链接：<a href="https://github.com/csimstu2/SPFN">https://github.com/csimstu2/SPFN</a></p><p>41、PointConv: Deep Convolutional Networks on 3D Point Clouds<br>作者：Wenxuan Wu, Zhongang Qi, Li Fuxin<br>论文链接：<a href="https://arxiv.org/abs/1811.07246">https://arxiv.org/abs/1811.07246</a><br>源码链接：<a href="https://github.com/DylanWusee/pointconv">https://github.com/DylanWusee/pointconv</a></p><p>42、Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling<br>作者：Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, Qi Tian<br>论文链接：<a href="https://arxiv.org/abs/1904.03375v1">https://arxiv.org/abs/1904.03375v1</a></p><p>41、Spherical Fractal Convolutional Neural Networks for Point Cloud Recognition<br>作者：Yongming Rao, Jiwen Lu, Jie Zhou<br>论文链接：<a href="https://raoyongming.github.io/files/SFCNN.pdf">https://raoyongming.github.io/files/SFCNN.pdf</a></p><p>42、Neural RGB-&gt;D Sensing: Depth and Uncertainty from a Video Camera(Oral,英伟达)<br>作者：Chao Liu, Jinwei Gu, Kihwan Kim, Srinivasa Narasimhan, Jan Kautz<br>论文链接：<a href="https://arxiv.org/abs/1901.02571">https://arxiv.org/abs/1901.02571</a><br>源码链接：<a href="https://github.com/NVlabs/neuralrgbd">https://github.com/NVlabs/neuralrgbd</a><br>项目链接：<a href="https://research.nvidia.com/publication/2019-06_Neural-RGBD">https://research.nvidia.com/publication/2019-06_Neural-RGBD</a></p><p>43、Reducing Uncertainty in Undersampled MRI Reconstruction with Active Acquisition(Facebook)<br>作者：Zizhao Zhang, Adriana Romero, Matthew J. Muckley, Pascal Vincent, Lin Yang, Michal Drozdzal<br>论文链接：<a href="https://research.fb.com/wp-content/uploads/2019/05/Reducing-Uncertainty-in-Undersampled-MRI-Reconstruction-with-Active-Acquisition.pdf">https://research.fb.com/wp-content/uploads/2019/05/Reducing-Uncertainty-in-Undersampled-MRI-Reconstruction-with-Active-Acquisition.pdf</a></p><p>44、Occupancy Networks: Learning 3D Reconstruction in Function Space<br>作者：Lars Mescheder and Michael Oechsle and Michael Niemeyer and Sebastian Nowozin and Andreas Geiger<br>论文链接：<a href="https://avg.is.tuebingen.mpg.de/publications/occupancy-networks">https://avg.is.tuebingen.mpg.de/publications/occupancy-networks</a><br>源码链接：<a href="https://github.com/autonomousvision/occupancy_networks">https://github.com/autonomousvision/occupancy_networks</a></p><h3 id="VQA、视觉语言导航-12"><a href="#VQA、视觉语言导航-12" class="headerlink" title="VQA、视觉语言导航 12"></a>VQA、视觉语言导航 12</h3><p>1、MUREL: Multimodal Relational Reasoning for Visual Question Answering<br>作者：Remi Cadene, Hedi Ben-younes, Matthieu Cord, Nicolas Thome<br>论文链接：<a href="https://arxiv.org/abs/1902.09487">https://arxiv.org/abs/1902.09487</a></p><p>2、Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation<br>作者：Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang<br>论文链接：<a href="https://arxiv.org/abs/1811.10092">https://arxiv.org/abs/1811.10092</a><br>论文解读：<a href="https://mp.weixin.qq.com/s/LsHWkdwqqrOPFgCNNcBdpg">https://mp.weixin.qq.com/s/LsHWkdwqqrOPFgCNNcBdpg</a></p><p>3、Image-Question-Answer Synergistic Network for Visual Dialog<br>作者：Dalu Guo, Chang Xu, Dacheng Tao<br>论文链接：<a href="https://arxiv.org/abs/1902.09774">https://arxiv.org/abs/1902.09774</a></p><p>4、Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation(oral)<br>作者：Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, Siddhartha Srinivasa<br>论文链接：<a href="https://arxiv.org/abs/1903.02547">https://arxiv.org/abs/1903.02547</a><br>YouTube:<a href="https://youtu.be/ik9uz06Fcpk">https://youtu.be/ik9uz06Fcpk</a></p><p>5、Learning to Compose Dynamic Tree Structures for Visual Contexts（VQA,Oral）<br>作者：Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, Wei Liu<br>论文链接：<a href="https://arxiv.org/abs/1812.01880">https://arxiv.org/abs/1812.01880</a><br>代码链接：<br><a href="https://github.com/KaihuaTang/VCTree-Visual-Question-Answering">https://github.com/KaihuaTang/VCTree-Visual-Question-Answering</a></p><p>6、Transfer Learning via Unsupervised Task Discovery for Visual Question Answering（VQA)<br>作者：Hyeonwoo Noh, Taehoon Kim, Jonghwan Mun, Bohyung Han<br>论文链接：<a href="https://arxiv.org/abs/1810.02358">https://arxiv.org/abs/1810.02358</a></p><p>7、Information Maximizing Visual Question Generation(VQA)<br>作者：Zhongdao Wang, Liang Zheng, Yali Li, Shengjin Wang<br>论文链接：<a href="https://arxiv.org/abs/1903.11306">https://arxiv.org/abs/1903.11306</a></p><p>8、Answer Them All! Toward Universal Visual Question Answering Models（VQA)<br>作者：Robik Shrestha, Kushal Kafle, Christopher Kanan<br>论文链接：<a href="https://arxiv.org/abs/1903.00366">https://arxiv.org/abs/1903.00366</a></p><p>9、Cycle-Consistency for Robust Visual Question Answering（VQA)<br>作者：Gao Peng, Zhengkai Jiang, Haoxuan You, Zhengkai Jiang, Pan Lu, Steven Hoi, Xiaogang Wang, Hongsheng Li<br>论文链接：<a href="https://arxiv.org/pdf/1812.05252.pdf">https://arxiv.org/pdf/1812.05252.pdf</a></p><p>10、Towards VQA Models that can Read<br>作者：Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, Marcus Rohrbach<br>论文链接：<a href="https://arxiv.org/abs/1904.08920">https://arxiv.org/abs/1904.08920</a></p><p>11、Grounded Video Description (Oral)<br>作者：Luowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J. Corso, Marcus Rohrbach<br>论文链接：<a href="https://arxiv.org/abs/1812.06587">https://arxiv.org/abs/1812.06587</a><br>源码链接：<a href="https://github.com/facebookresearch/grounded-video-description">https://github.com/facebookresearch/grounded-video-description</a></p><p>12、Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning<br>作者：Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi<br>论文链接：<a href="https://arxiv.org/abs/1812.00971">https://arxiv.org/abs/1812.00971</a><br>源码链接：<a href="https://github.com/allenai/savn">https://github.com/allenai/savn</a></p><h3 id="OCR、文本检测-10"><a href="#OCR、文本检测-10" class="headerlink" title="OCR、文本检测 10"></a>OCR、文本检测 10</h3><p>1、Shape Robust Text Detection with Progressive Scale Expansion Network（文本检测）<br>作者：Xiang Li, Wenhai Wang, Wenbo Hou, Ruo-Ze Liu, Tong Lu, Jian Yang<br>论文链接：<a href="https://arxiv.org/abs/1806.02559">https://arxiv.org/abs/1806.02559</a><br>代码链接：<a href="https://github.com/whai362/PSENet">https://github.com/whai362/PSENet</a><br>网友复现：<a href="https://github.com/liuheng92/tensorflow_PSENet">https://github.com/liuheng92/tensorflow_PSENet</a></p><p>2、Towards Robust Curve Text Detection with Conditional Spatial Expansion<br>作者：Zichuan Liu, Guosheng Lin, Sheng Yang, Fayao Liu, Weisi Lin, Wang Ling Goh<br>论文链接：<a href="https://arxiv.org/abs/1903.08836">https://arxiv.org/abs/1903.08836</a></p><p>3、Shape Robust Text Detection with Progressive Scale Expansion Network<br>作者：Wenhai Wang, Enze Xie, Xiang Li, Wenbo Hou, Tong Lu, Gang Yu, Shuai Shao<br>论文链接：<a href="https://arxiv.org/abs/1903.12473">https://arxiv.org/abs/1903.12473</a></p><p>4、Handwriting Recognition in Low-resource Scripts using Adversarial Learning<br>作者：Ayan Kumar Bhunia, Abhirup Das, Ankan Kumar Bhunia, Perla Sai Raj Kishore, Partha Pratim Roy<br>论文链接：<a href="https://arxiv.org/pdf/1811.01396.pdf">https://arxiv.org/pdf/1811.01396.pdf</a></p><p>5、Handwriting Recognition in Low-resource Scripts using Adversarial Learning<br>作者：Ayan Kumar Bhunia, Abhirup Das, Ankan Kumar Bhunia, Perla Sai Raj Kishore, Partha Pratim Roy<br>论文链接：<a href="https://arxiv.org/abs/1811.01396">https://arxiv.org/abs/1811.01396</a></p><p>6、Tightness-aware Evaluation Protocol for Scene Text Detection<br>作者：Yuliang Liu, Lianwen Jin, Zecheng Xie, Canjie Luo, Shuaitao Zhang, Lele Xie<br>论文链接：<a href="https://arxiv.org/abs/1904.00813">https://arxiv.org/abs/1904.00813</a></p><p>7、Character Region Awareness for Text Detection（文本检测）<br>作者：Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, Hwalsuk Lee<br>论文链接：<a href="https://arxiv.org/abs/1904.01941">https://arxiv.org/abs/1904.01941</a></p><p>8、Look More Than Once: An Accurate Detector for Text of Arbitrary Shapes<br>作者：Chengquan Zhang, Borong Liang, Zuming Huang, Mengyi En, Junyu Han, Errui Ding, Xinghao Ding<br>论文链接：<a href="https://arxiv.org/abs/1904.06535">https://arxiv.org/abs/1904.06535</a></p><p>9、Learning Shape-Aware Embedding for Scene Text Detection<br>作者：Zhuotao Tian, Michelle Shu, Pengyuan Lyu, Ruiyu Li, Chao Zhou, Xiaoyong Shen, Jiaya Jia†<br>论文链接：<a href="https://jiaya.me/papers/textdetection_cvpr19.pdf">https://jiaya.me/papers/textdetection_cvpr19.pdf</a></p><p>10、Look More Than Once: An Accurate Detector for Text of Arbitrary Shapes Screen reader support enabled<br>作者：Chengquan Zhang, Borong Liang, Zuming Huang, Mengyi En, Junyu Han, Errui Ding, Xinghao Ding<br>论文链接：<a href="https://arxiv.org/pdf/1904.06535.pdf">https://arxiv.org/pdf/1904.06535.pdf</a></p><h3 id="自动驾驶、SLAM-12"><a href="#自动驾驶、SLAM-12" class="headerlink" title="自动驾驶、SLAM 12"></a>自动驾驶、SLAM 12</h3><p>1、Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving（自动驾驶）<br>作者：Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, Kilian Q. Weinberger<br>论文链接：<a href="https://arxiv.org/abs/1812.07179">https://arxiv.org/abs/1812.07179</a><br>项目链接：<a href="https://mileyan.github.io/pseudo_lidar/">https://mileyan.github.io/pseudo_lidar/</a><br>代码链接：<a href="https://github.com/mileyan/pseudo_lidar">https://github.com/mileyan/pseudo_lidar</a></p><p>2、ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving<br>作者：Xibin Song, Peng Wang, Dingfu Zhou, Rui Zhu, Chenye Guan, Yuchao Dai, Hao Su, Hongdong Li, Ruigang Yang<br>论文链接：<a href="https://arxiv.org/abs/1811.12222">https://arxiv.org/abs/1811.12222</a><br>简要：The first large-scale database suitable for 3D car instance understanding, ApolloCar3D, collected by Baidu. The dataset contains 5,277 driving images and over 60K car instances, where each car is fitted with an industry-grade 3D CAD model with absolute model size and semantically labelled keypoints.</p><p>3、Group-wise Correlation Stereo Network<br>作者：Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang, Hongsheng Li<br>论文链接：<a href="https://arxiv.org/abs/1903.04025">https://arxiv.org/abs/1903.04025</a></p><p>4、Stereo R-CNN based 3D Object Detection for Autonomous Driving<br>作者：Peiliang Li, Xiaozhi Chen, Shaojie Shen<br>论文链接：<a href="https://arxiv.org/abs/1902.09738">https://arxiv.org/abs/1902.09738</a></p><p>5、Deep Rigid Instance Scene Flow<br>作者：Wei-Chiu Ma 、Shenlong Wang 、Rui Hu、Yuwen Xiong、 Raquel Urtasun<br>论文链接：<br><a href="https://people.csail.mit.edu/weichium/papers/cvpr19-dsisf/paper.pdf">https://people.csail.mit.edu/weichium/papers/cvpr19-dsisf/paper.pdf</a><br>论文摘要：在本文中，我们解决了自动驾驶环境下的场景流量估计问题。 我们利用深度学习技术以及强大的先验，因为在我们的应用领域中，场景的运动可以由机器人的运动和场景中的演员的3D运动来组成。</p><p>6、An Efficient Schmidt-EKF for 3D Visual-Inertial SLAM<br>作者：Patrick Geneva, James Maley, Guoquan Huang<br>论文链接：<a href="https://arxiv.org/abs/1903.08636">https://arxiv.org/abs/1903.08636</a></p><p>7、LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving<br>作者：Gregory P. Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-Gonzalez, Carl K. Wellington<br>论文链接：<a href="https://arxiv.org/abs/1903.08701">https://arxiv.org/abs/1903.08701</a></p><p>8、.GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving<br>作者：Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, Xiaogang Wang<br>论文链接：<a href="https://arxiv.org/abs/1903.10955">https://arxiv.org/abs/1903.10955</a></p><p>9、Learning to Adapt for Stereo<br>作者：Alessio Tonioni, Oscar Rahnama, Thomas Joy, Luigi Di Stefano, Thalaiyasingam Ajanthan, Philip H. S. Torr<br>论文链接：<a href="https://arxiv.org/abs/1904.02957">https://arxiv.org/abs/1904.02957</a><br>代码链接：<a href="https://github.com/CVLAB-Unibo/Learning2AdaptForStereo">https://github.com/CVLAB-Unibo/Learning2AdaptForStereo</a></p><p>10、What Object Should I Use? - Task Driven Object Detection<br>作者：Johann Sawatzky, Yaser Souri, Christian Grund, Juergen Gall<br>论文链接：<a href="https://arxiv.org/abs/1904.03000">https://arxiv.org/abs/1904.03000</a></p><p>11、YUVMultiNet: Real-time YUV multi-task CNN for autonomous driving<br>作者：Thomas Boulay, Said El-Hachimi, Mani Kumar Surisetti, Pullarao Maddu, Saranya Kandan<br>论文链接：<a href="https://arxiv.org/abs/1904.05673">https://arxiv.org/abs/1904.05673</a></p><p>12、L3-Net: Towards Learning based LiDAR Localization for Autonomous Driving<br>作者：Weixin Lu， Yao Zhou， Guowei Wan，Shenhua Hou，Shiyu Song<br>论文链接：<a href="https://songshiyu01.github.io/pdf/L3Net_W.Lu_Y.Zhou_S.Song_CVPR2019.pdf">https://songshiyu01.github.io/pdf/L3Net_W.Lu_Y.Zhou_S.Song_CVPR2019.pdf</a></p><h3 id="人群计数-3"><a href="#人群计数-3" class="headerlink" title="人群计数 3"></a>人群计数 3</h3><p>1、Learning from Synthetic Data for Crowd Counting in the Wild<br>作者：Qi Wang, Junyu Gao, Wei Lin, Yuan Yuan<br>论文链接：<a href="https://arxiv.org/abs/1903.03303">https://arxiv.org/abs/1903.03303</a></p><p>2、Point in, Box out: Beyond Counting Persons in Crowds<br>作者：待更新<br>论文链接：<a href="https://github.com/xiaofanglegoc/xiaofanglegoc.github.io/blob/master/publications/cvpr2019.pdf">https://github.com/xiaofanglegoc/xiaofanglegoc.github.io/blob/master/publications/cvpr2019.pdf</a></p><p>3、Learning the Depths of Moving People by Watching Frozen People（Oral）<br>作者：Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, William T. Freeman<br>论文链接：<a href="https://arxiv.org/abs/1904.111111、Learning">https://arxiv.org/abs/1904.111111、Learning</a> from Synthetic Data for Crowd Counting in the Wild<br>作者：Qi Wang, Junyu Gao, Wei Lin, Yuan Yuan<br>论文链接：<a href="https://arxiv.org/abs/1903.03303">https://arxiv.org/abs/1903.03303</a></p><p>2、Point in, Box out: Beyond Counting Persons in Crowds<br>作者：待更新<br>论文链接：<a href="https://github.com/xiaofanglegoc/xiaofanglegoc.github.io/blob/master/publications/cvpr2019.pdf">https://github.com/xiaofanglegoc/xiaofanglegoc.github.io/blob/master/publications/cvpr2019.pdf</a></p><p>3、Learning the Depths of Moving People by Watching Frozen People（Oral）<br>作者：Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, William T. Freeman<br>论文链接：<a href="https://arxiv.org/abs/1904.11111">https://arxiv.org/abs/1904.11111</a></p><h3 id="数据集、Benchmark-10"><a href="#数据集、Benchmark-10" class="headerlink" title="数据集、Benchmark 10"></a>数据集、Benchmark 10</h3><p>1、COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis<br>作者：Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, Jie Zhou<br>论文链接：<a href="https://arxiv.org/abs/1903.02874">https://arxiv.org/abs/1903.02874</a><br>项目链接：<a href="https://coin-dataset.github.io/">https://coin-dataset.github.io/</a><br>代码链接：<a href="https://github.com/coin-dataset/code">https://github.com/coin-dataset/code</a></p><p>2、RAVEN: A Dataset for Relational and Analogical Visual rEasoNing<br>作者：Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, Jie Zhou<br>论文链接：<a href="https://arxiv.org/abs/1903.02741">https://arxiv.org/abs/1903.02741</a><br>项目链接：<a href="https://wellyzhang.github.io/project/raven.html">https://wellyzhang.github.io/project/raven.html</a></p><p>3、SIXray : A Large-scale Security Inspection X-ray Benchmark for Prohibited Item Discovery in Overlapping Images（金山云大规模X光违禁品安检数据集）<br>作者：Caijing Miao, Lingxi Xie, Fang Wan, Chi Su, Hongye Liu, Jianbin Jiao, Qixiang Ye<br>论文链接：<a href="https://arxiv.org/abs/1901.00303">https://arxiv.org/abs/1901.00303</a><br>论文简要：本文针对X光安检数据集，提出了类别平衡的分层细化模型处置数据集存在的成绩。</p><p>4、A Cross-Season Correspondence Dataset for Robust Semantic Segmentation<br>作者：Måns Larsson, Erik Stenborg, Lars Hammarstrand, Torsten Sattler, Mark Pollefeys, Fredrik Kahl<br>论文链接：<a href="https://arxiv.org/abs/1903.06916">https://arxiv.org/abs/1903.06916</a></p><p>5、A Cross-Season Correspondence Dataset for Robust Semantic Segmentation<br>作者：Måns Larsson, Erik Stenborg, Lars Hammarstrand, Torsten Sattler, Mark Pollefeys, Fredrik Kahl<br>论文链接：<a href="https://arxiv.org/abs/1903.06916">https://arxiv.org/abs/1903.06916</a></p><p>6、A Realistic Dataset and Baseline Temporal Model for Early Drowsiness Detection<br>作者：Reza Ghoddoosian, Marnim Galib, Vassilis Athitsos<br>论文链接：<a href="https://arxiv.org/abs/1904.07312">https://arxiv.org/abs/1904.07312</a></p><p>7、A Poisson-Gaussian Denoising Dataset with Real Fluorescence Microscopy Images<br>作者：Yide Zhang, Yinhao Zhu, Evan Nichols, Qingfei Wang, Siyuan Zhang, Cody Smith, Scott Howard<br>论文链接：<a href="https://arxiv.org/abs/1812.10366">https://arxiv.org/abs/1812.10366</a><br>源码链接：<a href="https://github.com/bmmi/denoising-fluorescence">https://github.com/bmmi/denoising-fluorescence</a></p><p>8、ABC: A Big CAD Model Dataset For Geometric Deep Learning<br>作者：Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, Daniele Panozzo<br>论文链接：<a href="https://arxiv.org/abs/1812.06216">https://arxiv.org/abs/1812.06216</a><br>源码链接：<a href="https://github.com/deep-geometry/abc-dataset">https://github.com/deep-geometry/abc-dataset</a></p><p>9、A Dataset and Benchmark for Large-scale Multi-modal Face Anti-spoofing<br>作者：Sachin Mehta, Mohammad Rastegari, Linda Shapiro, Hannaneh Hajishirzi<br>论文链接：<a href="https://arxiv.org/abs/1811.11431">https://arxiv.org/abs/1811.11431</a><br>源码链接：<a href="https://github.com/sacmehta/ESPNetv2">https://github.com/sacmehta/ESPNetv2</a></p><p>10、DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images<br>作者：Yuying Ge, Ruimao Zhang, Lingyun Wu, Xiaogang Wang, Xiaoou Tang, Ping Luo<br>论文链接：<a href="https://arxiv.org/abs/1901.07973">https://arxiv.org/abs/1901.07973</a><br>源码链接：<a href="https://github.com/switchablenorms/DeepFashion2">https://github.com/switchablenorms/DeepFashion2</a></p><h3 id="行人重识别、行人检测-9"><a href="#行人重识别、行人检测-9" class="headerlink" title="行人重识别、行人检测 9"></a>行人重识别、行人检测 9</h3><p>1、Dissecting Person Re-identification from the Viewpoint of Viewpoint<br>作者：Xiaoxiao Sun, Liang Zheng<br>论文链接：<a href="https://arxiv.org/abs/1812.02162">https://arxiv.org/abs/1812.02162</a><br>源码链接：<a href="https://github.com/sxzrt/Dissecting-Person-Re-ID-from-the-Viewpoint-of-Viewpoint">https://github.com/sxzrt/Dissecting-Person-Re-ID-from-the-Viewpoint-of-Viewpoint</a></p><p>2、Unsupervised Person Re-identification by Soft Multilabel Learning(行人再识别，Oral)<br>作者：Hong-Xing Yu, Wei-Shi Zheng, Ancong Wu, Xiaowei Guo, Shaogang Gong, Jian-Huang Lai<br>论文链接：<a href="https://arxiv.org/abs/1903.06325">https://arxiv.org/abs/1903.06325</a><br>源码链接：<a href="https://github.com/KovenYu/MAR">https://github.com/KovenYu/MAR</a></p><p>3、Perceive Where to Focus: Learning Visibility-aware Part-level Features for Partial Person Re-identification<br>作者：Yifan Sun, Qin Xu, Yali Li, Chi Zhang, Yikang Li, Shengjin Wang, Jian Sun<br>论文链接：<a href="https://arxiv.org/abs/1904.00537">https://arxiv.org/abs/1904.00537</a></p><p>4、Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification<br>作者：Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li, Yi Yang<br>论文链接：<a href="https://arxiv.org/abs/1904.01990">https://arxiv.org/abs/1904.01990</a><br>代码链接：<a href="https://github.com/zhunzhong07/ECN">https://github.com/zhunzhong07/ECN</a></p><p>5、SSA-CNN: Semantic Self-Attention CNN for Pedestrian Detection<br>作者：Chengju Zhou,Meiqing Wu,Siew-Kei Lam<br>论文链接：<a href="https://arxiv.org/abs/1902.09080v1">https://arxiv.org/abs/1902.09080v1</a><br>论文摘要：本文将语义分割结果作为自我关注线索进行探索，以显着提高行人检测性能。</p><p>6、High-level Semantic Feature Detection:A New Perspective for Pedestrian Detection<br>作者：Wei Liu, Shengcai Liao, Weiqiang Ren, Weidong Hu, Yinan Yu<br>论文链接：<a href="https://arxiv.org/abs/1904.02948">https://arxiv.org/abs/1904.02948</a></p><p>7、High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection<br>作者：Zhao-Min Chen, Xiu-Shen Wei Peng Wang3Yanwen Guo1<br>论文链接：<a href="https://github.com/liuwei16/CSP/blob/master/docs/2019CVPR-CSP.pdf">https://github.com/liuwei16/CSP/blob/master/docs/2019CVPR-CSP.pdf</a><br>源码链接：<a href="https://github.com/liuwei16/CSP">https://github.com/liuwei16/CSP</a></p><p>8、Pedestrian Detection in Thermal Images using Saliency Maps<br>作者：Debasmita Ghose, Shasvat Mukeshkumar Desai, Sneha Bhattacharya, Deep Chakraborty, Madalina Fiterau, Tauhidur Rahman<br>论文链接：<a href="https://arxiv.org/abs/1904.06859">https://arxiv.org/abs/1904.06859</a></p><p>9、Learning to Reduce Dual-level Discrepancy for Infrared-Visible Person Re-identification(行人重识别）<br>作者：Zhixiang Wang, Zheng Wang, Yinqiang Zheng, Yung-Yu Chuang, Shin’ichi Satoh<br>论文链接：<a href="https://www.csie.ntu.edu.tw/~cyy/publications/papers/Wang2019LRD.pdf">https://www.csie.ntu.edu.tw/~cyy/publications/papers/Wang2019LRD.pdf</a></p><p>10、Pedestrian Detection with Autoregressive Network Phases<br>具有自回归网络阶段的行人检测<br>作者：Garrick Brazil, Xiaoming Liu<br>论文链接：<a href="https://arxiv.org/abs/1812.00440">https://arxiv.org/abs/1812.00440</a><br>源码链接：<a href="https://github.com/garrickbrazil/AR-Ped">https://github.com/garrickbrazil/AR-Ped</a></p><h3 id="其他-200"><a href="#其他-200" class="headerlink" title="其他 200"></a>其他 200</h3><p>2、Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video Demonstration<br>作者：De-An Huang, Suraj Nair, Danfei Xu, Yuke Zhu, Animesh Garg, Li Fei-Fei, Silvio Savarese, Juan Carlos Niebles<br>论文链接：<a href="https://arxiv.org/abs/1807.03480">https://arxiv.org/abs/1807.03480</a></p><p>3、Variational Bayesian Dropout<br>作者：Yuhang Liu, Wenyong Dong, Lei Zhang, Dong Gong, Qinfeng Shi<br>论文链接：<a href="https://arxiv.org/abs/1811.07533">https://arxiv.org/abs/1811.07533</a></p><p>4、LiFF: Light Field Features in Scale and Depth<br>作者：Donald G. Dansereau, Bernd Girod, Gordon Wetzstein<br>论文链接：<a href="https://arxiv.org/abs/1901.03916">https://arxiv.org/abs/1901.03916</a></p><p>5、Classification-Reconstruction Learning for Open-Set Recognition<br>作者：Ryota Yoshihashi, Wen Shao, Rei Kawakami, Shaodi You, Makoto Iida, Takeshi Naemura<br>论文链接：<a href="https://arxiv.org/abs/1812.04246">https://arxiv.org/abs/1812.04246</a></p><p>6、Weakly Supervised Deep Image Hashing through Tag Embeddings<br>作者：Vijetha Gattupalli, Yaoxin Zhuo, Baoxin Li<br>论文链接：<a href="https://arxiv.org/abs/1806.05804">https://arxiv.org/abs/1806.05804</a></p><p>7、InverseRenderNet: Learning single image inverse rendering<br>作者：Ye Yu, William A. P. Smith<br>论文链接：<a href="https://arxiv.org/abs/1811.12328">https://arxiv.org/abs/1811.12328</a></p><p>8、End-to-End Efficient Representation Learning via Cascading Combinatorial Optimization<br>作者：Yeonwoo Jeong, Yoonsuing Kim, Hyun Oh Song<br>论文链接：<a href="https://arxiv.org/abs/1902.10990">https://arxiv.org/abs/1902.10990</a><br>代码链接：<a href="https://github.com/maestrojeong/Deep-Hash-Table-CVPR19">https://github.com/maestrojeong/Deep-Hash-Table-CVPR19</a></p><p>9、Taking A Closer Look at Domain Shift: Category-level Adversaries for Semantics Consistent Domain Adaptation<br>作者：Yawei Luo, Liang Zheng, Tao Guan, Junqing Yu, Yi Yang<br>论文链接：<a href="https://arxiv.org/abs/1809.09478">https://arxiv.org/abs/1809.09478</a></p><p>10、Efficient Parameter-free Clustering Using First Neighbor Relations<br>作者：M. Saquib Sarfraz, Vivek Sharma, Rainer Stiefelhagen<br>论文链接：<a href="https://arxiv.org/abs/1902.11266">https://arxiv.org/abs/1902.11266</a><br>Reading Notes:FINCH, a new clustering algorithm, absolutily no hyperparameters , no need to specify no. of clusters. Scalable(Memory O(N)), very fast (ON(logN)) clusters ~8 million samples in 18 minutes on standard CPU.</p><p>11、3D Hand Shape and Pose from Images in the Wild<br>作者：Adnane Boukhayma, Rodrigo de Bem, Philip H.S. Torr<br>论文链接：<a href="https://arxiv.org/pdf/1902.03451.pdf">https://arxiv.org/pdf/1902.03451.pdf</a><br>Github链接：<a href="https://github.com/boukhayma/3dhand">https://github.com/boukhayma/3dhand</a></p><p>12、Monocular Total Capture: Posing Face, Body, and Hands in the Wild<br>作者：Donglai Xiang, Hanbyul Joo, Yaser Sheikh<br>论文链接：<a href="https://arxiv.org/pdf/1812.01598.pdf">https://arxiv.org/pdf/1812.01598.pdf</a><br>项目链接：<a href="https://domedb.perception.cs.cmu.edu/monototalcapture.html">https://domedb.perception.cs.cmu.edu/monototalcapture.html</a></p><p>13、Learning to Synthesize Motion Blur(Oral Presentation)<br>作者：Tim Brooks, Jonathan T. Barron<br>论文链接：<a href="https://arxiv.org/abs/1811.11745">https://arxiv.org/abs/1811.11745</a><br>project链接：<a href="https://timothybrooks.com/tech/motion-blur/">https://timothybrooks.com/tech/motion-blur/</a><br>Reading note:Frame interpolation techniques can be used to train a network to directly synthesize linear motion blur.</p><p>14、A General and Adaptive Robust Loss Function(Oral Presentation)<br>作者：Jonathan T. Barron<br>论文链接：<a href="https://arxiv.org/abs/1701.03077">https://arxiv.org/abs/1701.03077</a><br>Reading Note:A single robust loss function is a superset of many other common robust loss functions, and allows training to automatically adapt the robustness of its own loss.</p><p>15、Context-Aware Visual Compatibility Prediction<br>作者：Guillem Cucurull, Perouz Taslakian, David Vazquez<br>论文链接：<a href="https://arxiv.org/abs/1902.03646">https://arxiv.org/abs/1902.03646</a><br>Reading Note:It proposes a graph convolutional neural network that predicts compatibility between two items based on their visual features, as well as their context</p><p>16、A Kernelized Manifold Mapping to Diminish the Effect of Adversarial Perturbations<br>作者：Saeid Asgari Taghanaki Kumar Abhishek1 Shekoofeh Azizi and Ghassan Hamarneh<br>论文链接：<a href="https://cs.sfu.ca/~hamarneh/ecopy/cvpr2019.pdf">https://cs.sfu.ca/~hamarneh/ecopy/cvpr2019.pdf</a><br>Arxiv链接：<a href="https://arxiv.org/abs/1903.01015">https://arxiv.org/abs/1903.01015</a></p><p>17、Self-supervised Learning of Dense Shape Correspondence(Oral Presentation)<br>作者：Oshri Halimi, Or Litany, Emanuele Rodolà, Alex Bronstein, Ron Kimmel<br>论文链接：<a href="https://arxiv.org/abs/1812.02415">https://arxiv.org/abs/1812.02415</a></p><p>18、Art2Real: Unfolding the Reality of Artworks via Semantically-Aware Image-to-Image Translation<br>作者：Matteo Tomei, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara<br>论文链接：<a href="https://arxiv.org/abs/1811.10666">https://arxiv.org/abs/1811.10666</a></p><p>19、Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions<br>作者：Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara<br>论文链接：<a href="https://arxiv.org/abs/1811.10652">https://arxiv.org/abs/1811.10652</a><br>代码链接：<a href="https://github.com/aimagelab/show-control-and-tell">https://github.com/aimagelab/show-control-and-tell</a></p><p>20、Improving Referring Expression Grounding with Cross-modal Attention-guided Erasing<br>作者：Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, Hongsheng Li<br>论文链接：<a href="https://arxiv.org/abs/1903.00839">https://arxiv.org/abs/1903.00839</a></p><p>21、Learning From Noisy Labels By Regularized Estimation Of Annotator Confusion<br>作者：Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C. Alexander, Nathan Silberman<br>论文链接：<a href="https://arxiv.org/abs/1902.03680">https://arxiv.org/abs/1902.03680</a></p><p>22、Variational Autoencoders Pursue PCA Directions (by Accident)<br>作者：Michal Rolinek, Dominik Zietlow, Georg Martius<br>论文链接：<a href="https://arxiv.org/abs/1812.06775">https://arxiv.org/abs/1812.06775</a></p><p>23、The Regretful Agent: Heuristic-Aided Navigation through Progress Estimation(oral)<br>作者：Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming Xiong, Zsolt Kira<br>论文链接：<a href="https://arxiv.org/abs/1903.01602">https://arxiv.org/abs/1903.01602</a><br>Github:<a href="https://github.com/chihyaoma/regretful-agent">https://github.com/chihyaoma/regretful-agent</a></p><p>24、Understanding and Visualizing Deep Visual Saliency Models<br>作者：Sen He, Hamed R. Tavakoli, Ali Borji, Yang Mi, Nicolas Pugeault<br>论文链接：<a href="https://arxiv.org/abs/1903.02501">https://arxiv.org/abs/1903.02501</a></p><p>25、Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation（非最终版）<br>作者：Zhi Tian, Chunhua Shen, Tong He, Youliang Yanl<br>论文链接：<a href="https://arxiv.org/abs/1903.02120">https://arxiv.org/abs/1903.02120</a></p><p>26、Defense Against Adversarial Images using Web-Scale Nearest-Neighbor Search（oral)<br>作者：Abhimanyu Dubey, Laurens van der Maaten, Zeki Yalniz, Yixuan Li, Dhruv Mahajan<br>论文链接：<a href="https://arxiv.org/abs/1903.01612">https://arxiv.org/abs/1903.01612</a></p><p>27、Unsupervised Domain-Specific Deblurring via Disentangled Representations<br>作者：Boyu Lu, Jun-Cheng Chen, Rama Chellappa<br>论文链接：<a href="https://arxiv.org/abs/1903.01594">https://arxiv.org/abs/1903.01594</a></p><p>28、Selective Sensor Fusion for Neural Visual-Inertial Odometry<br>作者：Changhao Chen, Stefano Rosa, Yishu MiaoChris Xiaoxuan Lu, Wei Wu, Andrew Markham, Niki Trigoni<br>论文链接：<a href="https://arxiv.org/abs/1903.01534">https://arxiv.org/abs/1903.01534</a></p><p>29、.Learning Deep Compositional Grammatical Architectures for Visual Recognition<br>作者：Xilai Li, Tianfu Wu, Xi Song<br>论文链接：<a href="https://arxiv.org/abs/1711.05847">https://arxiv.org/abs/1711.05847</a><br>代码链接：<a href="https://github.com/xilaili/AOGNet">https://github.com/xilaili/AOGNet</a></p><p>30、Taking a Deeper Look at the Inverse Compositional Algorithm(oral)<br>作者：Zhaoyang Lv, Frank Dellaert, James M. Rehg, Andreas Geiger<br>论文链接：<a href="https://arxiv.org/pdf/1812.06861.pdf">https://arxiv.org/pdf/1812.06861.pdf</a><br>代码链接：<a href="https://github.com/lvzhaoyang/DeeperInverseCompositionalAlgorithm">https://github.com/lvzhaoyang/DeeperInverseCompositionalAlgorithm</a></p><p>31、Learning Transformation Synchronization<br>作者：Xiangru Huang, Zhenxiao Liang, Xiaowei Zhou, Yao Xie, Leonidas Guibas, and Qixing Huang<br>论文链接：<a href="https://arxiv.org/pdf/1901.09458.pdf">https://arxiv.org/pdf/1901.09458.pdf</a><br>代码链接: <a href="https://github.com/xiangruhuang/Learning2Sync">https://github.com/xiangruhuang/Learning2Sync</a></p><p>32、SR-LSTM: State Refinement for LSTM towards Pedestrian Trajectory Prediction<br>作者：Pu Zhang, Wanli Ouyang, Pengfei Zhang, Jianru Xue, Nanning Zheng<br>论文链接：<a href="https://arxiv.org/abs/1903.02793">https://arxiv.org/abs/1903.02793</a></p><p>33、ChamNet: Towards Efficient Network Design through Platform-Aware Model Adaptation(Facebook mobile vision team)<br>作者：Xiaoliang Dai, Peizhao Zhang, Bichen Wu, Hongxu Yin, Fei Sun, Yanghan Wang, Marat Dukhan, Yunqing Hu, Yiming Wu, Yangqing Jia, Peter Vajda, Matt Uyttendaele, Niraj K. Jha<br>论文链接：<a href="https://arxiv.org/abs/1812.08934">https://arxiv.org/abs/1812.08934</a></p><p>34、FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search(Facebook mobile vision team)<br>作者：Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, Kurt Keutzer<br>论文链接：<a href="https://arxiv.org/abs/1812.03443">https://arxiv.org/abs/1812.03443</a></p><p>35、PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding<br>作者：Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, Hao Su<br>项目链接：<a href="https://cs.stanford.edu/~kaichun/partnet/">https://cs.stanford.edu/~kaichun/partnet/</a><br>论文链接：<a href="https://arxiv.org/abs/1812.02713">https://arxiv.org/abs/1812.02713</a><br>简要：A 3D object database with fine-grained and hierarchical part annotation. To assist segmentation and affordance research.</p><p>36、Adversarial Defense by Stratified Convolutional Sparse Coding<br>作者：Bo Sun, Nian-hsuan Tsai, Fangchen Liu, Ronald Yu, Hao Su<br>论文链接：<a href="https://arxiv.org/abs/1812.00037">https://arxiv.org/abs/1812.00037</a><br>简要：An attack-agnostic defense mechanism for neural networks.</p><p>37、Semantically Tied Paired Cycle Consistency for Zero-Shot Sketch-based Image Retrieval<br>作者：Anjan Dutta, Zeynep Akata<br>论文链接：<a href="https://arxiv.org/abs/1903.03372">https://arxiv.org/abs/1903.03372</a></p><p>39、Ranked List Loss for Deep Metric Learning<br>作者：Xinshao Wang, Yang Hua, Elyor Kodirov, Guosheng Hu, Romain Garnier, Neil M. Robertson<br>论文链接：<a href="https://arxiv.org/abs/1903.03238">https://arxiv.org/abs/1903.03238</a></p><p>40、Anatomical Priors in Convolutional Networks for Unsupervised Biomedical Segmentation<br>作者：Adrian V. Dalca, John Guttag, Mert R. Sabuncu<br>论文链接：<a href="https://arxiv.org/abs/1903.03148">https://arxiv.org/abs/1903.03148</a></p><p>41、Refine and Distill: Exploiting Cycle-Inconsistency and Knowledge Distillation for Unsupervised Monocular Depth Estimation<br>作者：Andrea Pilzer, Stéphane Lathuilière, Nicu Sebe, Elisa Ricci<br>论文链接：<a href="https://arxiv.org/pdf/1903.04202.pdf">https://arxiv.org/pdf/1903.04202.pdf</a></p><p>42、Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation（领域自适应）<br>作者：Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, Daniel Ulbricht<br>论文链接：<a href="https://arxiv.org/abs/1903.04064">https://arxiv.org/abs/1903.04064</a></p><p>43、Deep Robust Subjective Visual Property Prediction in Crowdsourcing<br>作者：Qianqian Xu, Zhiyong Yang, Yangbangyan Jiang, Xiaochun Cao, Qingming Huang, Yuan Yao<br>论文链接：<a href="https://arxiv.org/abs/1903.03956">https://arxiv.org/abs/1903.03956</a></p><p>44、Shape2Motion: Joint Analysis of Motion Parts and Attributes from 3D Shapes<br>作者：Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qinping Zhao, Kai Xu<br>论文链接：<a href="https://arxiv.org/abs/1903.03911">https://arxiv.org/abs/1903.03911</a></p><p>45、Fast Single Image Reflection Suppression via Convex Optimization<br>作者：Yang Yang, Wenye Ma, Yin Zheng, Jian-Feng Cai, Weiyu Xu<br>论文链接：<a href="https://arxiv.org/abs/1903.03889">https://arxiv.org/abs/1903.03889</a></p><p>46、Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks<br>作者：Kuan Fang, Alexander Toshev, Li Fei-Fei, Silvio Savarese<br>论文链接：<a href="https://arxiv.org/abs/1903.03878">https://arxiv.org/abs/1903.03878</a></p><p>47、SSN: Learning Sparse Switchable Normalization via SparsestMax<br>作者：Wenqi Shao, Tianjian Meng, Jingyu Li, Ruimao Zhang, Yudian Li, Xiaogang Wang, Ping Luo<br>论文链接：<a href="https://arxiv.org/abs/1903.03793">https://arxiv.org/abs/1903.03793</a></p><p>48、Partial Order Pruning: for Best Speed/Accuracy Trade-off in Neural Architecture Search<br>作者：Xin Li, Yiming Zhou, Zheng Pan, Jiashi Feng<br>论文链接：<a href="https://arxiv.org/abs/1903.03777">https://arxiv.org/abs/1903.03777</a></p><p>49、Dense Classification and Implanting for Few-Shot Learning<br>作者：Yann Lifchitz, Yannis Avrithis, Sylvaine Picard, Andrei Bursuc<br>论文链接：<a href="https://arxiv.org/abs/1903.05050">https://arxiv.org/abs/1903.05050</a></p><p>50、A Skeleton-bridged Deep Learning Approach for Generating Meshes of Complex Topologies from Single RGB Images(oral)<br>作者：Jiapeng Tang, Xiaoguang Han, Junyi Pan, Kui Jia, Xin Tong<br>论文链接：<a href="https://arxiv.org/abs/1903.04704">https://arxiv.org/abs/1903.04704</a></p><p>51、Real-time self-adaptive deep stereo(oral)<br>作者：Alessio Tonioni, Fabio Tosi, Matteo Poggi, Stefano Mattoccia, Luigi Di Stefano<br>论文链接：<a href="https://arxiv.org/abs/1903.04704">https://arxiv.org/abs/1903.04704</a><br>源码链接：<a href="https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo">https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo</a></p><p>52、Scan2CAD: Learning CAD Model Alignment in RGB-D Scans(oral)<br>作者：Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X. Chang, Matthias Nießner<br>论文链接：<a href="https://arxiv.org/abs/1811.11187">https://arxiv.org/abs/1811.11187</a><br>源码链接：<a href="https://github.com/skanti/Scan2CAD">https://github.com/skanti/Scan2CAD</a><br>简要：Present Scan2CAD, a novel data-driven method that learns to align 3D CAD models from a shape database to 3D scans.</p><p>53、HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch Data Augmentation<br>作者：Cheng Sun, Chi-Wei Hsiao, Min Sun, Hwann-Tzong Chen<br>论文链接：<a href="https://arxiv.org/abs/1901.03861">https://arxiv.org/abs/1901.03861</a><br>源码链接：<a href="https://github.com/sunset1995/HorizonNet">https://github.com/sunset1995/HorizonNet</a></p><p>54、A Skeleton-bridged Deep Learning Approach for Generating Meshes of Complex Topologies from Single RGB Images(oral)<br>作者：Jiapeng Tang, Xiaoguang Han, Junyi Pan, Kui Jia, Xin Tong<br>论文链接：<a href="https://arxiv.org/abs/1903.04704">https://arxiv.org/abs/1903.04704</a></p><p>56、Tangent-Normal Adversarial Regularization for Semi-supervised Learning<br>作者：Bing Yu, Jingfeng Wu, Jinwen Ma, Zhanxing Zhu<br>论文链接：<a href="https://arxiv.org/abs/1808.06088">https://arxiv.org/abs/1808.06088</a></p><p>57、Bringing Alive Blurred Moments<br>作者：Kuldeep Purohit, Anshul Shah, A. N. Rajagopalan<br>论文链接：<a href="https://arxiv.org/abs/1804.02913">https://arxiv.org/abs/1804.02913</a></p><p>58、A Decomposition Algorithm for the Sparse Generalized Eigenvalue Problem<br>作者：Ganzhao Yuan, Li Shen, Wei-Shi Zheng<br>论文链接：<a href="https://arxiv.org/abs/1802.09303">https://arxiv.org/abs/1802.09303</a></p><p>59、Hardness-Aware Deep Metric Learning（oral)<br>作者：Wenzhao Zheng, Zhaodong Chen, Jiwen Lu, Jie Zhou<br>论文链接：<a href="https://arxiv.org/abs/1903.05503">https://arxiv.org/abs/1903.05503</a><br>代码链接：<a href="https://github.com/wzzheng/HDML（待更新）">https://github.com/wzzheng/HDML（待更新）</a></p><p>60、Depth Coefficients for Depth Completion<br>作者：Saif Imran, Yunfei Long, Xiaoming Liu, Daniel Morris<br>论文链接：<a href="https://arxiv.org/abs/1903.05421">https://arxiv.org/abs/1903.05421</a></p><p>61、3D Guided Fine-Grained Face Manipulation<br>作者：Zhenglin Geng, Chen Cao, Sergey Tulyakov<br>论文链接：<a href="https://arxiv.org/abs/1902.08900">https://arxiv.org/abs/1902.08900</a><br>简要：Disentangle shape and texture and can continuously manipulate the facial expression.</p><p>62、Scene Categorization from Contours: Medial Axis Based Salience Measures<br>作者：Morteza Rezanejad, Gabriel Downs, John Wilder, Dirk B. Walther, Allan Jepson, Sven Dickinson, Kaleem Siddiqi<br>论文链接：<a href="https://arxiv.org/abs/1811.10524v1">https://arxiv.org/abs/1811.10524v1</a></p><p>64、Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning<br>作者：Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, In So Kweon<br>论文链接：<a href="https://arxiv.org/abs/1903.05942">https://arxiv.org/abs/1903.05942</a></p><p>65、Putting Humans in a Scene: Learning Affordance in 3D Indoor Environments<br>作者：Xueting Li, SIfei Liu, Kihwan Kim, Xiaolong Wang, Ming-Hsuan Yang, Jan Kautz<br>论文链接：<a href="https://arxiv.org/abs/1903.05690">https://arxiv.org/abs/1903.05690</a></p><p>66、Bringing Blurry Alive at High Frame-Rate with an Event Camera<br>作者：Liyuan Pan, Richard Hartley, Cedric Scheerlinck, Miaomiao Liu, Xin Yu, Yuchao Dai<br>论文链接：<a href="https://arxiv.org/abs/1903.06531">https://arxiv.org/abs/1903.06531</a></p><p>67、MFAS: Multimodal Fusion Architecture Search<br>作者：Juan-Manuel Pérez-Rúa, Valentin Vielzeuf, Stéphane Pateux, Moez Baccouche, Frédéric Jurie<br>论文链接：<a href="https://arxiv.org/abs/1903.06496">https://arxiv.org/abs/1903.06496</a></p><p>68、SimulCap : Single-View Human Performance Capture with Cloth Simulation<br>作者：Tao Yu, Zerong Zheng, Yuan Zhong, Jianhui Zhao, Qionghai Dai, Gerard Pons-Moll, Yebin Liu<br>论文链接：<a href="https://arxiv.org/abs/1903.06323">https://arxiv.org/abs/1903.06323</a></p><p>69、Learning to Reconstruct People in Clothing from a Single RGB Camera<br>作者：Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar, Christian Theobalt, Gerard Pons-Moll<br>论文链接：<a href="https://arxiv.org/abs/1903.05885">https://arxiv.org/abs/1903.05885</a></p><p>70、Pluralistic Image Completion<br>作者：Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai<br>论文链接：<a href="https://arxiv.org/abs/1903.04227">https://arxiv.org/abs/1903.04227</a><br>源码链接：<a href="https://github.com/lyndonzheng/Pluralistic-Inpainting">https://github.com/lyndonzheng/Pluralistic-Inpainting</a><br>项目链接：<a href="https://www.chuanxiaz.com/publication/pluralistic/">https://www.chuanxiaz.com/publication/pluralistic/</a></p><p>71、Snapshot Distillation: Teacher-Student Optimization in One Generation（金山云）<br>作者：Chenglin Yang, Lingxi Xie, Chi Su, Alan L. Yuille<br>论文链接：<a href="https://arxiv.org/abs/1812.00123v1">https://arxiv.org/abs/1812.00123v1</a><br>论文简要：本文引见了第一种可以在训练单个模型的条件下完成教员-先生优化的办法——快照蒸馏(Snapshot Distillation)，在不引入过多的计算耗费状况下，完成了继续的功能提升。</p><p>72、Iterative Reorganization with Weak Spatial Constraints: Solving Arbitrary Jigsaw Puzzles for Unsupervised Representation Learning（金山云）<br>作者：Chen Wei, Lingxi Xie, Xutong Ren, Yingda Xia, Chi Su, Jiaying Liu, Qi Tian, Alan L. Yuille<br>论文链接：<a href="https://arxiv.org/abs/1812.00329">https://arxiv.org/abs/1812.00329</a><br>论文简要：本文提出一种适用于恣意网格尺寸与维度的“拼图”成绩的新办法，同时提出了一个根本且具有普遍意义的准绳，即在无监视场景中较弱的信息更容易被学习，且具有更好的可迁移性。</p><p>73、Learning Correspondence from the Cycle-Consistency of Time<br>作者：Xiaolong Wang, Allan Jabri, Alexei A. Efros<br>论文链接：<a href="https://arxiv.org/abs/1903.07593">https://arxiv.org/abs/1903.07593</a><br>项目链接：<a href="https://ajabri.github.io/timecycle/">https://ajabri.github.io/timecycle/</a></p><p>74、Understanding the Limitations of CNN-based Absolute Camera Pose Regression<br>作者：Torsten Sattler, Qunjie Zhou, Marc Pollefeys, Laura Leal-Taixe<br>论文链接：<a href="https://arxiv.org/abs/1903.07504">https://arxiv.org/abs/1903.07504</a></p><p>75、Semantic Image Synthesis with Spatially-Adaptive Normalization(Oral, 英伟达)<br>作者：Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu<br>论文链接：<a href="https://arxiv.org/abs/1903.07291">https://arxiv.org/abs/1903.07291</a></p><p>76、Graph Convolutional Label Noise Cleaner: Train a Plug-and-play Action Classifier for Anomaly Detection<br>作者：Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, Thomas H. Li, Ge Li<br>论文链接：<a href="https://arxiv.org/abs/1903.07256">https://arxiv.org/abs/1903.07256</a></p><p>77、QATM: Quality-Aware Template Matching For Deep Learning<br>作者：Jiaxin Cheng, Yue Wu, Wael Abd-Almageed, Premkumar Natarajan<br>论文链接：<a href="https://arxiv.org/abs/1903.07254">https://arxiv.org/abs/1903.07254</a></p><p>78、AdaGraph: Unifying Predictive and ContinuousDomain Adaptation through Graphs(Oral)<br>作者：Massimiliano Mancini, Samuel Rota Bulò, Barbara Caputo, Elisa Ricci<br>论文链接：<a href="https://arxiv.org/abs/1903.07062">https://arxiv.org/abs/1903.07062</a></p><p>79、Unsupervised Part-Based Disentangling of Object Shape and Appearance(Oral)<br>作者：Dominik Lorenz, Leonard Bereska, Timo Milbich, Björn Ommer<br>论文链接：<a href="https://arxiv.org/abs/1903.06946">https://arxiv.org/abs/1903.06946</a></p><p>80、Fast Interactive Object Annotation with Curve-GCN<br>作者：Huan Ling, Jun Gao, Amlan Kar, Wenzheng Chen, Sanja Fidler<br>论文链接：<a href="https://arxiv.org/abs/1903.06874">https://arxiv.org/abs/1903.06874</a></p><p>81、Domain Generalization by Solving Jigsaw Puzzles<br>作者：Fabio Maria Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, Tatiana Tommasi<br>论文链接：<a href="https://arxiv.org/abs/1903.06864">https://arxiv.org/abs/1903.06864</a></p><p>82、Learning Correspondence from the Cycle-Consistency of Time<br>作者：Xiaolong Wang, Allan Jabri, Alexei A. Efros<br>论文链接：<a href="https://arxiv.org/abs/1903.07593">https://arxiv.org/abs/1903.07593</a><br>项目链接：<a href="https://ajabri.github.io/timecycle/">https://ajabri.github.io/timecycle/</a></p><p>83、Understanding the Limitations of CNN-based Absolute Camera Pose Regression<br>作者：Torsten Sattler, Qunjie Zhou, Marc Pollefeys, Laura Leal-Taixe<br>论文链接：<a href="https://arxiv.org/abs/1903.07504">https://arxiv.org/abs/1903.07504</a></p><p>84、Semantic Image Synthesis with Spatially-Adaptive Normalization(Oral, 英伟达)<br>作者：Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu<br>论文链接：<a href="https://arxiv.org/abs/1903.07291">https://arxiv.org/abs/1903.07291</a></p><p>85、Graph Convolutional Label Noise Cleaner: Train a Plug-and-play Action Classifier for Anomaly Detection<br>作者：Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, Thomas H. Li, Ge Li<br>论文链接：<a href="https://arxiv.org/abs/1903.07256">https://arxiv.org/abs/1903.07256</a></p><p>86、QATM: Quality-Aware Template Matching For Deep Learning<br>作者：Jiaxin Cheng, Yue Wu, Wael Abd-Almageed, Premkumar Natarajan<br>论文链接：<a href="https://arxiv.org/abs/1903.07254">https://arxiv.org/abs/1903.07254</a></p><p>87、AdaGraph: Unifying Predictive and ContinuousDomain Adaptation through Graphs(Oral)<br>作者：Massimiliano Mancini, Samuel Rota Bulò, Barbara Caputo, Elisa Ricci<br>论文链接：<a href="https://arxiv.org/abs/1903.07062">https://arxiv.org/abs/1903.07062</a></p><p>88、Unsupervised Part-Based Disentangling of Object Shape and Appearance(Oral)<br>作者：Dominik Lorenz, Leonard Bereska, Timo Milbich, Björn Ommer<br>论文链接：<a href="https://arxiv.org/abs/1903.06946">https://arxiv.org/abs/1903.06946</a></p><p>89、Fast Interactive Object Annotation with Curve-GCN<br>作者：Huan Ling, Jun Gao, Amlan Kar, Wenzheng Chen, Sanja Fidler<br>论文链接：<a href="https://arxiv.org/abs/1903.06874">https://arxiv.org/abs/1903.06874</a></p><p>90、Domain Generalization by Solving Jigsaw Puzzles<br>作者：Fabio Maria Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, Tatiana Tommasi<br>论文链接：<a href="https://arxiv.org/abs/1903.06864">https://arxiv.org/abs/1903.06864</a></p><p>91、Neural Sequential Phrase Grounding (SeqGROUND)<br>作者：Pelin Dogan, Leonid Sigal, Markus Gross<br>论文链接：<a href="https://arxiv.org/abs/1903.07669">https://arxiv.org/abs/1903.07669</a></p><p>92、Probabilistic End-to-end Noise Correction for Learning with Noisy Labels<br>作者：Kun Yi, Jianxin Wu<br>论文链接：<a href="https://arxiv.org/abs/1903.07788">https://arxiv.org/abs/1903.07788</a></p><p>93、MagicVO: End-to-End Monocular Visual Odometry through Deep Bi-directional Recurrent Convolutional Neural Network（单目视觉测距）<br>作者：Jian Jiao, Jichao Jiao, Yaokai Mo, Weilun Liu, Zhongliang Deng<br>论文链接：<a href="https://arxiv.org/abs/1811.10964">https://arxiv.org/abs/1811.10964</a><br>论文摘要：本文提出了一种解决单眼视觉测距问题的新框架，称为MagicVO。 基于卷积神经网络（CNN）和双向LSTM（Bi-LSTM），MagicVO在摄像机的每个位置输出6-DoF绝对标度姿势，并以一系列连续单目图像作为输入。</p><p>94、Hierarchical Discrete Distribution Decomposition for Match Density Estimation（立体匹配）<br>作者：Zhichao Yin, Trevor Darrell, Fisher Yu<br>论文链接：<a href="https://arxiv.org/abs/1812.06264">https://arxiv.org/abs/1812.06264</a><br>论文简要：在本文中，我们提出了分层离散分布分解，称为HD3，以学习概率点和区域匹配。它不仅可以模拟匹配不确定性，还可以模拟区域传播。</p><p>95、Learning Linear Transformations for Fast Arbitrary Style Transfer<br>作者：Xueting Li, Sifei Liu, Jan Kautz, Ming-Hsuan Yang<br>论文链接：<a href="https://arxiv.org/pdf/1808.04537v1.pdf">https://arxiv.org/pdf/1808.04537v1.pdf</a></p><p>96、Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses(Oral)<br>作者：Jérôme Rony, Luiz G. Hafemann, Luiz S. Oliveira, Ismail Ben Ayed, Robert Sabourin, Eric Granger<br>论文链接：<a href="https://arxiv.org/abs/1811.09600">https://arxiv.org/abs/1811.09600</a><br>代码链接：<a href="https://github.com/jeromerony/fast_adversarial">https://github.com/jeromerony/fast_adversarial</a></p><p>97、Graphical Contrastive Losses for Scene Graph Generation<br>作者：Ji Zhang, Kevin J. Shih, Ahmed Elgammal, Andrew Tao, Bryan Catanzaro<br>论文链接：<a href="https://arxiv.org/abs/1903.02728">https://arxiv.org/abs/1903.02728</a><br>代码链接：<a href="https://github.com/NVIDIA/ContrastiveLosses4VRD">https://github.com/NVIDIA/ContrastiveLosses4VRD</a></p><p>98、Pay attention! - Robustifying a Deep Visuomotor Policy through Task-Focused Attention<br>作者：Pooya Abolghasemi, Amir Mazaheri, Mubarak Shah, Ladislau Bölöni<br>论文链接：<a href="https://arxiv.org/abs/1809.10093">https://arxiv.org/abs/1809.10093</a></p><p>99、Cross-task weakly supervised learning from instructional videos<br>作者：Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, Josef Sivic<br>论文链接：<a href="https://arxiv.org/abs/1903.08225">https://arxiv.org/abs/1903.08225</a></p><p>100、Explainable and Explicit Visual Reasoning over Scene Graphs<br>作者：Jiaxin Shi, Hanwang Zhang, Juanzi Li<br>论文链接：<a href="https://arxiv.org/abs/1812.01855">https://arxiv.org/abs/1812.01855</a><br>代码链接：<a href="https://github.com/shijx12/XNM-Net">https://github.com/shijx12/XNM-Net</a></p><p>101、Single Image Deraining: A Comprehensive Benchmark Analysis<br>作者：Siyuan Li, Iago Breno Araujo, Wenqi Ren, Zhangyang Wang, Eric K. Tokuda, Roberto Hirata Junior, Roberto Cesar-Junior, Jiawan Zhang, Xiaojie Guo, Xiaochun Cao<br>论文链接：<a href="https://arxiv.org/abs/1903.08558">https://arxiv.org/abs/1903.08558</a><br>代码链接：<a href="https://github.com/lsy17096535/Single-Image-Deraining">https://github.com/lsy17096535/Single-Image-Deraining</a></p><p>102、Im2Pencil: Controllable Pencil Illustration from Photographs<br>作者：Yijun Li, Chen Fang, Aaron Hertzmann, Eli Shechtman, Ming-Hsuan Yang<br>论文链接：<a href="https://arxiv.org/abs/1903.08682">https://arxiv.org/abs/1903.08682</a></p><p>104、DSFD: Dual Shot Face Detector（腾讯优图）<br>作者：Jian Li, Yabiao Wang, Changan Wang, Ying Tai<br>论文链接：<a href="https://arxiv.org/abs/1810.10220">https://arxiv.org/abs/1810.10220</a><br>代码链接：<a href="https://github.com/TencentYoutuResearch/FaceDetection-DSFD">https://github.com/TencentYoutuResearch/FaceDetection-DSFD</a><br>微信公众号介绍链接：<a href="https://mp.weixin.qq.com/s/0rTCeHumVSv07hMCaCd7EA">https://mp.weixin.qq.com/s/0rTCeHumVSv07hMCaCd7EA</a></p><p>105、Attention-aware Multi-stroke Style Transfer<br>作者：Yuan Yao, Jianqiang Ren, Xuansong Xie, Weidong Liu, Yong-Jin Liu, Jun Wang<br>论文链接：<a href="https://arxiv.org/abs/1901.05127">https://arxiv.org/abs/1901.05127</a><br>项目链接：<a href="https://sites.google.com/view/yuanyao/attention-aware-multi-stroke-style-transfer">https://sites.google.com/view/yuanyao/attention-aware-multi-stroke-style-transfer</a></p><p>106、Veritatem Dies Aperit- Temporally Consistent Depth Prediction Enabled by a Multi-Task Geometric and Semantic Scene Understanding Approach<br>作者：Amir Atapour-Abarghouei, Toby P. Breckon<br>论文链接：<a href="https://arxiv.org/abs/1903.10764">https://arxiv.org/abs/1903.10764</a></p><p>107、Semantic Alignment: Finding Semantically Consistent Ground-truth for Facial Landmark Detection<br>作者：Zhiwei Liu, Xiangyu Zhu, Guosheng Hu, Haiyun Guo, Ming Tang, Zhen Lei, Neil M. Robertson, Jinqiao Wang<br>论文链接：<a href="https://arxiv.org/abs/1903.10661">https://arxiv.org/abs/1903.10661</a></p><p>108、Discovering Visual Patterns in Art Collections with Spatially-consistent Feature Learning<br>作者：Xi Shen, Alexei A. Efros, Mathieu Aubry<br>论文链接：<a href="https://arxiv.org/abs/1903.02678">https://arxiv.org/abs/1903.02678</a></p><p>109、DeeperLab: Single-Shot Image Parser<br>作者：Tien-Ju Yang, Maxwell D. Collins, Yukun Zhu, Jyh-Jing Hwang, Ting Liu, Xiao Zhang, Vivienne Sze, George Papandreou, Liang-Chieh Chen<br>论文链接：<a href="https://arxiv.org/abs/1902.05093">https://arxiv.org/abs/1902.05093</a><br>代码链接：<a href="https://github.com/tensorflow/models/tree/master/research/deeplab/evaluation">https://github.com/tensorflow/models/tree/master/research/deeplab/evaluation</a><br>项目链接：<a href="https://deeperlab.mit.edu/">https://deeperlab.mit.edu/</a></p><p>110、Im2Pencil: Controllable Pencil Illustration from Photographs(Adobe与谷歌云等）<br>作者：Yijun Li,Chen Fang, Aaron Hertzmann, Eli Shechtman, Ming-Hsuan Yang<br>论文链接：<a href="https://drive.google.com/file/d/1sl5IBD36bMWAvKH7Uz7An0mcrIOmlopv/view">https://drive.google.com/file/d/1sl5IBD36bMWAvKH7Uz7An0mcrIOmlopv/view</a><br>代码链接：<a href="https://github.com/Yijunmaverick/Im2Pencil">https://github.com/Yijunmaverick/Im2Pencil</a></p><p>111、Unsupervised Image Captioning<br>作者：Yang Feng, Lin Ma, Wei Liu, Jiebo Luo<br>论文链接：<a href="https://arxiv.org/abs/1811.10787">https://arxiv.org/abs/1811.10787</a><br>代码链接：<a href="https://github.com/fengyang0317/unsupervised_captioning">https://github.com/fengyang0317/unsupervised_captioning</a></p><p>112、An End-to-End Network for Generating Social Relationship Graphs<br>作者：Arushi Goel, Keng Teck Ma, Cheston Tan<br>论文链接：<a href="https://arxiv.org/abs/1903.09784">https://arxiv.org/abs/1903.09784</a></p><p>113、f-VAEGAN-D2: A Feature Generating Framework for Any-Shot Learning<br>作者：Yongqin Xian, Saurabh Sharma, Bernt Schiele, Zeynep Akata<br>论文链接：<a href="https://arxiv.org/abs/1903.10132">https://arxiv.org/abs/1903.10132</a></p><p>114、Scale-Adaptive Neural Dense Features: Learning via Hierarchical Context Aggregation<br>作者：Jaime Spencer, Richard Bowden, Simon Hadfield<br>论文链接：<a href="https://arxiv.org/abs/1903.10427">https://arxiv.org/abs/1903.10427</a></p><p>115、Learning Attraction Field Reprensentation for Robust Line Segment Detection<br>作者：Nan Xue, Song Bai, Fudong Wang, Gui-Song Xia, Tianfu Wu, Liangpei Zhang<br>论文链接：<a href="https://arxiv.org/abs/1812.02122">https://arxiv.org/abs/1812.02122</a><br>代码链接：<a href="https://github.com/cherubicXN/afm_cvpr2019">https://github.com/cherubicXN/afm_cvpr2019</a></p><p>116、Feature Denoising for Improving Adversarial Robustness<br>作者：Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He<br>论文链接：<a href="https://arxiv.org/abs/1812.03411v2">https://arxiv.org/abs/1812.03411v2</a><br>代码链接：<a href="https://github.com/facebookresearch/ImageNet-Adversarial-Training">https://github.com/facebookresearch/ImageNet-Adversarial-Training</a></p><p>117、DynTypo: Example-based Dynamic Text Effects Transfer<br>作者：Yifang Men Zhouhui Lian Yingmin Tang Jianguo Xiao<br>项目链接：<a href="https://menyifang.github.io/projects/DynTypo/DynTypo.html">https://menyifang.github.io/projects/DynTypo/DynTypo.html</a></p><p>118、Progressive Image Deraining Networks: A Better and Simpler Baseline<br>作者：Dongwei Ren, Wangmeng Zuo, Qinghua Hu, Pengfei Zhu, Deyu Meng<br>论文链接：<a href="https://arxiv.org/abs/1901.09221">https://arxiv.org/abs/1901.09221</a><br>代码链接：<a href="https://github.com/csdwren/PReNet">https://github.com/csdwren/PReNet</a></p><p>119、Transferable Interactiveness Prior for Human-Object Interaction Detection<br>作者：Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma, Hao-Shu Fang, Yan-Feng Wang, Cewu Lu<br>论文链接：<a href="https://arxiv.org/abs/1811.08264">https://arxiv.org/abs/1811.08264</a><br>代码链接：<a href="https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network">https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network</a></p><p>120、Speed Invariant Time Surface for Learning to Detect Corner Points with Event-Based Cameras<br>作者：Jacques Manderscheid, Amos Sironi, Nicolas Bourdis, Davide Migliore, Vincent Lepetit<br>论文链接：<a href="https://arxiv.org/abs/1903.11332">https://arxiv.org/abs/1903.11332</a></p><p>121、Self-Supervised Learning via Conditional Motion Propagation<br>作者：Xiaohang Zhan, Xingang Pan, Ziwei Liu, Dahua Lin, Chen Change Loy<br>论文链接：<a href="https://arxiv.org/abs/1903.11412">https://arxiv.org/abs/1903.11412</a></p><p>122、Privacy Protection in Street-View Panoramas using Depth and Multi-View Imagery<br>作者：Ries Uittenbogaard, Clint Sebastian, Julien Vijverberg, Bas Boom, Dariu M. Gavrila, Peter H.N. de With<br>论文链接：<a href="https://arxiv.org/abs/1903.11532">https://arxiv.org/abs/1903.11532</a></p><p>123、Rethinking Knowledge Graph Propagation for Zero-Shot Learning（零样本学习/图卷积网络）<br>作者：Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, Eric P. Xing<br>论文链接：<a href="https://arxiv.org/abs/1805.11724v3">https://arxiv.org/abs/1805.11724v3</a><br>代码链接：<a href="https://github.com/cyvius96/adgpm">https://github.com/cyvius96/adgpm</a></p><p>124、End-to-End Multi-Task Learning with Attention<br>作者：Shikun Liu, Edward Johns, Andrew J. Davison<br>论文链接：<a href="https://arxiv.org/abs/1803.10704">https://arxiv.org/abs/1803.10704</a></p><p>125、Deep Transfer Learning for Multiple Class Novelty Detection<br>作者：Pramuditha Perera, Vishal M. Patel<br>论文链接：<a href="https://arxiv.org/abs/1903.02196">https://arxiv.org/abs/1903.02196</a></p><p>126、Zoom to Learn, Learn to Zoom（Oral)<br>作者：Xuaner Zhang Qifeng Chen Ren Ng Vladlen Koltun<br>论文链接：<a href="https://cqf.io/papers/Zoom_To_Learn_CVPR2019.pdf">https://cqf.io/papers/Zoom_To_Learn_CVPR2019.pdf</a></p><p>127、Learning to Transfer Examples for Partial Domain Adaptation（领域自适应）<br>作者：Zhangjie Cao, Kaichao You, Mingsheng Long, Jianmin Wang, Qiang Yang<br>论文链接：<a href="https://arxiv.org/abs/1903.12230">https://arxiv.org/abs/1903.12230</a></p><p>128、Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning<br>作者：Wenbin Li, Lei Wang, Jinglin Xu, Jing Huo, Yang Gao, Jiebo Luo<br>论文链接：<a href="https://arxiv.org/abs/1903.12290">https://arxiv.org/abs/1903.12290</a></p><p>129、Lending Orientation to Neural Networks for Cross-view Geo-localization<br>作者：Liu Liu, Hongdong Li<br>论文链接：<a href="https://arxiv.org/abs/1903.12351">https://arxiv.org/abs/1903.12351</a><br>代码链接：<a href="https://github.com/Liumouliu/OriCNN">https://github.com/Liumouliu/OriCNN</a></p><p>130、Towards Accurate Task Accomplishment with Low-Cost Robotic Arms<br>作者：Yiming Zuo, Weichao Qiu, Lingxi Xie, Fangwei Zhong, Yizhou Wang, Alan L. Yuille<br>论文链接：<a href="https://arxiv.org/abs/1812.00725">https://arxiv.org/abs/1812.00725</a></p><p>131、Robustness of 3D Deep Learning in an Adversarial Setting<br>作者：Matthew Wicker, Marta Kwiatkowska<br>论文链接：<a href="https://arxiv.org/abs/1904.00923">https://arxiv.org/abs/1904.00923</a><br>源码链接：<a href="https://github.com/matthewwicker/IterativeSalienceOcclusion">https://github.com/matthewwicker/IterativeSalienceOcclusion</a></p><p>132、Depth from a polarisation + RGB stereo pair<br>作者：Dizhong Zhu, William A.P. Smith<br>论文链接：<a href="https://arxiv.org/abs/1903.12061">https://arxiv.org/abs/1903.12061</a><br>源码链接：<a href="https://github.com/AmosZhu/CVPR2019">https://github.com/AmosZhu/CVPR2019</a></p><p>133、Curls &amp; Whey: Boosting Black-Box Adversarial Attacks(Oral)<br>作者：Yucheng Shi, Siyu Wang, Yahong Han<br>论文链接：<a href="https://arxiv.org/abs/1904.01160">https://arxiv.org/abs/1904.01160</a></p><p>134、Effective Aesthetics Prediction with Multi-level Spatially Pooled Features<br>作者：Vlad Hosu, Bastian Goldlucke, Dietmar Saupe<br>论文链接：<a href="https://arxiv.org/abs/1904.01382">https://arxiv.org/abs/1904.01382</a></p><p>135、Context and Attribute Grounded Dense Captioning<br>作者：Guojun Yin, Lu Sheng, Bin Liu, Nenghai Yu, Xiaogang Wang, Jing Shao<br>论文链接：<a href="https://arxiv.org/abs/1904.01410">https://arxiv.org/abs/1904.01410</a></p><p>136、Good News, Everyone! Context driven entity-aware captioning for news images<br>作者：Ali Furkan Biten, Lluis Gomez, Marçal Rusiñol, Dimosthenis Karatzas<br>论文链接：<a href="https://arxiv.org/abs/1904.01475">https://arxiv.org/abs/1904.01475</a></p><p>137、Single Image Reflection Removal Exploiting Misaligned Training Data and Network Enhancements<br>作者：Kaixuan Wei, Jiaolong Yang, Ying Fu, David Wipf, Hua Huang<br>论文链接：<a href="https://arxiv.org/abs/1904.00637">https://arxiv.org/abs/1904.00637</a><br>代码链接：<a href="https://github.com/Vandermode/ERRNet">https://github.com/Vandermode/ERRNet</a></p><p>138、NM-Net: Mining Reliable Neighbors for Robust Feature Correspondences<br>作者：Chen Zhao, Zhiguo Cao, Chi Li, Xin Li, Jiaqi Yang<br>论文链接：<a href="https://arxiv.org/abs/1904.00320">https://arxiv.org/abs/1904.00320</a></p><p>139、Scene Graph Generation with External Knowledge and Image Reconstruction<br>作者：Jiuxiang Gu, Handong Zhao, Zhe Lin, Sheng Li, Jianfei Cai, Mingyang Ling<br>论文链接：<a href="https://arxiv.org/abs/1904.00560">https://arxiv.org/abs/1904.00560</a></p><p>140、Multi-source weak supervision for saliency detection<br>作者：Yu Zeng, Yunzhi Zhuge, Huchuan Lu, Lihe Zhang, Mingyang Qian, Yizhou Yu<br>论文链接：<a href="https://arxiv.org/abs/1904.00566">https://arxiv.org/abs/1904.00566</a></p><p>141、Single Image Reflection Removal Exploiting Misaligned Training Data and Network Enhancements<br>作者：Kaixuan Wei, Jiaolong Yang, Ying Fu, David Wipf, Hua Huang<br>论文链接：<a href="https://arxiv.org/abs/1904.00637">https://arxiv.org/abs/1904.00637</a><br>代码链接：<a href="https://github.com/Vandermode/ERRNet">https://github.com/Vandermode/ERRNet</a></p><p>142、Searching for A Robust Neural Architecture in Four GPU Hours<br>作者：Xuanyi Dong, Yi Yang<br>代码链接：<a href="https://github.com/D-X-Y/GDAS">https://github.com/D-X-Y/GDAS</a> (will be public soon)</p><p>143、Conditional Adversarial Generative Flow for Controllable Image Synthesis<br>作者：Rui Liu, Yu Liu, Xinyu Gong, Xiaogang Wang, Hongsheng Li<br>论文链接：<a href="https://arxiv.org/pdf/1904.01782.pdf">https://arxiv.org/pdf/1904.01782.pdf</a></p><p>144、SFNet: Learning Object-aware Semantic Correspondence(Oral)<br>作者：Junghyup Lee, Dohyung Kim, Jean Ponce, Bumsub Ham<br>论文链接：<a href="https://arxiv.org/pdf/1904.01810.pdf">https://arxiv.org/pdf/1904.01810.pdf</a></p><p>145、Learning Context Graph for Person Search<br>作者：Yichao Yan, Qiang Zhang, Bingbing Ni, Wendong Zhang, Minghao Xu, Xiaokang Yang<br>论文链接：<a href="https://arxiv.org/pdf/1904.01830.pdf">https://arxiv.org/pdf/1904.01830.pdf</a></p><p>146、Geometry-Aware Symmetric Domain Adaptation for Monocular Depth Estimation<br>作者：Shanshan Zhao, Huan Fu, Mingming Gong, Dacheng Tao<br>论文链接：<a href="https://arxiv.org/abs/1904.01870">https://arxiv.org/abs/1904.01870</a></p><p>147、CAM-Convs: Camera-Aware Multi-Scale Convolutions for Single-View Depth<br>作者：Jose M. Facil, Benjamin Ummenhofer, Huizhong Zhou, Luis Montesano, Thomas Brox, Javier Civera<br>论文链接：<a href="https://arxiv.org/abs/1904.02028">https://arxiv.org/abs/1904.02028</a><br>项目链接：<a href="https://webdiis.unizar.es/~jmfacil/camconvs/">https://webdiis.unizar.es/~jmfacil/camconvs/</a></p><p>148、Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning<br>作者：Tongtong Yuan, Weihong Deng, Jian Tang, Yinan Tang, Binghui Chen<br>论文链接：<a href="https://arxiv.org/abs/1904.02616">https://arxiv.org/abs/1904.02616</a></p><p>149、T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order Tensor<br>作者：Jean Kossaifi, Adrian Bulat, Georgios Tzimiropoulos, Maja Pantic<br>论文链接：<a href="https://arxiv.org/abs/1904.02698">https://arxiv.org/abs/1904.02698</a></p><p>150、Assessment of Faster R-CNN in Man-Machine collaborative search<br>作者：Arturo Deza, Amit Surana, Miguel P. Eckstein<br>论文链接：<a href="https://arxiv.org/abs/1904.02805">https://arxiv.org/abs/1904.02805</a></p><p>151、Semantic Attribute Matching Networks<br>作者：Seungryong Kim, Dongbo Min, Somi Jeong, Sunok Kim, Sangryul Jeon, Kwanghoon Sohn<br>论文链接：<a href="https://arxiv.org/abs/1904.02969">https://arxiv.org/abs/1904.02969</a></p><p>152、Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning<br>作者：Oleksiy Ostapenko, Tassilo Klein, Mihai Puscas, Patrick Jähnichen, Moin Nabi<br>论文链接：<a href="https://arxiv.org/abs/1904.03137">https://arxiv.org/abs/1904.03137</a></p><p>153、Unsupervised Image Matching and Object Discovery as Optimization<br>作者：Huy V. Vo, Francis Bach, Minsu Cho, Kai Han, Yann LeCun, Patrick Perez, Jean Ponce<br>论文链接：<a href="https://arxiv.org/abs/1904.03148">https://arxiv.org/abs/1904.03148</a></p><p>154、Calibration of Asynchronous Camera Networks for Object Reconstruction Tasks<br>作者：Amy Tabb, Henry Medeiros<br>论文链接：<a href="https://arxiv.org/abs/1903.06811">https://arxiv.org/abs/1903.06811</a></p><p>155、LP-3DCNN: Unveiling Local Phase in 3D Convolutional Neural Networks<br>作者：Sudhakar Kumawat, Shanmuganathan Raman<br>论文链接：<a href="https://arxiv.org/abs/1904.03498">https://arxiv.org/abs/1904.03498</a></p><p>156、A Variational Auto-Encoder Model for Stochastic Point Processes<br>作者：Nazanin Mehrasa, Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, Greg Mori<br>论文链接：<a href="https://arxiv.org/abs/1904.03273">https://arxiv.org/abs/1904.03273</a></p><p>157、2.5D Visual Sound（FAIR Oral)<br>作者：Ruohan Gao, Kristen Grauman<br>论文链接：<a href="https://arxiv.org/abs/1812.04204">https://arxiv.org/abs/1812.04204</a><br>项目链接：<a href="https://vision.cs.utexas.edu/projects/2.5D_visual_sound/">https://vision.cs.utexas.edu/projects/2.5D_visual_sound/</a><br>源码链接：<a href="https://github.com/facebookresearch/FAIR-Play">https://github.com/facebookresearch/FAIR-Play</a></p><p>158、DeepLight: Learning Illumination for Unconstrained Mobile Mixed Reality<br>作者：Chloe LeGendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent Charbonnel, Jay Busch, Paul Debevec<br>论文链接：<a href="https://arxiv.org/abs/1904.01175">https://arxiv.org/abs/1904.01175</a></p><p>159、What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment<br>作者：Paritosh Parmar, Brendan Tran Morris<br>论文链接：<a href="https://arxiv.org/abs/1904.04346">https://arxiv.org/abs/1904.04346</a></p><p>160、SoDeep: a Sorting Deep net to learn ranking loss surrogates<br>作者：Martin Engilberge, Louis Chevallier, Patrick Pérez, Matthieu Cord<br>论文链接：<a href="https://arxiv.org/abs/1904.04272">https://arxiv.org/abs/1904.04272</a></p><p>161、3D Local Features for Direct Pairwise Registration<br>作者：Haowen Deng, Tolga Birdal, Slobodan Ilic<br>论文链接：<a href="https://arxiv.org/abs/1904.04281">https://arxiv.org/abs/1904.04281</a></p><p>162、Neural Rerendering in the Wild(Oral)<br>作者：Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues Hoppe, Rohit Pandey, Noah Snavely, Ricardo Martin-Brualla<br>论文链接：<a href="https://arxiv.org/abs/1904.04290">https://arxiv.org/abs/1904.04290</a></p><p>163、End-to-end Projector Photometric Compensation<br>作者：Bingyao Huang, Haibin Ling<br>论文链接：<a href="https://arxiv.org/abs/1904.04335">https://arxiv.org/abs/1904.04335</a></p><p>164、Reliable and Efficient Image Cropping: A Grid Anchor based Approach<br>作者：Hui Zeng, Lida Li, Zisheng Cao, Lei Zhang<br>论文链接：<a href="https://arxiv.org/abs/1904.04441">https://arxiv.org/abs/1904.04441</a><br>代码链接：<a href="https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping">https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping</a></p><p>165、Graphonomy: Universal Human Parsing via Graph Transfer Learning<br>作者：Ke Gong, Yiming Gao, Xiaodan Liang, Xiaohui Shen, Meng Wang, Liang Lin<br>论文链接：<a href="https://arxiv.org/abs/1904.04536">https://arxiv.org/abs/1904.04536</a><br>源码链接：<a href="https://github.com/Gaoyiminggithub/Graphonomy">https://github.com/Gaoyiminggithub/Graphonomy</a></p><p>166、Deep Virtual Networks for Memory Efficient Inference of Multiple Tasks<br>作者：Eunwoo Kim, Chanho Ahn, Philip H.S. Torr, Songhwai Oh<br>论文链接：<a href="https://arxiv.org/abs/1904.04562">https://arxiv.org/abs/1904.04562</a></p><p>167、Holistic and Comprehensive Annotation of Clinically Significant Findings on Diverse CT Images: Learning from Radiology Reports and Label Ontology（Oral)<br>作者：Ke Yan, Yifan Peng, Veit Sandfort, Mohammadhadi Bagheri, Zhiyong Lu, Ronald M. Summers<br>论文链接：<a href="https://arxiv.org/abs/1904.04661">https://arxiv.org/abs/1904.04661</a></p><p>168、Domain-Symmetric Networks for Adversarial Domain Adaptation<br>作者：Yabin Zhang, Hui Tang, Kui Jia, Mingkui Tan<br>论文链接：<a href="https://arxiv.org/abs/1904.04663">https://arxiv.org/abs/1904.04663</a></p><p>169、Label Propagation for Deep Semi-supervised Learning<br>作者：Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Ondrej Chum<br>论文链接：<a href="https://arxiv.org/abs/1904.04717">https://arxiv.org/abs/1904.04717</a></p><p>170、Leveraging the Invariant Side of Generative Zero-Shot Learning<br>作者：Jingjing Li, Mengmeng Jin, Ke Lu, Zhengming Ding, Lei Zhu, Zi Huang<br>论文链接：<a href="https://arxiv.org/abs/1904.04092">https://arxiv.org/abs/1904.04092</a></p><p>171、Learning monocular depth estimation infusing traditional stereo knowledge<br>作者：Fabio Tosi, Filippo Aleotti, Matteo Poggi, Stefano Mattoccia<br>论文链接：<a href="https://arxiv.org/abs/1904.04144">https://arxiv.org/abs/1904.04144</a><br>代码链接：<a href="https://github.com/fabiotosi92/monoResMatch-Tensorflow">https://github.com/fabiotosi92/monoResMatch-Tensorflow</a></p><p>172、nsupervised learning of action classes with continuous temporal embedding<br>作者：Anna Kukleva, Hilde Kuehne, Fadime Sener, Juergen Gall<br>论文链接：<a href="https://arxiv.org/abs/1904.04189">https://arxiv.org/abs/1904.04189</a></p><p>173、GA-Net: Guided Aggregation Net for End-to-end Stereo Matching(Oral)<br>作者：Feihu Zhang, Victor Prisacariu, Ruigang Yang, Philip H.S. Torr<br>论文链接：<a href="https://arxiv.org/abs/1904.06587">https://arxiv.org/abs/1904.06587</a></p><p>174、LiveSketch: Query Perturbations for Guided Sketch-based Visual Search<br>作者：John Collomosse, Tu Bui, Hailin Jin<br>论文链接：<a href="https://arxiv.org/abs/1904.06611">https://arxiv.org/abs/1904.06611</a></p><p>175、Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning<br>论文链接：<a href="https://arxiv.org/abs/1904.06627">https://arxiv.org/abs/1904.06627</a><br>源码链接：<a href="https://github.com/MalongTech/research-ms-loss">https://github.com/MalongTech/research-ms-loss</a></p><p>176、ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging（Oral)<br>作者：Samarth Brahmbhatt, Cusuh Ham, Charles C. Kemp, James Hays<br>论文链接：<a href="https://arxiv.org/abs/1904.06830">https://arxiv.org/abs/1904.06830</a></p><p>177、Self-critical n-step Training for Image Captioning（图像生成）<br>作者：Junlong Gao, Shiqi Wang, Shanshe Wang, Siwei Ma, Wen Gao<br>论文链接：<a href="https://arxiv.org/abs/1904.06861">https://arxiv.org/abs/1904.06861</a></p><p>178、C3AE: Exploring the Limits of Compact Model for Age Estimation<br>作者：Chao Zhang, Shuaicheng Liu, Xun Xu, Ce Zhu<br>论文链接：<a href="https://arxiv.org/abs/1904.05059">https://arxiv.org/abs/1904.05059</a></p><p>179、DAVANet: Stereo Deblurring with View Aggregation（Oral)<br>作者：Shangchen Zhou, Jiawei Zhang, Wangmeng Zuo, Haozhe Xie, Jinshan Pan, Jimmy Ren<br>论文链接：<a href="https://arxiv.org/abs/1904.05065">https://arxiv.org/abs/1904.05065</a></p><p>180、Actor-Critic Instance Segmentation<br>作者：Kwang In Kim, Hyung Jin Chang<br>论文链接：<a href="https://arxiv.org/abs/1904.05126">https://arxiv.org/abs/1904.05126</a></p><p>181、Joint Manifold Diffusion for Combining Predictions on Decoupled Observations<br>作者：Kwang In Kim, Hyung Jin Chang<br>论文链接：<a href="https://arxiv.org/abs/1904.05159">https://arxiv.org/abs/1904.05159</a></p><p>182、Iterative Residual Refinement for Joint Optical Flow and Occlusion Estimation<br>作者：Junhwa Hur, Stefan Roth<br>论文链接：<a href="https://arxiv.org/abs/1904.05290">https://arxiv.org/abs/1904.05290</a></p><p>183、Spherical Regression: Learning Viewpoints, Surface Normals and 3D Rotations on n-Spheres<br>作者：Shuai Liao, Efstratios Gavves, Cees G. M. Snoek<br>论文链接：<a href="https://arxiv.org/abs/1904.05404">https://arxiv.org/abs/1904.05404</a></p><p>184、Sliced Wasserstein Generative Models<br>作者：Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, Luc Van Gool<br>论文链接：<a href="https://arxiv.org/abs/1904.05408">https://arxiv.org/abs/1904.05408</a><br>源码链接：<a href="https://github.com/musikisomorphie/swd">https://github.com/musikisomorphie/swd</a></p><p>185、Learning to Generate Synthetic Data via Compositing<br>作者：Shashank Tripathi, Siddhartha Chandra, Amit Agrawal, Ambrish Tyagi, James M. Rehg, Visesh Chari<br>论文链接：<a href="https://arxiv.org/abs/1904.05475">https://arxiv.org/abs/1904.05475</a></p><p>186、Mitigating Information Leakage in Image Representations: A Maximum Entropy Approach(Oral)<br>作者：Proteek Chandan Roy, Vishnu Naresh Boddeti<br>论文链接：<a href="https://arxiv.org/abs/1904.05514">https://arxiv.org/abs/1904.05514</a></p><p>187、Unified Visual-Semantic Embeddings: Bridging Vision and Language with Structured Meaning Representations<br>作者：Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun, Wei-Ying Ma<br>论文链接：<a href="https://arxiv.org/abs/1904.05521">https://arxiv.org/abs/1904.05521</a></p><p>188、Reasoning Visual Dialogs with Structural and Partial Observations(Oral)<br>作者：Zilong Zheng, Wenguan Wang, Siyuan Qi, Song-Chun Zhu<br>论文链接：<a href="https://arxiv.org/abs/1904.05548">https://arxiv.org/abs/1904.05548</a></p><p>189、C-MIL: Continuation Multiple Instance Learning for Weakly Supervised Object Detection<br>作者：Fang Wan, Chang Liu, Wei Ke, Xiangyang Ji, Jianbin Jiao, Qixiang Ye<br>论文链接：<a href="https://arxiv.org/abs/1904.05647">https://arxiv.org/abs/1904.05647</a></p><p>190、TAFE-Net: Task-Aware Feature Embeddings for Low Shot Learning<br>作者：Xin Wang, Fisher Yu, Ruth Wang, Trevor Darrell, Joseph E. Gonzalez<br>论文链接：<a href="https://arxiv.org/abs/1904.05967">https://arxiv.org/abs/1904.05967</a></p><p>191、Real-Time Dense Stereo Embedded in A UAV for Road Inspection<br>作者：Rui Fan, Jianhao Jiao, Jie Pan, Huaiyang Huang, Shaojie Shen, Ming Liu<br>论文链接：<a href="https://arxiv.org/abs/1904.06017">https://arxiv.org/abs/1904.06017</a></p><p>192、Unifying Heterogeneous Classifiers with Distillation<br>作者：Jayakorn Vongkulbhisal, Phongtharin Vinayavekhin, Marco Visentini-Scarzanella<br>论文链接：<a href="https://arxiv.org/abs/1904.06062">https://arxiv.org/abs/1904.06062</a></p><p>193、Learning joint reconstruction of hands and manipulated objects<br>作者：Yana Hasson, Gül Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael J. Black, Ivan Laptev, Cordelia Schmid<br>论文链接：<a href="https://arxiv.org/abs/1904.05767">https://arxiv.org/abs/1904.05767</a></p><p>194、Probabilistic Permutation Synchronization using the Riemannian Structure of the Birkhoff Polytope(Oral)<br>作者：Tolga Birdal, Umut Şimşekli<br>论文链接：<a href="https://arxiv.org/abs/1904.05814">https://arxiv.org/abs/1904.05814</a></p><p>195、Variational Information Distillation for Knowledge Transfer<br>作者：Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D. Lawrence, Zhenwen Dai<br>论文链接：<a href="https://arxiv.org/abs/1904.05835">https://arxiv.org/abs/1904.05835</a></p><p>196、Expressive Body Capture: 3D Hands, Face, and Body from a Single Image<br>作者：Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, Michael J. Black<br>论文链接：<a href="https://arxiv.org/abs/1904.05866">https://arxiv.org/abs/1904.05866</a></p><p>197、A Simple Baseline for Audio-Visual Scene-Aware Dialog<br>作者：Idan Schwartz, Alexander Schwing, Tamir Hazan<br>论文链接：<a href="https://arxiv.org/abs/1904.05876">https://arxiv.org/abs/1904.05876</a></p><p>198、Two Body Problem: Collaborative Visual Task Completion<br>作者：Unnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana Lazebnik, Ali Farhadi, Alexander Schwing, Aniruddha Kembhavi<br>论文链接：<a href="https://arxiv.org/abs/1904.05879">https://arxiv.org/abs/1904.05879</a></p><p>199、Factor Graph Attention<br>作者：Idan Schwartz, Seunghak Yu, Tamir Hazan, Alexander Schwing<br>论文链接：<a href="https://arxiv.org/abs/1904.05880">https://arxiv.org/abs/1904.05880</a></p><p>200、Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning<br>作者：Wenbin Li, Lei Wang, Jinglin Xu, Jing Huo, Yang Gao, Jiebo Luo<br>论文链接：<a href="https://cs.nju.edu.cn/rl/people/liwb/CVPR19.pdf">https://cs.nju.edu.cn/rl/people/liwb/CVPR19.pdf</a><br>源码链接：<a href="https://github.com/WenbinLee/DN4.git">https://github.com/WenbinLee/DN4.git</a></p><p>201、Generalized Zero- and Few-Shot Learning via Aligned Variational Autoencoders<br>作者：Edgar Schönfeld, Sayna Ebrahimi, Samarth Sinha, Trevor Darrell, Zeynep Akata<br>论文链接：<a href="https://arxiv.org/abs/1812.01784">https://arxiv.org/abs/1812.01784</a><br>源码链接：<a href="https://github.com/edgarschnfld/CADA-VAE-PyTorch">https://github.com/edgarschnfld/CADA-VAE-PyTorch</a></p><p>202、Learning Attraction Field Representation for Robust Line Segment Detection<br>作者：Nan Xue, Song Bai, Fudong Wang, Gui-Song Xia, Tianfu Wu, Liangpei Zhang<br>论文链接：<a href="https://arxiv.org/abs/1812.02122">https://arxiv.org/abs/1812.02122</a><br>源码链接：<a href="https://github.com/cherubicXN/afm_cvpr2019">https://github.com/cherubicXN/afm_cvpr2019</a></p><p>203、Live Reconstruction of Large-Scale Dynamic Outdoor Worlds<br>作者：Ondrej Miksik, Vibhav Vineet<br>论文链接：<a href="https://arxiv.org/abs/1903.06708">https://arxiv.org/abs/1903.06708</a></p><p>204、Automatic adaptation of object detectors to new domains using self-training<br>作者：Aruni RoyChowdhury, Prithvijit Chakrabarty, Ashish Singh, SouYoung Jin, Huaizu Jiang, Liangliang Cao, Erik Learned-Miller<br>论文链接：<a href="https://arxiv.org/abs/1904.07305">https://arxiv.org/abs/1904.07305</a></p><p>205、A Bayesian Perspective on the Deep Image Prior<br>作者：Zezhou Cheng, Matheus Gadelha, Subhransu Maji, Daniel Sheldon<br>论文链接：<a href="https://arxiv.org/abs/1904.07457">https://arxiv.org/abs/1904.07457</a><br>源码链接：<a href="https://github.com/ZezhouCheng/GP-DIP">https://github.com/ZezhouCheng/GP-DIP</a></p><p>206、Focus Is All You Need: Loss Functions For Event-based Vision<br>作者：Guillermo Gallego, Mathias Gehrig, Davide Scaramuzza<br>论文链接：<a href="https://arxiv.org/abs/1904.07235">https://arxiv.org/abs/1904.07235</a></p><p>207、Semantically Aligned Bias Reducing Zero Shot Learning<br>作者：Akanksha Paul, Narayanan C. Krishnan, Prateek Munjal<br>论文链接：<a href="https://arxiv.org/abs/1904.07659">https://arxiv.org/abs/1904.07659</a></p><p>208、ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples<br>作者：Xiaojun Jia, Xingxing Wei, Xiaochun Cao, Hassan Foroosh<br>论文链接：<a href="https://nlpr-web.ia.ac.cn/mmc/homepage/jygao/gct_cvpr2019.html">https://nlpr-web.ia.ac.cn/mmc/homepage/jygao/gct_cvpr2019.html</a><br>源码链接：<a href="https://github.com/jiaxiaojunQAQ/Comdefend">https://github.com/jiaxiaojunQAQ/Comdefend</a><br>论文解读：<a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid=2247488330&amp;idx=2&amp;sn=2a5758305811458572e19da2c12954bd&amp;chksm=ec1ffeb3db6877a5df4f484c8259c085c5a3abf1d6b9f05e65fefdf9c0c462effc3aa05f5a15&amp;token=644613083&amp;lang=zh_CN&amp;scene=21#wechat_redirect">CVPR 2019 | 图像压缩重建也能抵御对抗样本，这是一种新的防守策略</a></p><p>209、REPAIR: Removing Representation Bias by Dataset Resampling<br>作者：Yi Li, Nuno Vasconcelos<br>论文链接：<a href="https://arxiv.org/abs/1904.07911">https://arxiv.org/abs/1904.07911</a><br>源码链接：<a href="https://github.com/JerryYLi/Dataset-REPAIR/">https://github.com/JerryYLi/Dataset-REPAIR/</a></p><p>210、Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations(Oral）<br>作者：David Acuna, Amlan Kar, Sanja Fidler<br>论文链接：<a href="https://arxiv.org/abs/1904.07934">https://arxiv.org/abs/1904.07934</a><br>项目链接：<a href="https://nv-tlabs.github.io/STEAL/">https://nv-tlabs.github.io/STEAL/</a></p><p>211、Multi-Scale Geometric Consistency Guided Multi-View Stereo<br>作者：Qingshan Xu, Wenbing Tao<br>论文链接：<a href="https://arxiv.org/abs/1904.08103">https://arxiv.org/abs/1904.08103</a></p><p>212、DistanceNet: Estimating Traveled Distance from Monocular Images using a Recurrent Convolutional Neural Network<br>作者：Robin Kreuzig, Matthias Ochs, Rudolf Mester<br>论文链接：<a href="https://arxiv.org/abs/1904.08105">https://arxiv.org/abs/1904.08105</a></p><p>213、Guided Anisotropic Diffusion and Iterative Learning for Weakly Supervised Change Detection<br>作者：Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, Yann Gousseau<br>论文链接：<a href="https://arxiv.org/abs/1904.08208">https://arxiv.org/abs/1904.08208</a></p><p>214、LO-Net: Deep Real-time Lidar Odometry<br>作者：Qing Li, Shaoyang Chen, Cheng Wang, Xin Li, Chenglu Wen, Ming Cheng, Jonathan Li<br>论文链接：<a href="https://arxiv.org/abs/1904.08242">https://arxiv.org/abs/1904.08242</a></p><p>215、Events-to-Video: Bringing Modern Computer Vision to Event Cameras<br>作者：Henri Rebecq, René Ranftl, Vladlen Koltun, Davide Scaramuzza<br>论文链接：<a href="https://arxiv.org/abs/1904.08298">https://arxiv.org/abs/1904.08298</a></p><p>216、ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging(Oral)<br>作者： Samarth Brahmbhatt, Cusuh Ham, Charles C. Kemp, and James Hays<br>论文链接：<a href="https://contactdb.cc.gatech.edu/contactdb_paper.pdf">https://contactdb.cc.gatech.edu/contactdb_paper.pdf</a><br>源码链接：<a href="https://github.com/samarth-robo/contactdb_prediction">https://github.com/samarth-robo/contactdb_prediction</a></p><p>217、Variational Prototyping-Encoder: One-Shot Learning with Prototypical Images<br>作者：Junsik Kim, Tae-Hyun Oh, Seokju Lee, Fei Pan, In So Kweon<br>论文链接：<a href="https://arxiv.org/abs/1904.08482">https://arxiv.org/abs/1904.08482</a></p><p>218、Few-Shot Learning with Localization in Realistic Settings<br>作者：Davis Wertheimer, Bharath Hariharan<br>论文链接：<a href="https://arxiv.org/abs/1904.08502">https://arxiv.org/abs/1904.08502</a></p><p>219、Progressive Attention Memory Network for Movie Story Question Answering<br>作者：Junyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin Kim, Chang D. Yoo<br>论文链接：<a href="https://arxiv.org/abs/1904.08607">https://arxiv.org/abs/1904.08607</a></p><p>220、DDLSTM: Dual-Domain LSTM for Cross-Dataset Action Recognition<br>作者：Toby Perrett, Dima Damen<br>论文链接：<a href="https://arxiv.org/abs/1904.08634">https://arxiv.org/abs/1904.08634</a></p><p>221、A Theoretically Sound Upper Bound on the Triplet Loss for Improving the Efficiency of Deep Distance Metric Learning<br>作者：Thanh-Toan Do, Toan Tran, Ian Reid, Vijay Kumar, Tuan Hoang, Gustavo Carneiro<br>论文链接：<a href="https://arxiv.org/abs/1904.08720">https://arxiv.org/abs/1904.08720</a></p><p>222、4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks<br>作者：Christopher Choy, JunYoung Gwak, Silvio Savarese<br>论文链接：<a href="https://arxiv.org/abs/1904.08755">https://arxiv.org/abs/1904.08755</a></p><p>223、Attentive Single-Tasking of Multiple Tasks<br>作者：Kevis-Kokitsi Maninis, Ilija Radosavovic, Iasonas Kokkinos<br>论文链接：<a href="https://arxiv.org/abs/1904.08918">https://arxiv.org/abs/1904.08918</a></p><p>224、Listen to the Image<br>作者：Di Hu, Dong Wang, Xuelong Li, Feiping Nie, Qi Wang<br>论文链接：<a href="https://arxiv.org/abs/1904.09115">https://arxiv.org/abs/1904.09115</a></p><p>225、SelFlow: Self-Supervised Learning of Optical Flow<br>作者：Pengpeng Liu, Michael Lyu, Irwin King, Jia Xu<br>论文链接：<a href="https://arxiv.org/abs/1904.09117">https://arxiv.org/abs/1904.09117</a></p><p>226、Visualizing the decision-making process in deep neural decision forest<br>作者：Shichao Li, Kwang-Ting Cheng<br>论文链接：<a href="https://arxiv.org/abs/1904.09201">https://arxiv.org/abs/1904.09201</a><br>源码链接：<a href="https://github.com/Nicholasli1995/VisualizingNDF">https://github.com/Nicholasli1995/VisualizingNDF</a></p><p>227、Data-Driven Neuron Allocation for Scale Aggregation Networks<br>作者：Yi Li, Zhanghui Kuang, Yimin Chen, Wayne Zhang<br>论文链接：<a href="https://arxiv.org/abs/1904.09460">https://arxiv.org/abs/1904.09460</a></p><p>228、TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation<br>作者：Wayne Wu, Kaidi Cao, Cheng Li, Chen Qian, Chen Change Loy<br>论文链接：<a href="https://arxiv.org/abs/1904.09571">https://arxiv.org/abs/1904.09571</a></p><p>229、Deep Metric Learning Beyond Binary Supervision（Oral）<br>作者：Sungyeon Kim, Minkyo Seo, Ivan Laptev, Minsu Cho, Suha Kwak<br>论文链接：<a href="https://arxiv.org/abs/1904.09626">https://arxiv.org/abs/1904.09626</a></p><p>230、Superquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids<br>作者： Despoina Paschalidou, Ali Osman Ulusoy, Andreas Geiger<br>论文链接：<a href="https://arxiv.org/abs/1904.09970">https://arxiv.org/abs/1904.09970</a><br>源码链接：<a href="https://github.com/paschalidoud/superquadric_parsing">https://github.com/paschalidoud/superquadric_parsing</a></p><p>231、Unsupervised Person Image Generation with Semantic Parsing Transformation<br>作者：Sijie Song, Wei Zhang, Jiaying Liu, Tao Mei<br>论文链接：<a href="https://arxiv.org/abs/1904.03379">https://arxiv.org/abs/1904.03379</a><br>项目链接：<a href="https://github.com/SijieSong/person_generation_spt">https://github.com/SijieSong/person_generation_spt</a></p><p>232、Multi-Agent Tensor Fusion for Contextual Trajectory Prediction<br>作者：Tianyang Zhao, Yifei Xu, Mathew Monfort, Wongun Choi, Chris Baker, Yibiao Zhao, Yizhou Wang, Ying Nian Wu<br>论文链接：<a href="https://arxiv.org/abs/1904.04776">https://arxiv.org/abs/1904.04776</a></p><p>233、Learning Actor Relation Graphs for Group Activity Recognition<br>作者：Jianchao Wu, Limin Wang, Li Wang, Jie Guo, Gangshan Wu<br>论文链接：<a href="https://arxiv.org/abs/1904.10117">https://arxiv.org/abs/1904.10117</a></p><p>234、Student Becoming the Master: Knowledge Amalgamation for Joint Scene Parsing, Depth Estimation, and More<br>作者：Jingwen Ye, Yixin Ji, Xinchao Wang, Kairi Ou, Dapeng Tao, Mingli Song<br>论文链接：<a href="https://arxiv.org/abs/1904.10167">https://arxiv.org/abs/1904.10167</a></p><p>235、Attention-guided Network for Ghost-free High Dynamic Range Imaging<br>作者：Qingsen Yan, Dong Gong, Qinfeng Shi, Anton van den Hengel, Chunhua Shen, Ian Reid, Yanning Zhang<br>论文链接：<a href="https://arxiv.org/abs/1904.10293">https://arxiv.org/abs/1904.10293</a></p><p>236、Sim-Real Joint Reinforcement Transfer for 3D Indoor Navigation<br>作者：Fengda Zhu, Linchao Zhu, Yi Yang<br>论文链接：<a href="https://arxiv.org/abs/1904.03895">https://arxiv.org/abs/1904.03895</a></p><p>237、Detailed Human Shape Estimation from a Single Image by Hierarchical Mesh Deformation<br>作者：Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao, Ruigang Yang<br>论文链接：<a href="https://arxiv.org/abs/1904.10506v1">https://arxiv.org/abs/1904.10506v1</a><br>源码链接：<a href="https://github.com/zhuhao-nju/hmd.git">https://github.com/zhuhao-nju/hmd.git</a></p><p>238、Universal Domain Adaptation<br>作者：Kaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael . Jordan<br>论文链接：<a href="https://youkaichao.github.io/files/cvpr2019/1628.pdf">https://youkaichao.github.io/files/cvpr2019/1628.pdf</a><br>源码链接：<a href="https://github.com/thuml/Universal-Domain-Adaptation">https://github.com/thuml/Universal-Domain-Adaptation</a></p><p>239、STGAN: A Unified Selective Transfer Network for Arbitrary Image Attribute Editing<br>作者：Ming Liu, Yukang Ding, Min Xia, Xiao Liu, Errui Ding, Wangmeng Zuo, Shilei Wen<br>论文链接：<a href="https://arxiv.org/abs/1904.09709">https://arxiv.org/abs/1904.09709</a><br>源码链接：<a href="https://github.com/csmliu/STGAN">https://github.com/csmliu/STGAN</a></p><p>240、DeepCaps: Going Deeper with Capsule Networks<br>作者：Jathushan Rajasegaran, Vinoj Jayasundara, Sandaru Jayasekara, Hirunima Jayasekara, Suranga Seneviratne, Ranga Rodrigo<br>论文链接：<a href="https://arxiv.org/abs/1904.09546">https://arxiv.org/abs/1904.09546</a><br>源码链接：<a href="https://github.com/brjathu/deepcaps">https://github.com/brjathu/deepcaps</a></p><p>241、Representation Similarity Analysis for Efficient Task taxonomy &amp; Transfer Learning<br>作者：Kshitij Dwivedi, Gemma Roig<br>论文链接：<a href="https://arxiv.org/abs/1904.11740">https://arxiv.org/abs/1904.11740</a><br>源码链接：<a href="https://github.com/kshitijd20/RSA-CVPR19-release">https://github.com/kshitijd20/RSA-CVPR19-release</a></p><p>242、Improving Few-Shot User-Specific Gaze Adaptation via Gaze Redirection Synthesis<br>作者：Yu Yu, Gang Liu, Jean-Marc Odobez<br>论文链接：<a href="https://arxiv.org/abs/1904.10638">https://arxiv.org/abs/1904.10638</a></p><p>243、Transferrable Prototypical Networks for Unsupervised Domain Adaptation（Oral）<br>作者：Yingwei Pan, Ting Yao, Yehao Li, Yu Wang, Chong-Wah Ngo, Tao Mei<br>论文链接：<a href="https://arxiv.org/abs/1904.11227">https://arxiv.org/abs/1904.11227</a></p><p>244、Exploring Object Relation in Mean Teacher for Cross-Domain Detection<br>作者：Qi Cai, Yingwei Pan, Chong-Wah Ngo, Xinmei Tian, Lingyu Duan, Ting Yao<br>论文链接：<a href="https://arxiv.org/abs/1904.11245">https://arxiv.org/abs/1904.11245</a></p><p>245、Pointing Novel Objects in Image Captioning<br>作者：Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, Tao Mei<br>论文链接：<a href="https://arxiv.org/abs/1904.11251">https://arxiv.org/abs/1904.11251</a></p><p>246、Style Transfer by Relaxed Optimal Transport and Self-Similarity<br>作者：Nicholas Kolkin, Jason Salavon, Gregory Shakhnarovich<br>论文链接：<a href="https://arxiv.org/pdf/1904.12785.pdf">https://arxiv.org/pdf/1904.12785.pdf</a><br>源码链接：<a href="https://github.com/nkolkin13/STROTSS">https://github.com/nkolkin13/STROTSS</a></p><p>247、Decoders Matter for Semantic Segmentation:Data-Dependent Decoding Enables Flexible Feature Aggregation<br>作者：Zhi Tian, Tong He, Chunhua Shen, Youliang Yan<br>论文链接：<a href="https://arxiv.org/abs/1903.02120">https://arxiv.org/abs/1903.02120</a></p><p>248、Improving Transferability of Adversarial Examples with Input Diversity<br>作者：Cihang Xie; Yuyin Zhou; Song Bai; Zhishuai Zhang; Jianyu Wang; Zhou Ren; Alan Yuille<br>论文链接：<a href="https://arxiv.org/abs/1803.06978">https://arxiv.org/abs/1803.06978</a><br>源码链接：<a href="https://github.com/cihangxie/DI-2-FGSM">https://github.com/cihangxie/DI-2-FGSM</a></p><p>249、Contrastive Adaptation Network for Unsupervised Domain Adaptation<br>作者：Guoliang Kang, Lu Jiang, Yi Yang, Alexander G Hauptmann<br>论文链接：<a href="https://arxiv.org/pdf/1901.00976.pdf">https://arxiv.org/pdf/1901.00976.pdf</a></p><p>250、Weakly Supervised Open-set Domain Adaptation by Dual-domain Collaboration<br>作者：Shuhan Tan, Jiening Jiao, Wei-Shi Zheng<br>论文链接：<a href="https://arxiv.org/abs/1904.13179">https://arxiv.org/abs/1904.13179</a></p><p>251、Self-Supervised Convolutional Subspace Clustering Network<br>作者：Junjian Zhang, Chun-Guang Li, Chong You, Xianbiao Qi, Honggang Zhang, Jun Guo, Zhouchen Lin<br>论文链接：<a href="https://arxiv.org/abs/1905.00149">https://arxiv.org/abs/1905.00149</a></p><p>252、AdaCos: Adaptively Scaling Cosine Logits for Effectively Learning Deep Face Representations<br>作者：Xiao Zhang, Rui Zhao, Yu Qiao, Xiaogang Wang, Hongsheng Li<br>论文链接：<a href="https://arxiv.org/abs/1905.00292">https://arxiv.org/abs/1905.00292</a></p><p>253、Pushing the Boundaries of View Extrapolation with Multiplane Images<br>作者：Pratul P. Srinivasan, Richard Tucker, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng, Noah Snavely<br>论文链接：<a href="https://arxiv.org/abs/1905.00413">https://arxiv.org/abs/1905.00413</a></p><p>254、Lifting Vectorial Variational Problems: A Natural Formulation based on Geometric Measure Theory and Discrete Exterior Calculus<br>作者：Thomas Möllenhoff, Daniel Cremers<br>论文链接：<a href="https://arxiv.org/abs/1905.00851">https://arxiv.org/abs/1905.00851</a></p><p>255、Learning Cross-Modal Embeddings with Adversarial Networks for Cooking Recipes and Food Images<br>作者：Hao Wang, Doyen Sahoo, Chenghao Liu, Ee-peng Lim, Steven C. H. Hoi<br>论文链接：<a href="https://arxiv.org/abs/1905.01273">https://arxiv.org/abs/1905.01273</a></p><p>256、Dissecting Person Re-identification from the Viewpoint of Viewpoint<br>作者：Ting Zhao, Xiangqian Wu<br>论文链接：<a href="https://arxiv.org/abs/1812.02162">https://arxiv.org/abs/1812.02162</a><br>源码链接：<a href="https://github.com/sxzrt/Dissecting-Person-Re-ID-from-the-Viewpoint-of-Viewpoint">https://github.com/sxzrt/Dissecting-Person-Re-ID-from-the-Viewpoint-of-Viewpoint</a></p><p>257、Meta-Transfer Learning for Few-Shot Learning<br>作者：Qianru Sun, Yaoyao Liu, Tat-Seng Chua, Bernt Schiele<br>论文链接：<a href="https://arxiv.org/pdf/1812.02391.pdf">https://arxiv.org/pdf/1812.02391.pdf</a><br>源码链接：<a href="https://github.com/y2l/meta-transfer-learning-tensorflow">https://github.com/y2l/meta-transfer-learning-tensorflow</a></p><p>258、Generating Classification Weights with GNN Denoising Autoencoders for Few-Shot Learning<br>作者：Spyros Gidaris, Nikos Komodakis<br>论文链接：<a href="https://arxiv.org/abs/1905.01102">https://arxiv.org/abs/1905.01102</a><br>源码链接：<a href="https://github.com/gidariss/wDAE_GNN_FewShot">https://github.com/gidariss/wDAE_GNN_FewShot</a></p><p>259、Query-guided End-to-End Person Search<br>作者：Bharti Munjal, Sikandar Amin, Federico Tombari, Fabio Galasso<br>论文链接：<a href="https://arxiv.org/abs/1905.01203">https://arxiv.org/abs/1905.01203</a></p><p>260、Edge-labeling Graph Neural Network for Few-shot Learning<br>作者：Jongmin Kim, Taesup Kim, Sungwoong Kim, Chang D. Yoo<br>论文链接：<a href="https://arxiv.org/abs/1905.01436">https://arxiv.org/abs/1905.01436</a></p><p>261、Leveraging Crowdsourced GPS Data for Road Extraction from Aerial Imagery<br>作者：Tao Sun, Zonglin Di, Pengyu Che, Chun Liu, Yin Wang<br>论文链接：<a href="https://arxiv.org/abs/1905.01447">https://arxiv.org/abs/1905.01447</a></p><p>262、Towards Instance-level Image-to-Image Translation<br>作者：Zhiqiang Shen, Mingyang Huang, Jianping Shi, Xiangyang Xue, Thomas Huang<br>论文链接：<a href="https://arxiv.org/abs/1905.01744">https://arxiv.org/abs/1905.01744</a><br>项目链接：<a href="https://zhiqiangshen.com/projects/INIT/index.html">https://zhiqiangshen.com/projects/INIT/index.html</a></p><p>263、P2SGrad: Refined Gradients for Optimizing Deep Face Models<br>作者：Xiao Zhang, Rui Zhao, Junjie Yan, Mengya Gao, Yu Qiao, Xiaogang Wang, Hongsheng Li<br>论文链接：<a href="https://arxiv.org/abs/1905.02479">https://arxiv.org/abs/1905.02479</a></p><p>264、Capture, Learning, and Synthesis of 3D Speaking Styles<br>作者：Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ranjan, Michael J. Black<br>论文链接：<a href="https://arxiv.org/abs/1905.03079">https://arxiv.org/abs/1905.03079</a></p><p>265、Convolutional Mesh Regression for Single-Image Human Shape Reconstruction<br>作者：Nikos Kolotouros, Georgios Pavlakos, Kostas Daniilidis<br>论文链接：<a href="https://arxiv.org/abs/1905.03244">https://arxiv.org/abs/1905.03244</a><br>项目链接：<a href="https://www.seas.upenn.edu/~nkolot/projects/cmr/">https://www.seas.upenn.edu/~nkolot/projects/cmr/</a></p><p>266、Learning Loss for Active Learning<br>作者：Donggeun Yoo, In So Kweon<br>论文链接：<a href="https://arxiv.org/abs/1905.03677">https://arxiv.org/abs/1905.03677</a></p><p>267、Deep Sky Modeling for Single Image Outdoor Lighting Estimation<br>作者：Yannick Hold-Geoffroy, Akshaya Athawale, Jean-François Lalonde<br>论文链接：<a href="https://arxiv.org/abs/1905.03897">https://arxiv.org/abs/1905.03897</a></p><p>268、Exact Adversarial Attack to Image Captioning via Structured Output Learning with Latent Variables<br>作者：Yan Xu, Baoyuan Wu, Fumin Shen, Yanbo Fan, Yong Zhang, Heng Tao Shen, Wei Liu<br>论文链接：<a href="https://arxiv.org/abs/1905.04016">https://arxiv.org/abs/1905.04016</a></p><p>269、Domain-Aware Generalized Zero-Shot Learning<br>作者：Yuval Atzmon, Gal Chechik<br>论文链接：<a href="https://arxiv.org/pdf/1812.09903.pdf">https://arxiv.org/pdf/1812.09903.pdf</a></p><p>270、Hybrid-Attention based Decoupled Metric Learning for Zero-Shot Image Retrieval<br>作者：Binghui Chen, Weihong Deng<br>论文链接：<a href="https://www.bhchen.cn/paper/cvpr19.pdf">https://www.bhchen.cn/paper/cvpr19.pdf</a><br>源码链接：<a href="https://github.com/chenbinghui1/Hybrid-Attention-based-Decoupled-Metric-Learning">https://github.com/chenbinghui1/Hybrid-Attention-based-Decoupled-Metric-Learning</a></p><p>271、Generative Dual Adversarial Network for Generalized Zero-shot Learning<br>作者：He Huang, Changhu Wang, Philip S. Yu, Chang-Dong Wang<br>论文链接：<a href="https://arxiv.org/abs/1811.04857">https://arxiv.org/abs/1811.04857</a><br>源码链接：<a href="https://github.com/stevehuanghe/GDAN">https://github.com/stevehuanghe/GDAN</a></p><p>281、Learning to Fuse Things and Stuff<br>作者：Jie Li, Allan Raventos, Arjun Bhargava, Takaaki Tagawa, Adrien Gaidon<br>论文链接：<a href="https://arxiv.org/pdf/1812.01192.pdf">https://arxiv.org/pdf/1812.01192.pdf</a></p><p>282、On Finding Gray Pixels<br>作者：Yanlin Qian, Joni-Kristian Kämäräinen, Jarno Nikkanen, Jiri Matas<br>论文链接：<a href="https://arxiv.org/abs/1901.03198">https://arxiv.org/abs/1901.03198</a><br>源码链接：<a href="https://github.com/yanlinqian/Grayness-Index">https://github.com/yanlinqian/Grayness-Index</a> （matlab）</p><p>283、Wide-Context Semantic Image Extrapolation<br>作者：Yi Wang, Xin Tao, Xiaoyong Shen, Jiaya Jia<br>论文链接：<a href="https://jiaya.me/papers/imgextrapolation_cvpr19.pdf">https://jiaya.me/papers/imgextrapolation_cvpr19.pdf</a></p><p>284、Polarimetric Camera Calibration Using an LCD Monitor<br>作者：Zhixiang Wang, Yinqiang Zheng, Yung-Yu Chuang<br>论文链接：<a href="https://www.csie.ntu.edu.tw/~cyy/publications/papers/Wang2019PCC.pdf">https://www.csie.ntu.edu.tw/~cyy/publications/papers/Wang2019PCC.pdf</a></p><p>285、SiCloPe: Silhouette-Based Clothed People<br>作者：Ryota Natsume, Shunsuke Saito, Zeng Huang, Weikai Chen, Chongyang Ma, Hao Li, Shigeo Morishima<br>论文链接：<a href="https://arxiv.org/abs/1901.00049">https://arxiv.org/abs/1901.00049</a></p><p>286、From Coarse to Fine: Robust Hierarchical Localization at Large Scale<br>作者：Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, Marcin Dymczyk<br>论文链接：<a href="https://arxiv.org/abs/1812.03506">https://arxiv.org/abs/1812.03506</a></p><p>287、Label Efficient Semi-Supervised Learning via Graph Filtering<br>作者：Qimai Li，Xiao-Ming Wu，Han Liu，Xiaotong Zhang<br>论文链接：<a href="https://arxiv.org/pdf/1901.09993.pdf">https://arxiv.org/pdf/1901.09993.pdf</a></p><p>288、Learning Video Representations from Correspondence Proposals(Oral)<br>作者：Xingyu Liu, Joon-Young Lee, Hailin Jin<br>论文链接：<a href="https://arxiv.org/abs/1905.07853">https://arxiv.org/abs/1905.07853</a></p><p>289、Side Window Filtering(Oral)<br>作者：Hui Yin, Yuanhao Gong, Guoping Qiu<br>论文链接：<a href="https://arxiv.org/abs/1905.07177">https://arxiv.org/abs/1905.07177</a><br>源码链接：<a href="https://github.com/YuanhaoGong/SideWindowFilter">https://github.com/YuanhaoGong/SideWindowFilter</a></p><p>290、Spectral Metric for Dataset Complexity Assessment<br>作者：Frederic Branchaud-Charron, Andrew Achkar, Pierre-Marc Jodoin<br>论文链接：<a href="https://arxiv.org/abs/1905.07299">https://arxiv.org/abs/1905.07299</a></p><p>291、Disentangling Adversarial Robustness and Generalization<br>作者：David Stutz, Matthias Hein, Bernt Schiele<br>论文链接：<a href="https://arxiv.org/abs/1812.00740">https://arxiv.org/abs/1812.00740</a></p><p>292、Kernel Transformer Networks for Compact Spherical Convolution(Facebook)<br>作者：Yu-Chuan Su, Kristen Grauman<br>论文链接：<a href="https://research.fb.com/wp-content/uploads/2019/05/Kernel-Transformer-Networks-for-Compact-Spherical-Convolution.pdf">https://research.fb.com/wp-content/uploads/2019/05/Kernel-Transformer-Networks-for-Compact-Spherical-Convolution.pdf</a></p><p>293、Learning 3D Human Dynamics from Video<br>作者：Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, Jitendra Malik<br>论文链接：<a href="https://arxiv.org/abs/1812.01601">https://arxiv.org/abs/1812.01601</a><br>项目链接：<a href="https://akanazawa.github.io/human_dynamics/">https://akanazawa.github.io/human_dynamics/</a><br>源码链接：<a href="https://github.com/akanazawa/human_dynamics">https://github.com/akanazawa/human_dynamics</a></p><p>294、Bridging Stereo Matching and Optical Flow via Spatiotemporal Correspondence<br>作者：Hsueh-Ying Lai, Yi-Hsuan Tsai, Wei-Chen Chiu<br>论文链接：<a href="https://arxiv.org/abs/1905.09265">https://arxiv.org/abs/1905.09265</a><br>源码链接：<a href="https://github.com/lelimite4444/BridgeDepthFlow">https://github.com/lelimite4444/BridgeDepthFlow</a></p><p>295、Multi-Level Context Ultra-Aggregation for Stereo Matching（南开大学）<br>作者：Guang-Yu Nie, Ming-Ming Cheng, Yun Liu, Zhengfa Liang<br>论文链接：<a href="https://dpfan.net/wp-content/uploads/09CVPR_StereoMatching_CameraReady_V12_final.pdf">https://dpfan.net/wp-content/uploads/09CVPR_StereoMatching_CameraReady_V12_final.pdf</a><br>项目链接：<a href="https://mmcheng.net/zh/mcua/">https://mmcheng.net/zh/mcua/</a></p><p>296、DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation<br>作者：Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove<br>论文链接：<a href="https://arxiv.org/abs/1901.05103">https://arxiv.org/abs/1901.05103</a><br>源码链接：<a href="https://github.com/Oktosha/DeepSDF-explained">https://github.com/Oktosha/DeepSDF-explained</a></p><p>297、Learning Implicit Fields for Generative Shape Modeling<br>作者：Zhiqin Chen, Hao Zhang<br>论文链接：<a href="https://arxiv.org/abs/1812.02822">https://arxiv.org/abs/1812.02822</a><br>源码链接：<a href="https://github.com/czq142857/implicit-decoder">https://github.com/czq142857/implicit-decoder</a></p><p>298、ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware<br>作者：Han Cai, Ligeng Zhu, Song Han<br>论文链接：<a href="https://arxiv.org/abs/1812.00332">https://arxiv.org/abs/1812.00332</a><br>源码链接：<a href="https://github.com/MIT-HAN-LAB/ProxylessNAS">https://github.com/MIT-HAN-LAB/ProxylessNAS</a></p><p>299、Detect-to-Retrieve: Efficient Regional Aggregation for Image Search<br>作者：Marvin Teichmann, Andre Araujo, Menglong Zhu, Jack Sim<br>论文链接：<a href="https://arxiv.org/abs/1812.01584">https://arxiv.org/abs/1812.01584</a><br>源码链接：<a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></p><p>300、Speech2Face: Learning the Face Behind a Voice<br>作者：Tae-Hyun Oh, Tali Dekel, Changil Kim, Inbar Mosseri, William T. Freeman, Michael Rubinstein, Wojciech Matusik<br>论文链接：<a href="https://arxiv.org/abs/1905.09773">https://arxiv.org/abs/1905.09773</a><br>项目链接：<a href="https://speech2face.github.io/">https://speech2face.github.io/</a></p><p>301、Detecting Overfitting of Deep Generators via Latent Recovery</p><p>作者：Ryan Webster, Julien Rabin, Loic Simon, Frederic Jurie<br>论文链接：<a href="https://arxiv.org/pdf/1901.03396v1.pdf">https://arxiv.org/pdf/1901.03396v1.pdf</a><br>源码链接：<a href="https://github.com/ryanwebster90/gen-overfitting-latent-recovery">https://github.com/ryanwebster90/gen-overfitting-latent-recovery</a></p><p>302、Unsupervised Deep Epipolar Flow for Stationary or Dynamic Scenes</p><p>用于静止或动态场景的无监督深度极线流</p><p>作者：Yiran Zhong, Pan Ji, Jianyuan Wang, Yuchao Dai, Hongdong Li<br>论文链接：<a href="https://arxiv.org/pdf/1904.03848v1.pdf">https://arxiv.org/pdf/1904.03848v1.pdf</a><br>源码链接：<a href="https://github.com/yiranzhong/EPIflow">https://github.com/yiranzhong/EPIflow</a></p><p>303、Isospectralization, or how to hear shape, style, and correspondence</p><p>Isospectization，或如何听取形状，风格和通信</p><p>作者：Luca Cosmo, Mikhail Panine, Arianna Rampini, Maks Ovsjanikov, Michael M. Bronstein, Emanuele Rodolà<br>论文链接：<a href="https://arxiv.org/abs/1811.11465v2">https://arxiv.org/abs/1811.11465v2</a><br>源码链接：<a href="https://github.com/lcosmo/isospectralization">https://github.com/lcosmo/isospectralization</a></p><p>304、From Recognition to Cognition: Visual Commonsense Reasoning(Oral)</p><p>从认知到认知：视觉常识推理</p><p>作者：Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi<br>论文链接：<a href="https://arxiv.org/pdf/1811.10830v2.pdf">https://arxiv.org/pdf/1811.10830v2.pdf</a><br>源码链接：<a href="https://github.com/rowanz/r2c">https://github.com/rowanz/r2c</a></p><p>305、Unsupervised Visual Domain Adaptation: A Deep Max-Margin Gaussian Process Approach（Oral)</p><p>Unsupervised Visual Domain Adaptation：深度最大边缘高斯过程方法</p><p>作者：Minyoung Kim, Pritish Sahu, Behnam Gholami, Vladimir Pavlovic<br>论文链接：<a href="https://arxiv.org/pdf/1902.08727.pdf">https://arxiv.org/pdf/1902.08727.pdf</a><br>源码链接：<a href="https://github.com/seqam-lab/GPDA">https://github.com/seqam-lab/GPDA</a></p><p>306、Deformable ConvNets v2: More Deformable, Better Results</p><p>Deformable ConvNets v2：更加可变形，更好的结果</p><p>作者：Xizhou Zhu, Han Hu, Stephen Lin, Jifeng Dai<br>论文链接：<a href="https://arxiv.org/pdf/1811.11168v2.pdf">https://arxiv.org/pdf/1811.11168v2.pdf</a><br>源码链接：<a href="https://github.com/msracver/Deformable-ConvNets">https://github.com/msracver/Deformable-ConvNets</a></p>]]></content>
      
      
      <categories>
          
          <category> 资料汇总 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR </tag>
            
            <tag> 2019 </tag>
            
            <tag> 资料 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Markdown】入门笔记</title>
      <link href="2020/11/08/markdown-ru-men-bi-ji/"/>
      <url>2020/11/08/markdown-ru-men-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="Markdown快速入门-typora"><a href="#Markdown快速入门-typora" class="headerlink" title="Markdown快速入门(typora)"></a>Markdown快速入门(typora)</h1><h3 id="1-代码块"><a href="#1-代码块" class="headerlink" title="1.代码块"></a>1.代码块</h3><pre class=" language-lang-java"><code class="language-lang-java">//代码块语法：英文状态下~~~加语言名，例：~~~java</code></pre><h3 id="2-标题"><a href="#2-标题" class="headerlink" title="2.标题"></a>2.标题</h3><pre class=" language-lang-shell"><code class="language-lang-shell">//标题语法# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题</code></pre><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><h6 id="六级标题"><a href="#六级标题" class="headerlink" title="六级标题"></a>六级标题</h6><h3 id="3-字体"><a href="#3-字体" class="headerlink" title="3.字体"></a>3.字体</h3><pre class=" language-lang-java"><code class="language-lang-java">//加粗 **得不到你**//删除线~~等到天黑~~//斜体*数值计算*</code></pre><p><strong>得不到你</strong></p><p><del>等到天黑</del><br><em>数值计算</em></p><h3 id="4-引用"><a href="#4-引用" class="headerlink" title="4.引用"></a>4.引用</h3><pre class=" language-lang-java"><code class="language-lang-java">//引用语法>作者：宋明望>>作者：宋明望>>>作者：宋明望</code></pre><blockquote><p>作者：宋明望</p><blockquote><p>作者：宋明望</p><blockquote><p>作者：宋明望</p></blockquote></blockquote></blockquote><h3 id="5-分割线"><a href="#5-分割线" class="headerlink" title="5.分割线"></a>5.分割线</h3><pre class=" language-lang-java"><code class="language-lang-java">//分割线，两种方法---***</code></pre><hr><hr><h3 id="6-图片插入"><a href="#6-图片插入" class="headerlink" title="6.图片插入"></a>6.图片插入</h3><pre class=" language-lang-java"><code class="language-lang-java">//在线/本地图片插图![图片名字](图片路径)</code></pre><p><img src="D:\文件\图片\ 紫霞99.jpeg" alt="我的图片"></p><p><img src="https://pic1.zhimg.com/80/v2-73823d0c5d826d034937dbeabae9b6df_720w.jpg?source=1940ef5c" alt="图片"></p><h3 id="7-超链接"><a href="#7-超链接" class="headerlink" title="7.超链接"></a>7.超链接</h3><pre class=" language-lang-java"><code class="language-lang-java">//超链接语法[超链接名字](超链接地址)在typora中不能跳转，写在博客中后点击就可直接跳转</code></pre><p><a href="https://www.bilibili.com/">B站</a></p><h3 id="8-列表"><a href="#8-列表" class="headerlink" title="8.列表"></a>8.列表</h3><pre class=" language-lang-java"><code class="language-lang-java">//无序列表- 目录1(有空格)- 目录2- 目录3//有序列表  数字 + . + 名称</code></pre><ul><li>目录1(有空格)</li><li>目录2</li><li>目录3</li></ul><ol><li><p>目录1</p></li><li><p>目录</p></li><li>目录</li><li>嘿嘿</li></ol><h3 id="9-表格"><a href="#9-表格" class="headerlink" title="9.表格"></a>9.表格</h3><div class="table-container"><table><thead><tr><th style="text-align:center">成绩</th><th style="text-align:center">语文</th><th style="text-align:center">数学</th></tr></thead><tbody><tr><td style="text-align:center">A同学</td><td style="text-align:center">23</td><td style="text-align:center">43</td></tr><tr><td style="text-align:center">B同学</td><td style="text-align:center">54</td><td style="text-align:center">76</td></tr><tr><td style="text-align:center">C同学</td><td style="text-align:center">45</td><td style="text-align:center">98</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blog </tag>
            
            <tag> MarkDown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Hexo博客】Hexo Command Line</title>
      <link href="2020/11/01/hexo-bo-ke-hexo-command-line/"/>
      <url>2020/11/01/hexo-bo-ke-hexo-command-line/</url>
      
        <content type="html"><![CDATA[<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post（创建新的文件）"><a href="#Create-a-new-post（创建新的文件）" class="headerlink" title="Create a new post（创建新的文件）"></a>Create a new post（创建新的文件）</h3><pre class=" language-lang-bash"><code class="language-lang-bash">方式一：普通创建方式，layout可不设置，Hexo 会创建一个以标题为名字的目录，并在目录中放置一个 index.md 文件       Hexo 有三种默认布局：post、page 和 draft。在创建者三种不同类型的文件时，它们将会被保存到不同的路径；           而您自定义的其他布局和 post 相同，都将储存到 source/_posts 文件夹。$ hexo new [layout] "My New Post"方式二：会创建一个 source/about/me.md 文件，同时 Front Matter 中的 title 为 "About me"$ hexo new page --path about/me "About me"参数描述：    -p, --path    自定义新文章的路径    -r, --replace    如果存在同名文章，将其替换    -s, --slug    文章的 Slug，作为新文章的文件名和发布后的 URL</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server（启动服务器）"><a href="#Run-server（启动服务器）" class="headerlink" title="Run server（启动服务器）"></a>Run server（启动服务器）</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo server参数描述：    -p, --port    重设端口    -s, --static    只使用静态文件    -l, --log    启动日记记录，使用覆盖记录格式</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files（生成静态文件）"><a href="#Generate-static-files（生成静态文件）" class="headerlink" title="Generate static files（生成静态文件）"></a>Generate static files（生成静态文件）</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo generate参数描述：    -d, --deploy    文件生成后立即部署网站    -w, --watch    监视文件变动    -b, --bail    生成过程中如果发生任何未处理的异常则抛出异常    -f, --force    强制重新生成文件，Hexo 引入了差分机制，如果 public 目录存在，那么 hexo g 只会重新生成改动的文件。使用该参数的效果接近 hexo clean && hexo generate    -c, --concurrency    最大同时生成文件的数量，默认无限制</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites（部署网站）"><a href="#Deploy-to-remote-sites（部署网站）" class="headerlink" title="Deploy to remote sites（部署网站）"></a>Deploy to remote sites（部署网站）</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo deploy参数描述：    -g, --generate    部署之前预先生成静态文件</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h3 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h3><pre class=" language-lang-bash"><code class="language-lang-bash">布局(layout)：Hexo 有三种默认布局：post、page 和 draft。在创建者三种不同类型的文件时，它们将会被保存到不同的路径；而您自定义的其他布局和 post 相同，都将储存到 source/_posts 文件夹。以下是不同布局对应的路径：</code></pre><div class="table-container"><table><thead><tr><th style="text-align:center">布局</th><th style="text-align:center">路径</th></tr></thead><tbody><tr><td style="text-align:center">post</td><td style="text-align:center">source/_posts</td></tr><tr><td style="text-align:center">page</td><td style="text-align:center">source</td></tr><tr><td style="text-align:center">draft</td><td style="text-align:center">source/_drafts</td></tr></tbody></table></div><pre class=" language-lang-bash"><code class="language-lang-bash">模板(Scaffold):在新建文章时，Hexo 会根据 scaffolds 文件夹内相对应的文件来建立文件，例如：$ hexo new photo "My Gallery"在执行这行指令时，Hexo 会尝试在 scaffolds 文件夹中寻找 photo.md，并根据其内容建立文章。以下是您可以在模版中使用的变量：</code></pre><div class="table-container"><table><thead><tr><th>变量</th><th>描述</th></tr></thead><tbody><tr><td>layout</td><td>布局</td></tr><tr><td>title</td><td>标题</td></tr><tr><td>date</td><td>文件建立日期</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blog </tag>
            
            <tag> Hexo </tag>
            
            <tag> command </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
